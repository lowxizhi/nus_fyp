{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e4af68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import decomposition, manifold\n",
    "from matplotlib import cm\n",
    "import scipy.io\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from scipy.stats import ortho_group\n",
    "import argparse\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TimeDistributed, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.python.ops.rnn_cell_impl import RNNCell\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "n_eachring = 32\n",
    "n_input, n_output = 1+n_eachring, 1+n_eachring\n",
    "batch_size_train = 64\n",
    "hp = {\n",
    "        # batch size for training\n",
    "        'batch_size_train': batch_size_train,\n",
    "        # batch size for validation\n",
    "        'batch_size_val': batch_size_train,\n",
    "        # Type of loss functions\n",
    "        'loss_type': 'lsq',\n",
    "        # Optimizer\n",
    "        'optimizer': 'adam',\n",
    "        # Type of activation runctions, relu, softplus, tanh, elu\n",
    "        'activation': 'relu',\n",
    "        # Time constant (ms)\n",
    "        'tau': 100,\n",
    "        # discretization time step (ms)\n",
    "        'dt': 20,\n",
    "        # discretization time step/time constant\n",
    "        'alpha': 0.2,\n",
    "        # recurrent noise\n",
    "#         'sigma_rec': 0.05,\n",
    "        'sigma_rec': 0,\n",
    "        # input noise\n",
    "#         'sigma_x': 0.01,\n",
    "        'sigma_x': 0,\n",
    "        # leaky_rec weight initialization, diag, randortho, randgauss\n",
    "        'w_rec_init': 'randortho',\n",
    "        # a default weak regularization prevents instability\n",
    "        'l1_h': 0,\n",
    "        # l2 regularization on activity\n",
    "        'l2_h': 0,\n",
    "        # l2 regularization on weight\n",
    "        'l1_weight': 0,\n",
    "        # l2 regularization on weight\n",
    "        'l2_weight': 0,\n",
    "        # l2 regularization on deviation from initialization\n",
    "        'l2_weight_init': 0,\n",
    "        # Stopping performance\n",
    "        'target_perf': 1.,\n",
    "        # number of units each ring\n",
    "        'n_eachring': n_eachring,\n",
    "        # first input index for rule units\n",
    "        'rule_start': 1+n_eachring,\n",
    "        # number of input units\n",
    "        'n_input': n_input,\n",
    "        # number of output units\n",
    "        'n_output': n_output,\n",
    "        # number of recurrent units\n",
    "        'n_rnn': 256,\n",
    "        # learning rate\n",
    "        'learning_rate': 0.0001,\n",
    "        # number of display epochs\n",
    "        'steps_per_epoch': 1,\n",
    "        # number of fixed locations for isomap\n",
    "        'n_loc': 128,\n",
    "        # accuracy threshold to stop training\n",
    "        'accuracy_threshold': 0.9,\n",
    "        }\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "n_loc = 128\n",
    "n_stim_loc1, n_stim_loc2, repeat = stim_loc_shape = n_loc, n_loc, 1\n",
    "stim_loc_size = np.prod(stim_loc_shape)\n",
    "ind_stim_loc1, ind_stim_loc2, ind_repeat = np.unravel_index(range(stim_loc_size),stim_loc_shape)\n",
    "stim1_locs = 2*np.pi*ind_stim_loc1/n_stim_loc1\n",
    "stim2_locs = 2*np.pi*ind_stim_loc2/n_stim_loc2\n",
    "\n",
    "palette1 = cm.get_cmap('autumn',n_loc+15)\n",
    "palette1 = [palette1(i)[:3] for i in range(n_loc)]\n",
    "palette2 = cm.get_cmap('summer',n_loc+15)\n",
    "palette2 = [palette2(i)[:3] for i in range(n_loc)]\n",
    "\n",
    "color1=np.array(palette1)[ind_stim_loc1]\n",
    "color2=np.array(palette2)[ind_stim_loc2]\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "class LeakyRNNCell2(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self,hp,**kwargs):\n",
    "\n",
    "        self.units = hp['n_rnn']\n",
    "        self.state_size = hp['n_rnn']\n",
    "        \n",
    "        activation = hp['activation']\n",
    "        if activation == 'softplus':\n",
    "            self._activation = tf.nn.softplus\n",
    "            self._w_in_start = 1.0\n",
    "            self._w_rec_start = 0.5\n",
    "        elif activation == 'tanh':\n",
    "            self._activation = tf.tanh\n",
    "            self._w_in_start = 1.0\n",
    "            self._w_rec_start = 1.0\n",
    "        elif activation == 'relu':\n",
    "            self._activation = tf.nn.relu\n",
    "            self._w_in_start = 1.0\n",
    "            self._w_rec_start = 0.5\n",
    "        elif activation == 'power':\n",
    "            self._activation = lambda x: tf.square(tf.nn.relu(x))\n",
    "            self._w_in_start = 1.0\n",
    "            self._w_rec_start = 0.01\n",
    "        elif activation == 'retanh':\n",
    "            self._activation = lambda x: tf.tanh(tf.nn.relu(x))\n",
    "            self._w_in_start = 1.0\n",
    "            self._w_rec_start = 0.5\n",
    "        else:\n",
    "            raise ValueError('Unknown activation')\n",
    "            \n",
    "\n",
    "        self.rng = hp['rng']\n",
    "        self.seed = hp['seed']\n",
    "        self._alpha = hp['alpha']\n",
    "        self._sigma = np.sqrt(2 / self._alpha) * hp['sigma_rec']\n",
    "        \n",
    "        super(LeakyRNNCell2, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        w_in0 = (self.rng.randn(input_shape[-1], self.units) /\n",
    "                 np.sqrt(input_shape[-1]) * self._w_in_start)\n",
    "        self.kernel = self.add_weight(shape=(input_shape[-1], self.units), dtype=tf.float32,\n",
    "                                      initializer=tf.constant_initializer(w_in0),\n",
    "                                      name='kernel')\n",
    "        w_rec0 = self._w_rec_start*ortho_group.rvs(dim=self.units, random_state=self.rng)\n",
    "        self.recurrent_kernel = self.add_weight(shape=(self.units, self.units), dtype=tf.float32,\n",
    "                                                initializer=tf.constant_initializer(w_rec0),\n",
    "                                                name='recurrent_kernel')\n",
    "        matrix0 = np.concatenate((w_in0, w_rec0), axis=0)\n",
    "    \n",
    "#         self.kernel = self.add_weight(\n",
    "#                 name='kernel',\n",
    "#                 shape=[input_shape[-1] + self.units, self.units], \n",
    "#                 dtype=tf.float32,\n",
    "#                 initializer=tf.constant_initializer(matrix0))\n",
    "        \n",
    "        self._bias = self.add_weight(\n",
    "                name='bias',\n",
    "                shape=[self.units],\n",
    "                dtype=tf.float32,\n",
    "                initializer=tf.zeros_initializer())\n",
    "        \n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, states):\n",
    "        prev_output = states[0]\n",
    "#         h = tf.keras.backend.dot(array_ops.concat([inputs, prev_output], 1), self.kernel)\n",
    "        h = tf.keras.backend.dot(inputs, self.kernel)\n",
    "        h = h + tf.keras.backend.dot(prev_output, self.recurrent_kernel)\n",
    "        h = tf.nn.bias_add(h, self._bias)\n",
    "        noise = tf.random.normal(tf.shape(prev_output), mean=0, stddev=self._sigma, seed=self.seed)\n",
    "        h = h + noise\n",
    "        output = self._activation(h)\n",
    "        output = (1-self._alpha) * prev_output + self._alpha * output\n",
    "        return output, [output]\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "def get_dist(original_dist):\n",
    "    '''Get the distance in periodic boundary conditions'''\n",
    "    return np.minimum(abs(original_dist),2*np.pi-abs(original_dist))\n",
    "\n",
    "class Trial(object):\n",
    "    \"\"\"Class representing a batch of trials.\"\"\"\n",
    "\n",
    "    def __init__(self, config, tdim, batch_size):\n",
    "        \"\"\"A batch of trials.\n",
    "\n",
    "        Args:\n",
    "            config: dictionary of configurations\n",
    "            tdim: int, number of time steps\n",
    "            batch_size: int, batch size\n",
    "        \"\"\"\n",
    "        self.float_type = 'float32' # This should be the default\n",
    "        self.config = config\n",
    "        self.dt = self.config['dt']\n",
    "\n",
    "        self.n_eachring = self.config['n_eachring']\n",
    "        self.n_input = self.config['n_input']\n",
    "        self.n_output = self.config['n_output']\n",
    "        self.pref  = np.arange(0,2*np.pi,2*np.pi/self.n_eachring) # preferences\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.tdim = tdim\n",
    "        self.x = np.zeros((tdim, batch_size, self.n_input), dtype=self.float_type)\n",
    "        self.y = np.zeros((tdim, batch_size, self.n_output), dtype=self.float_type)\n",
    "        if self.config['loss_type'] == 'lsq':\n",
    "            self.y[:,:,:] = 0.05\n",
    "        # y_loc is the stimulus location of the output, -1 for fixation, (0,2 pi) for response\n",
    "        self.y_loc = -np.ones((tdim, batch_size)      , dtype=self.float_type)\n",
    "\n",
    "        self._sigma_x = config['sigma_x']*np.sqrt(2/config['alpha'])\n",
    "\n",
    "    def expand(self, var):\n",
    "        \"\"\"Expand an int/float to list.\"\"\"\n",
    "        if not hasattr(var, '__iter__'):\n",
    "            var = [var] * self.batch_size\n",
    "        return var\n",
    "\n",
    "    def add(self, loc_type, locs=None, ons=None, offs=None, strengths=1, mods=None):\n",
    "        \"\"\"Add an input or stimulus output.\n",
    "\n",
    "        Args:\n",
    "            loc_type: str (fix_in, stim, fix_out, out), type of information to be added\n",
    "            locs: array of list of float (batch_size,), locations to be added, only for loc_type=stim or out\n",
    "            ons: int or list, index of onset time\n",
    "            offs: int or list, index of offset time\n",
    "            strengths: float or list, strength of input or target output\n",
    "            mods: int or list, modalities of input or target output\n",
    "        \"\"\"\n",
    "\n",
    "        ons = self.expand(ons)\n",
    "        offs = self.expand(offs)\n",
    "        strengths = self.expand(strengths)\n",
    "        mods = self.expand(mods)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            if loc_type == 'fix_in':\n",
    "                self.x[ons[i]: offs[i], i, 0] = 1\n",
    "            elif loc_type == 'stim':\n",
    "                # Assuming that mods[i] starts from 1\n",
    "                self.x[ons[i]: offs[i], i, 1+(mods[i]-1)*self.n_eachring:1+mods[i]*self.n_eachring]                     += self.add_x_loc(locs[i])*strengths[i]\n",
    "            elif loc_type == 'fix_out':\n",
    "                # Notice this shouldn't be set at 1, because the output is logistic and saturates at 1\n",
    "                if self.config['loss_type'] == 'lsq':\n",
    "                    self.y[ons[i]: offs[i], i, 0] = 0.8\n",
    "                else:\n",
    "                    self.y[ons[i]: offs[i], i, 0] = 1.0\n",
    "            elif loc_type == 'out':\n",
    "                if self.config['loss_type'] == 'lsq':\n",
    "                    self.y[ons[i]: offs[i], i, 1:] += self.add_y_loc(locs[i])*strengths[i]  #target response\n",
    "                else:\n",
    "                    y_tmp = self.add_y_loc(locs[i])\n",
    "                    y_tmp /= np.sum(y_tmp)\n",
    "                    self.y[ons[i]: offs[i], i, 1:] += y_tmp\n",
    "                self.y_loc[ons[i]: offs[i], i] = locs[i] #location\n",
    "            else:\n",
    "                raise ValueError('Unknown loc_type')\n",
    "\n",
    "    def add_x_noise(self):\n",
    "        \"\"\"Add input noise.\"\"\"\n",
    "        self.x += self.config['rng'].randn(*self.x.shape)*self._sigma_x\n",
    "\n",
    "    def add_c_mask(self, pre_offs, post_ons):\n",
    "        \"\"\"Add a cost mask.\n",
    "\n",
    "        Usually there are two periods, pre and post response\n",
    "        Scale the mask weight for the post period so in total it's as important\n",
    "        as the pre period\n",
    "        \"\"\"\n",
    "\n",
    "        pre_on   = int(100/self.dt) # never check the first 100ms\n",
    "        pre_offs = self.expand(pre_offs)\n",
    "        post_ons = self.expand(post_ons)\n",
    "\n",
    "        if self.config['loss_type'] == 'lsq':\n",
    "            c_mask = np.zeros((self.tdim, self.batch_size, self.n_output), dtype=self.float_type)\n",
    "            for i in range(self.batch_size):\n",
    "                # Post response periods usually have the same length across tasks\n",
    "                c_mask[post_ons[i]:, i, :] = 5.\n",
    "                # Pre-response periods usually have different lengths across tasks\n",
    "                # To keep cost comparable across tasks\n",
    "                # Scale the cost mask of the pre-response period by a factor\n",
    "                c_mask[pre_on:pre_offs[i], i, :] = 1.\n",
    "\n",
    "            # self.c_mask[:, :, 0] *= self.n_eachring # Fixation is important\n",
    "            c_mask[:, :, 0] *= 2. # Fixation is important\n",
    "\n",
    "            self.c_mask = c_mask.reshape((self.tdim*self.batch_size, self.n_output))\n",
    "        else:\n",
    "            c_mask = np.zeros((self.tdim, self.batch_size), dtype=self.float_type)\n",
    "            for i in range(self.batch_size):\n",
    "                # Post response periods usually have the same length across tasks\n",
    "                # Having it larger than 1 encourages the network to achieve higher performance\n",
    "                c_mask[post_ons[i]:, i] = 5.\n",
    "                # Pre-response periods usually have different lengths across tasks\n",
    "                # To keep cost comparable across tasks\n",
    "                # Scale the cost mask of the pre-response period by a factor\n",
    "                c_mask[pre_on:pre_offs[i], i] = 1.\n",
    "\n",
    "            self.c_mask = c_mask.reshape((self.tdim*self.batch_size,))\n",
    "            self.c_mask /= self.c_mask.mean()\n",
    "\n",
    "    def add_rule(self, rule, on=None, off=None, strength=1.):\n",
    "        \"\"\"Add rule input.\"\"\"\n",
    "        if isinstance(rule, int):\n",
    "            self.x[on:off, :, self.config['rule_start']+rule] = strength\n",
    "        else:\n",
    "            ind_rule = get_rule_index(rule, self.config)\n",
    "            self.x[on:off, :, ind_rule] = strength\n",
    "\n",
    "    def add_x_loc(self, x_loc):\n",
    "        \"\"\"Input activity given location.\"\"\"\n",
    "        dist = get_dist(x_loc-self.pref)  # periodic boundary\n",
    "        dist /= np.pi/8\n",
    "        return 0.8*np.exp(-dist**2/2)\n",
    "\n",
    "    def add_y_loc(self, y_loc):\n",
    "        \"\"\"Target response given location.\"\"\"\n",
    "        dist = get_dist(y_loc-self.pref)  # periodic boundary\n",
    "        if self.config['loss_type'] == 'lsq':\n",
    "            dist /= np.pi/8\n",
    "            y = 0.8*np.exp(-dist**2/2)\n",
    "        else:\n",
    "            # One-hot output\n",
    "            y = np.zeros_like(dist)\n",
    "            ind = np.argmin(dist)\n",
    "            y[ind] = 1.\n",
    "        return y\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "\n",
    "def plot_loss_over_epochs(history, foldername=''):\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.plot(history.history['loss'],label=\"Training set loss\")\n",
    "    # plt.plot(history.history['val_loss'],label=\"Validation set loss\")\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    plt.savefig('%sloss_over_epochs.png'%foldername)\n",
    "\n",
    "def get_delay_bins(delay):\n",
    "    dt=20\n",
    "    stim1_ons = int(500/dt)\n",
    "    stim1_offs = stim1_ons + int(300/dt)\n",
    "    stim2_ons =stim1_offs + int(1000/dt)\n",
    "    stim2_offs = stim2_ons + int(300/dt)\n",
    "    fix_offs  = stim2_offs + int(1000/dt)\n",
    "\n",
    "    baseline = (0,stim1_ons)\n",
    "\n",
    "    if delay == 1:\n",
    "            delay_bins = (stim2_ons - int(500/dt),stim2_ons)\n",
    "\n",
    "    elif delay == 2:\n",
    "            delay_bins = (fix_offs - int(500/dt),fix_offs)\n",
    "\n",
    "    return delay_bins\n",
    "\n",
    "def plot_tuning_curves(Z,foldername='',filename=''):\n",
    "    input\n",
    "    fig,ax = plt.subplots(min(Z.shape[1],10),1,figsize=(5,min(Z.shape[1],10)*3))\n",
    "    for i in range(min(Z.shape[1],10)):\n",
    "        if Z.shape[1]<10:\n",
    "            neuron = i\n",
    "        else:\n",
    "            neuron = i*int(Z.shape[1]/10)\n",
    "        for loc1 in [j*10 for j in range(int(128/10))]:\n",
    "            df = pd.DataFrame({'first_stim':ind_stim_loc1[ind_stim_loc1==loc1],'second_stim':ind_stim_loc2[ind_stim_loc1==loc1],'activity':Z[:,neuron][ind_stim_loc1==loc1]})\n",
    "            x = df['second_stim']\n",
    "            y = df['activity']\n",
    "            ax[i].scatter(x,y,s=1, color=palette1[loc1], label=loc1)\n",
    "        ax[i].set_ylabel('%dth neuron'%neuron, fontsize=13)\n",
    "    ax[0].legend(loc='upper left', bbox_to_anchor= (1.05, 1.05), title='Stim 1')\n",
    "    ax[0].set_xlabel('Stim 2', fontsize=13)\n",
    "    ax[0].xaxis.set_label_position('top') \n",
    "    if filename != '':\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(foldername+filename+'_stim2')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "        \n",
    "    fig,ax = plt.subplots(min(Z.shape[1],10),1,figsize=(5,min(Z.shape[1],10)*3))\n",
    "    for i in range(min(Z.shape[1],10)):\n",
    "        if Z.shape[1]<10:\n",
    "            neuron = i\n",
    "        else:\n",
    "            neuron = i*int(Z.shape[1]/10)\n",
    "        for loc2 in [j*10 for j in range(int(128/10))]:\n",
    "            df = pd.DataFrame({'first_stim':ind_stim_loc1[ind_stim_loc2==loc2],'second_stim':ind_stim_loc2[ind_stim_loc2==loc2],'activity':Z[:,neuron][ind_stim_loc2==loc2]})\n",
    "            x = df['first_stim']\n",
    "            y = df['activity']\n",
    "            ax[i].scatter(x,y,s=1, color=palette2[loc2], label=loc2)\n",
    "        ax[i].set_ylabel('%dth neuron'%neuron, fontsize=13)\n",
    "    ax[0].legend(loc='upper left', bbox_to_anchor= (1.05, 1.05), title='Stim 2')\n",
    "    ax[0].set_xlabel('Stim 1', fontsize=13)\n",
    "    ax[0].xaxis.set_label_position('top') \n",
    "    if filename != '':\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(foldername+filename+'_stim1')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "def fit_isomap(data_to_use, n_neighbors = 15, target_dim = 3):\n",
    "    iso_instance = manifold.Isomap(n_neighbors = n_neighbors, n_components = target_dim)\n",
    "    proj = iso_instance.fit_transform(data_to_use)\n",
    "    return proj\n",
    "\n",
    "def set_axes_equal(ax):\n",
    "    '''Make axes of 3D plot have equal scale so that spheres appear as spheres,\n",
    "    cubes as cubes, etc..  This is one possible solution to Matplotlib's\n",
    "    ax.set_aspect('equal') and ax.axis('equal') not working for 3D.\n",
    "\n",
    "    Input\n",
    "      ax: a matplotlib axis, e.g., as output from plt.gca().\n",
    "    '''\n",
    "\n",
    "    x_limits = ax.get_xlim3d()\n",
    "    y_limits = ax.get_ylim3d()\n",
    "    z_limits = ax.get_zlim3d()\n",
    "\n",
    "    x_range = abs(x_limits[1] - x_limits[0])\n",
    "    x_middle = np.mean(x_limits)\n",
    "    y_range = abs(y_limits[1] - y_limits[0])\n",
    "    y_middle = np.mean(y_limits)\n",
    "    z_range = abs(z_limits[1] - z_limits[0])\n",
    "    z_middle = np.mean(z_limits)\n",
    "\n",
    "    # The plot bounding box is a sphere in the sense of the infinity\n",
    "    # norm, hence I call half the max range the plot radius.\n",
    "    plot_radius = 0.5*max([x_range, y_range, z_range])\n",
    "\n",
    "    ax.set_xlim3d([x_middle - plot_radius, x_middle + plot_radius])\n",
    "    ax.set_ylim3d([y_middle - plot_radius, y_middle + plot_radius])\n",
    "    ax.set_zlim3d([z_middle - plot_radius, z_middle + plot_radius])\n",
    "\n",
    "def plot_isomap(data_plot, color, annotate=False):\n",
    "    fig = plt.figure(figsize=(16,16),dpi=200)\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    if annotate:\n",
    "        ax.scatter(data_plot[:,0], data_plot[:,1], data_plot[:,2], \n",
    "            s=5, alpha=1, edgecolor='face',c=color)\n",
    "        label = 0\n",
    "        for xyz in zip(data_plot[:,0], data_plot[:,1], data_plot[:,2]):\n",
    "            x, y, z = xyz\n",
    "            ax.text(x, y, z, '%s' % (label), size=5, zorder=1, color='k')\n",
    "            label += 1\n",
    "    else:\n",
    "        ax.scatter(data_plot[:,0], data_plot[:,1], data_plot[:,2], \n",
    "            s=20, alpha=1, edgecolor='face',c=color)\n",
    "    ax.grid(False)\n",
    "    ax.xaxis.pane.fill = False\n",
    "    ax.yaxis.pane.fill = False\n",
    "    ax.zaxis.pane.fill = False\n",
    "    ax.xaxis.pane.set_edgecolor('w')\n",
    "    ax.yaxis.pane.set_edgecolor('w')\n",
    "    ax.zaxis.pane.set_edgecolor('w')\n",
    "    return fig, ax\n",
    "\n",
    "def plot_single_distractor_or_target(palette, xlim, ylim, zlim, label_plot, proj_plot, annotate=False, filename=''):\n",
    "\n",
    "    color=np.array(palette)[label_plot]\n",
    "\n",
    "    # h0_longest,h1_longest,h2_longest = run_ripser(proj_plot,figure_dir+'ripser'+figure_subscript)\n",
    "    fig, ax = plot_isomap(data_plot=proj_plot, color=color, annotate=annotate)\n",
    "    plt.setp(ax, xlim=xlim, ylim=ylim, zlim=zlim)\n",
    "    fig.tight_layout()\n",
    "    if filename is not None:\n",
    "        fig.savefig(filename)\n",
    "    plt.show()\n",
    "    plt.close(fig) \n",
    "\n",
    "\n",
    "def plot_all_isomap_figures(proj,foldername='',filename=''):\n",
    "    fig,ax = plot_isomap(data_plot=proj, color=color1)\n",
    "    set_axes_equal(ax)\n",
    "    fig.tight_layout()\n",
    "    if filename is not None:\n",
    "        fig.savefig(foldername+filename+'_target_isomap.png')\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "    fig,ax = plot_isomap(data_plot=proj, color=color2)\n",
    "    set_axes_equal(ax)\n",
    "    fig.tight_layout()\n",
    "    if filename is not None:\n",
    "        fig.savefig(foldername+filename+'_distractor_isomap.png')\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "    xlim=fig.gca().get_xlim()\n",
    "    ylim=fig.gca().get_ylim()\n",
    "    zlim=fig.gca().get_zlim()\n",
    "\n",
    "    num=0\n",
    "    indices = ind_stim_loc1==num\n",
    "    label_plot = ind_stim_loc2[indices]\n",
    "    proj_plot = proj[indices,:]\n",
    "    plot_single_distractor_or_target(palette = palette2, xlim = xlim, ylim = ylim, zlim = zlim, label_plot=label_plot, proj_plot = proj_plot, filename = foldername+filename+'_single_target.png')\n",
    "\n",
    "    num=0\n",
    "    indices = ind_stim_loc2==num\n",
    "    label_plot = ind_stim_loc1[indices]\n",
    "    proj_plot = proj[indices,:]\n",
    "    plot_single_distractor_or_target(palette = palette1, xlim = xlim, ylim = ylim, zlim = zlim, label_plot=label_plot, proj_plot = proj_plot, filename = foldername+filename+'_single_distractor.png')\n",
    "\n",
    "\n",
    "# In[109]:\n",
    "\n",
    "\n",
    "def get_c_mask(batch_dim):\n",
    "\n",
    "    dt = hp['dt']\n",
    "    pre_on   = int(100/dt)\n",
    "    \n",
    "    stim1_strengths = 1\n",
    "    stim2_strengths = 1\n",
    "    stim1_ons = int(500/dt)\n",
    "    stim1_offs = stim1_ons + int(300/dt)\n",
    "    stim2_ons =stim1_offs + int(1000/dt)\n",
    "    stim2_offs = stim2_ons + int(300/dt)\n",
    "    fix_offs  = stim2_offs + int(1000/dt)\n",
    "    output_2_on = fix_offs + int(500/dt)\n",
    "    tdim = output_2_on + int(500/dt)\n",
    "    check_ons = fix_offs + int(100/dt)\n",
    "    \n",
    "    pre_offs=fix_offs\n",
    "    post_ons=check_ons\n",
    "    \n",
    "    c_mask = np.zeros(batch_dim, dtype='float32')\n",
    "    for i in range(batch_dim[0]):\n",
    "        c_mask[i, post_ons:, :] = 5.\n",
    "        c_mask[i, pre_on:pre_offs, :] = 1.\n",
    "    c_mask[:, :, 0] *= 2. # Fixation is important\n",
    "    c_mask = c_mask.reshape((batch_dim[0]*batch_dim[1], batch_dim[2]))\n",
    "    \n",
    "    return c_mask\n",
    "    \n",
    "\n",
    "def custom_mse(y_true, y_hat):\n",
    "\n",
    "    n_output = hp['n_output']\n",
    "    y_true_shaped = tf.reshape(y_true, (-1, n_output))\n",
    "    y_hat_shaped = tf.reshape(y_hat, (-1, n_output))\n",
    "    cost_lsq = K.mean(K.square((y_true_shaped - y_hat_shaped) * c_mask_shaped_tf))\n",
    "    \n",
    "#     cost = self.cost_lsq + self.cost_reg\n",
    "    cost = cost_lsq\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def popvec(y):\n",
    "    \"\"\"Population vector read out.\n",
    "\n",
    "    Assuming the last dimension is the dimension to be collapsed\n",
    "\n",
    "    Args:\n",
    "        y: population output on a ring network. Numpy array (Batch, Units)\n",
    "\n",
    "    Returns:\n",
    "        Readout locations: Numpy array (Batch,)\n",
    "    \"\"\"\n",
    "    pref = np.arange(0, 2*np.pi, 2*np.pi/y.shape[-1])  # preferences\n",
    "    temp_sum = y.sum(axis=-1)\n",
    "    temp_cos = np.sum(y*np.cos(pref), axis=-1)/temp_sum\n",
    "    temp_sin = np.sum(y*np.sin(pref), axis=-1)/temp_sum\n",
    "    loc = np.arctan2(temp_sin, temp_cos)\n",
    "    return np.mod(loc, 2*np.pi)\n",
    "\n",
    "def custom_response_accuracy(y_true, y_hat):\n",
    "    if type(y_true) is not np.ndarray:\n",
    "        y_true = y_true.numpy()\n",
    "    if type(y_hat) is not np.ndarray:\n",
    "        y_hat = y_hat.numpy()\n",
    "    y_hat_loc1 = popvec(np.mean(y_hat[:, 155:180, 1:], axis=1))\n",
    "    y_true_loc1 = popvec(np.mean(y_true[:, 155:180, 1:], axis=1))\n",
    "    original_dist1 = y_true_loc1 - y_hat_loc1\n",
    "    dist1 = np.minimum(abs(original_dist1), 2*np.pi-abs(original_dist1))\n",
    "    corr_loc1 = dist1 < 2*np.pi/hp['n_loc']\n",
    "\n",
    "    y_hat_loc2 = popvec(np.mean(y_hat[:, 180:, 1:], axis=1))\n",
    "    y_true_loc2 = popvec(np.mean(y_true[:, 180:, 1:], axis=1))\n",
    "    original_dist2 = y_true_loc2 - y_hat_loc2\n",
    "    dist2 = np.minimum(abs(original_dist2), 2*np.pi-abs(original_dist2))\n",
    "    corr_loc2 = dist2 < 2*np.pi/hp['n_loc']\n",
    "    \n",
    "    return np.sum(corr_loc1*0.5+corr_loc2*0.5)/corr_loc1.shape[0]\n",
    "\n",
    "def custom_response_loss(y_true, y_hat):\n",
    "    if type(y_true) is not np.ndarray:\n",
    "        y_true = y_true.numpy()\n",
    "    if type(y_hat) is not np.ndarray:\n",
    "        y_hat = y_hat.numpy()\n",
    "    y_hat_loc1 = popvec(np.mean(y_hat[:, 155:180, 1:], axis=1))\n",
    "    y_true_loc1 = popvec(np.mean(y_true[:, 155:180, 1:], axis=1))\n",
    "    original_dist1 = y_true_loc1 - y_hat_loc1\n",
    "    dist1 = np.minimum(abs(original_dist1), 2*np.pi-abs(original_dist1))\n",
    "\n",
    "    y_hat_loc2 = popvec(np.mean(y_hat[:, 180:, 1:], axis=1))\n",
    "    y_true_loc2 = popvec(np.mean(y_true[:, 180:, 1:], axis=1))\n",
    "    original_dist2 = y_true_loc2 - y_hat_loc2\n",
    "    dist2 = np.minimum(abs(original_dist2), 2*np.pi-abs(original_dist2))\n",
    "    \n",
    "    return np.sum(dist1*0.5+dist2*0.5)/dist1.shape[0]*(360/(2*np.pi))\n",
    "\n",
    "def custom_perf(y_true, y_hat):\n",
    "\n",
    "    if type(y_true) is not np.ndarray:\n",
    "        y_true = y_true.numpy()\n",
    "    if type(y_hat) is not np.ndarray:\n",
    "        y_hat = y_hat.numpy()\n",
    "\n",
    "    y_true = y_true[:,-1,:]\n",
    "    y_hat = y_hat[:,-1,:]\n",
    "\n",
    "    y_hat_loc = popvec(y_hat[..., 1:])\n",
    "    y_true_loc = popvec(y_true[..., 1:])\n",
    "    y_hat_fix = y_hat[..., 0]\n",
    "    fixating = y_hat_fix > 0.5 \n",
    "\n",
    "    original_dist = y_true_loc - y_hat_loc\n",
    "    dist = np.minimum(abs(original_dist), 2*np.pi-abs(original_dist))\n",
    "    corr_loc = dist < 2*np.pi/hp['n_loc']\n",
    "\n",
    "\n",
    "    # Should fixate?\n",
    "    should_fix = y_true_loc < 0\n",
    "\n",
    "    # performance\n",
    "    perf = should_fix * fixating + (1-should_fix) * corr_loc * (1-fixating) \n",
    "    return np.mean(perf)\n",
    "# In[83]:\n",
    "\n",
    "def generate_trial(mode='random',batch_size=hp['batch_size_train'],**kwargs):\n",
    "    dt = hp['dt']\n",
    "    if mode == 'random':\n",
    "        rng = hp['rng']\n",
    "        stim1_locs = rng.uniform(0, 2*np.pi, (batch_size,))\n",
    "        stim2_locs = rng.uniform(0, 2*np.pi, (batch_size,))\n",
    "\n",
    "        stims_mean = rng.uniform(0.8,1.2,(batch_size,))\n",
    "        stims_coh  = rng.choice([0.,0.08,0.16,0.32],(batch_size,))\n",
    "        stims_sign = rng.choice([1,-1], (batch_size,))\n",
    "\n",
    "        stim1_strengths = stims_mean + stims_coh*stims_sign\n",
    "        stim2_strengths = stims_mean - stims_coh*stims_sign\n",
    "\n",
    "    elif mode == 'fixed':\n",
    "        n_loc = kwargs['n_loc']\n",
    "        batch_size = n_loc*n_loc\n",
    "        n_stim_loc1, n_stim_loc2, repeat = stim_loc_shape = n_loc, n_loc, 1\n",
    "        stim_loc_size = np.prod(stim_loc_shape)\n",
    "        ind_stim_loc1, ind_stim_loc2, ind_repeat = np.unravel_index(range(stim_loc_size),stim_loc_shape)\n",
    "        stim1_locs = 2*np.pi*ind_stim_loc1/n_stim_loc1\n",
    "        stim2_locs = 2*np.pi*ind_stim_loc2/n_stim_loc2\n",
    "        \n",
    "        stim1_strengths = 1\n",
    "        stim2_strengths = 1\n",
    "        \n",
    "    else:\n",
    "        raise ValueError('Unknown mode: ' + str(mode))\n",
    "        \n",
    "    stim1_ons = int(500/dt)\n",
    "    stim1_offs = stim1_ons + int(300/dt)\n",
    "    stim2_ons =stim1_offs + int(1000/dt)\n",
    "    stim2_offs = stim2_ons + int(300/dt)\n",
    "    fix_offs  = stim2_offs + int(1000/dt)\n",
    "    output_2_on = fix_offs + int(500/dt)\n",
    "    tdim = output_2_on + int(500/dt)\n",
    "\n",
    "    check_ons = fix_offs + int(100/dt)\n",
    "\n",
    "    trial = Trial(hp, tdim, batch_size)\n",
    "    trial.add('fix_in', offs=fix_offs)\n",
    "    trial.add('stim', stim1_locs, ons=stim1_ons, offs=stim1_offs, strengths=stim1_strengths, mods=1)\n",
    "    trial.add('stim', stim2_locs, ons=stim2_ons, offs=stim2_offs, strengths=stim2_strengths, mods=1)\n",
    "    trial.add('fix_out', offs=fix_offs)\n",
    "    stim_locs = [stim1_locs[i] for i in range(batch_size)]\n",
    "    stim_locs2 = [stim2_locs[i] for i in range(batch_size)]\n",
    "    trial.add('out',stim_locs,ons=fix_offs,offs=output_2_on)\n",
    "    trial.add('out',stim_locs2,ons=output_2_on)\n",
    "\n",
    "    trial.add_c_mask(pre_offs=fix_offs,post_ons=check_ons)\n",
    "    trial.epochs = {'fix1':(None,stim1_ons),\n",
    "                    'stim1':(stim1_ons,stim1_offs),\n",
    "                    'delay1':(stim1_offs,stim2_ons),\n",
    "                    'stim2':(stim2_ons,stim2_offs),\n",
    "                    'delay2':(stim2_offs,fix_offs),\n",
    "                    'go1':(fix_offs,output_2_on),\n",
    "                    'go2':(output_2_on,None)}\n",
    "    return trial\n",
    "\n",
    "def train_generator():\n",
    "    for i in range(hp['steps_per_epoch']*hp['n_epochs']):\n",
    "        trial = generate_trial(mode='random',batch_size=hp['batch_size_train'])\n",
    "        x = trial.x.swapaxes(0,1)\n",
    "        y = trial.y.swapaxes(0,1)\n",
    "        yield x, y\n",
    "\n",
    "def val_generator():\n",
    "    for i in range(hp['steps_per_epoch']*hp['n_epochs']):\n",
    "        trial = generate_trial(mode='random',batch_size=hp['batch_size_val'])\n",
    "        x = trial.x.swapaxes(0,1)\n",
    "        y = trial.y.swapaxes(0,1)\n",
    "        yield x, y\n",
    "\n",
    "\n",
    "# In[84]:\n",
    "\n",
    "\n",
    "def create_model(rnn_layer):\n",
    "    model = Sequential()\n",
    "    model.add(rnn_layer)\n",
    "    model.add(TimeDistributed(Dense(33, activation='sigmoid')))\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp['learning_rate']),\n",
    "        loss=custom_mse,\n",
    "        metrics=[custom_perf, custom_response_accuracy,custom_response_loss],\n",
    "        run_eagerly=True)\n",
    "    # model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp['learning_rate']),\n",
    "    #     loss=custom_mse,\n",
    "    #     metrics=[custom_perf],\n",
    "    #     run_eagerly=True)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# In[85]:\n",
    "\n",
    "\n",
    "def generate_hidden_layer_plots(rnn_layer, superscript):\n",
    "    hidden = rnn_layer(x)\n",
    "\n",
    "    for delay in range(1,3):\n",
    "        print('delay:%s'%delay)\n",
    "        delay_bins = get_delay_bins(delay=delay)\n",
    "        #extract mean firing rates for delay bins\n",
    "        delay_hidden = np.mean(hidden[:, delay_bins[0]:delay_bins[1], :], axis=1)\n",
    "    #     delay_hidden = hidden[:, delay_bins[1], :]\n",
    "        plot_tuning_curves(delay_hidden,tuning_curve_folder,'%s_delay%d_hidden'%(superscript,delay))\n",
    "\n",
    "        proj = fit_isomap(data_to_use=delay_hidden)\n",
    "        plot_all_isomap_figures(proj,isomap_folder,'%s_delay%d_hidden'%(superscript,delay))\n",
    "\n",
    "def save_hp(hp, model_dir):\n",
    "    \"\"\"Save the hyper-parameter file of model save_name\"\"\"\n",
    "    hp_copy = hp.copy()\n",
    "    hp_copy.pop('rng', None)\n",
    "    with open(os.path.join(model_dir, 'hp.json'), 'w') as f:\n",
    "        json.dump(hp_copy, f)\n",
    "\n",
    "class NBatchLogger(tf.keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    A Logger that log average performance per `display` steps.\n",
    "    \"\"\"\n",
    "    def __init__(self, display):\n",
    "        self.step = 0\n",
    "        self.display = display\n",
    "        self.metric_cache = {}\n",
    "        self.t_start = time.time()\n",
    "        \n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        keys = list(logs.keys())\n",
    "        for k in keys:\n",
    "            self.metric_cache[k] = self.metric_cache.get(k, 0) + logs[k]\n",
    "        if self.step % self.display == 0:\n",
    "            metrics_log = ''\n",
    "            for (k, v) in self.metric_cache.items():\n",
    "                val = v / self.display\n",
    "                if abs(val) > 1e-3:\n",
    "                    metrics_log += ' - %s: %.4f' % (k, val)\n",
    "                else:\n",
    "                    metrics_log += ' - %s: %.4e' % (k, val)\n",
    "            print('time: {} | trial: {} | batch: {} ... {}'.format(time.time()-self.t_start, self.step * hp['batch_size_train'], self.step,\n",
    "                                          metrics_log))\n",
    "            self.metric_cache.clear()\n",
    "        self.step += 1\n",
    "\n",
    "class AccuracyThresholdCallback(tf.keras.callbacks.Callback): \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        acc = logs.get('custom_response_accuracy')\n",
    "        if(acc > hp['accuracy_threshold']):\n",
    "            print(\"\\nReached %2.2f%% accuracy!\" %(acc*100))   \n",
    "            self.model.stop_training = True\n",
    "\n",
    "class printeverybatch(tf.keras.Model):\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        tf.print('train batch:')\n",
    "        tf.print(x[0,90:105,0])\n",
    "        tf.print(y[0,180:,0])\n",
    "        tf.print()\n",
    "        tf.print(x[0,90:105,1])\n",
    "        tf.print(y[0,180:,1])\n",
    "        return super().train_step(data)\n",
    "\n",
    "    def test_step(self, data):\n",
    "        x, y = data\n",
    "        tf.print('val batch:')\n",
    "        tf.print(x[0,90:105,0])\n",
    "        tf.print(y[0,180:,0])\n",
    "        tf.print()\n",
    "        tf.print(x[0,90:105,1])\n",
    "        tf.print(y[0,180:,1])\n",
    "        return super().test_step(data)\n",
    "# # In[108]:\n",
    "\n",
    "# def parse_args():\n",
    "#     parser = argparse.ArgumentParser(\"HP for training\")\n",
    "#     parser.add_argument(\"--seed\",type=int,default = 0, help = \"Seed number\")\n",
    "#     parser.add_argument(\"--n_rnn\",type=int,default = 256, help = \"Number of hidden neurons\")\n",
    "#     parser.add_argument(\"--batch_size_train\",type=int,default = 512, help = \"Training Batch Size\")\n",
    "#     parser.add_argument(\"--accuracy_threshold\",type=float,default = 0.9, help = \"Accuracy Threshold To Stop Training\")\n",
    "#     parser.add_argument(\"--with_noise\",type=str,default = 'True', help = \"Whether to add input and recurrent noise\",choices=('True','False'))\n",
    "#     parser.add_argument(\"--load_model\",type=str,default = 'False', help = \"Whether to load model\",choices=('True','False'))\n",
    "\n",
    "#     return parser.parse_args()\n",
    "\n",
    "# arglist = parse_args()\n",
    "# seed = arglist.seed\n",
    "# n_rnn = arglist.n_rnn\n",
    "# batch_size_train = arglist.batch_size_train\n",
    "# accuracy_threshold = arglist.accuracy_threshold\n",
    "# with_noise = arglist.with_noise == 'True'\n",
    "# load_model = arglist.load_model == 'True'\n",
    "\n",
    "# print(f\"seed: {seed}, n_rnn: {n_rnn}, batch_size_train: {batch_size_train}, accuracy_threshold: {accuracy_threshold}, with_noise: {with_noise}, load_model: {load_model}\")\n",
    "\n",
    "# tf.random.set_seed(seed)\n",
    "\n",
    "# trial = generate_trial(mode='fixed',n_loc=hp['n_loc'])\n",
    "# x = trial.x.swapaxes(0,1)\n",
    "# y = trial.y.swapaxes(0,1)\n",
    "\n",
    "# hp['n_rnn']=n_rnn\n",
    "# hp['batch_size_train']=batch_size_train\n",
    "# hp['batch_size_val']=batch_size_train\n",
    "# hp['accuracy_threshold']=accuracy_threshold\n",
    "# hp['n_epochs']=int(100000000/batch_size_train)\n",
    "# hp['n_display']=int(32000/batch_size_train)\n",
    "# hp['n_patience']=hp['n_display']*100\n",
    "# hp['seed']=seed\n",
    "# hp['rng'] = np.random.RandomState(seed)\n",
    "\n",
    "# c_mask_shaped = get_c_mask((hp['batch_size_train'], x.shape[1], x.shape[2]))\n",
    "# c_mask_shaped_tf = tf.convert_to_tensor(c_mask_shaped, dtype=tf.float32)\n",
    "\n",
    "# model_folder = '/hpctmp/e0316055/2_stim_batch_size_%s/n_hidden_%s/2_stim_batch_size_%s_n_hidden_%s_acc_%s_seed_%s'%(batch_size_train,n_rnn,batch_size_train,n_rnn,int(accuracy_threshold*100),seed)\n",
    "# if with_noise:\n",
    "#     hp['sigma_rec']=0.05\n",
    "#     hp['sigma_x']=0.01\n",
    "#     model_folder += '_with_noise'\n",
    "# else:\n",
    "#     model_folder += '_with_noise'\n",
    "# print('model folder: ' + model_folder)\n",
    "\n",
    "# main_checkpoint_path = os.path.dirname(model_folder)+\"/checkpoint_seed_%s/cp.ckpt\"%seed\n",
    "# checkpoint_path = model_folder+\"/checkpoint/cp.ckpt\"\n",
    "# isomap_folder = model_folder+'/isomap/'\n",
    "# tuning_curve_folder = model_folder+'/tuning_curves/'\n",
    "# loss_curve_folder = model_folder+'/'\n",
    "\n",
    "# if not os.path.exists(model_folder):\n",
    "#     os.makedirs(model_folder)\n",
    "#     os.makedirs(tuning_curve_folder)\n",
    "#     os.makedirs(isomap_folder)\n",
    "\n",
    "# save_hp(hp, model_folder)\n",
    "# # Display hp\n",
    "# for key, val in hp.items():\n",
    "#     print('{:20s} = '.format(key) + str(val))\n",
    "\n",
    "# dataset_train = tf.data.Dataset.from_generator(train_generator, \n",
    "#                                 output_types=(np.float32,np.float32), \n",
    "#                                 output_shapes=((hp['batch_size_train'],x.shape[1],x.shape[2]),(hp['batch_size_train'],y.shape[1],y.shape[2])))\n",
    "# # dataset_val = tf.data.Dataset.from_generator(val_generator,\n",
    "# #                                 output_types=(np.float32,np.float32), \n",
    "# #                                 output_shapes=((hp['batch_size_val'],x.shape[1],x.shape[2]),(hp['batch_size_val'],y.shape[1],y.shape[2])))\n",
    "\n",
    "# cell = LeakyRNNCell2(hp)\n",
    "# rnn_layer = tf.keras.layers.RNN(cell,input_shape=((x.shape[1],x.shape[2])),\n",
    "#                                 return_sequences=True)\n",
    "\n",
    "# # print('generating untrained plots......')\n",
    "# # generate_hidden_layer_plots(rnn_layer, superscript='untrained')\n",
    "\n",
    "# model = create_model(rnn_layer)\n",
    "\n",
    "# if load_model and os.path.exists(os.path.dirname(main_checkpoint_path)):\n",
    "#     print('loading previous model......')\n",
    "#     model.load_weights(main_checkpoint_path)\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "# performance_dict = {}\n",
    "# performance_dict['untrained accuracy on %s trials'%x.shape[0]] = round(custom_response_accuracy(y, model.predict(x)),4)\n",
    "# performance_dict['untrained loss on %s trials'%x.shape[0]] = round(custom_response_loss(y, model.predict(x)),4)\n",
    "# performance_dict['untrained perf on %s trials'%x.shape[0]] = round(custom_perf(y, model.predict(x)),4)\n",
    "# print(performance_dict)\n",
    "\n",
    "# display_callback = NBatchLogger(display=hp['n_display'])\n",
    "# threshold_callback = AccuracyThresholdCallback()\n",
    "# history_logger_callback = tf.keras.callbacks.CSVLogger(loss_curve_folder+'log.csv', separator=\",\", append=True)\n",
    "# cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "#                                                 save_weights_only=True,\n",
    "#                                                 save_best_only=True,\n",
    "#                                                 monitor='loss',\n",
    "#                                                 verbose=0)\n",
    "# es_callback = tf.keras.callbacks.EarlyStopping(restore_best_weights=True, \n",
    "#                                                 patience=hp['n_patience'],\n",
    "#                                                 monitor='loss')\n",
    "\n",
    "# history = model.fit(dataset_train, epochs=hp['n_epochs'], steps_per_epoch=hp['steps_per_epoch'], verbose=0,\n",
    "#           callbacks=[cp_callback, history_logger_callback, display_callback, threshold_callback])\n",
    "\n",
    "# shutil.copytree(os.path.dirname(checkpoint_path), os.path.dirname(main_checkpoint_path), dirs_exist_ok=True)\n",
    "\n",
    "# plot_loss_over_epochs(history, foldername=loss_curve_folder)\n",
    "\n",
    "\n",
    "# hp['sigma_rec']=0\n",
    "# hp['sigma_x']=0\n",
    "# cell = LeakyRNNCell2(hp)\n",
    "# rnn_layer = tf.keras.layers.RNN(cell,input_shape=((x.shape[1],x.shape[2])),\n",
    "#                                 return_sequences=True)\n",
    "# model = create_model(rnn_layer)\n",
    "# model.load_weights(checkpoint_path)\n",
    "# rnn_layer = tf.keras.layers.RNN(cell,input_shape=((x.shape[1],x.shape[2])),\n",
    "#                                 return_sequences=True, weights=model.layers[0].get_weights())\n",
    "# print('generating trained plots......')\n",
    "# generate_hidden_layer_plots(rnn_layer, superscript='trained')\n",
    "\n",
    "# performance_dict['trained accuracy on %s trials'%x.shape[0]] = round(custom_response_accuracy(y, model.predict(x)),4)\n",
    "# performance_dict['trained loss on %s trials'%x.shape[0]] = round(custom_response_loss(y, model.predict(x)),4)\n",
    "# performance_dict['trained perf on %s trials'%x.shape[0]] = round(custom_perf(y, model.predict(x)),4)\n",
    "# print(performance_dict)\n",
    "\n",
    "# with open(os.path.join(model_folder, 'performance.json'), 'w') as f:\n",
    "#     json.dump(performance_dict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464d6925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pingouin as pg\n",
    "def get_selectivity_index(aov, selectivity_type='linear_mixed_selectivity'):\n",
    "    linV1 = aov.iloc[0,1]\n",
    "    linV2 = aov.iloc[1,1]\n",
    "    linV = aov.iloc[0,1]+aov.iloc[1,1]\n",
    "    nonlinV = aov.iloc[2,1]\n",
    "    totalV = aov.iloc[0,1]+aov.iloc[1,1]+aov.iloc[2,1]\n",
    "    \n",
    "    if selectivity_type=='pure_selectivity_to_1':\n",
    "        return linV1/totalV\n",
    "    if selectivity_type=='pure_selectivity_to_2':\n",
    "        return linV2/totalV\n",
    "    if selectivity_type=='linear_mixed_selectivity':\n",
    "        return linV/totalV\n",
    "    if selectivity_type=='non_linear_mixed_selectivity':\n",
    "        return nonlinV/totalV\n",
    "    \n",
    "def get_anova_p_value(aov, selectivity_type='linear_mixed_selectivity'):\n",
    "    if selectivity_type=='pure_selectivity_to_1':\n",
    "        p1 = aov.iloc[0,5]\n",
    "        return p1\n",
    "    if selectivity_type=='pure_selectivity_to_2':\n",
    "        p2 = aov.iloc[1,5]\n",
    "        return p2\n",
    "    if selectivity_type=='linear_mixed_selectivity':\n",
    "        plin = aov.iloc[0,5]*aov.iloc[1,5]\n",
    "        return plin\n",
    "    if selectivity_type=='non_linear_mixed_selectivity':\n",
    "        pnonlin = aov.iloc[2,5]\n",
    "        return pnonlin\n",
    "    \n",
    "def get_separability_index(A):\n",
    "    U, s, VT = svd(A)\n",
    "    sum_of_squares_s = np.array([s[i]**2 for i in range(len(s))]).sum()\n",
    "    separability_index = s[0]**2/sum_of_squares_s\n",
    "    return separability_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03336f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pingouin as pg\n",
    "from numpy import diag\n",
    "from numpy import dot\n",
    "from scipy.linalg import svd\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "def get_diag_meas(block_dist):\n",
    "\n",
    "    d = block_dist.shape[0]\n",
    "    j = np.ones(d)\n",
    "    r = np.array(range(1,d+1))\n",
    "    r2 = np.array(list(map(lambda x: x*x, range(1,d+1))))\n",
    "\n",
    "    block_dist = block_dist + np.abs(np.min(block_dist))\n",
    "    highest = np.nanmax(np.abs(block_dist))\n",
    "    block_dist = block_dist/highest\n",
    "\n",
    "    n = j.dot(block_dist.dot(j.T))\n",
    "    sum_x = r.dot(block_dist.dot(j.T))\n",
    "    sum_y = j.dot(block_dist.dot(r.T))\n",
    "    sum_x2 = r2.dot(block_dist.dot(j.T))\n",
    "    sum_y2 = j.dot(block_dist.dot(r2.T))\n",
    "    sum_xy = r.dot(block_dist.dot(r.T))\n",
    "\n",
    "    corr = (n*sum_xy - sum_x*sum_y)/((math.sqrt(n*sum_x2 - (sum_x)**2))*(math.sqrt(n*sum_y2 - (sum_y)**2)))\n",
    "    return corr\n",
    "\n",
    "def plot_weight_selectivity_matrix(target,Z,W,n_loc,foldername,filename,stim,delay):\n",
    "    \n",
    "    # mean firing rate of each neuron for each location. shape: (n_rnn, n_loc)\n",
    "    mean_delay_FR = np.array([[np.mean(Z[target==i,j]) for i in range(n_loc)] for j in range(Z.shape[1])])\n",
    "    \n",
    "    # the most sensitive location (location with highest mean firing rate) of each neuron. shape: n_rnn\n",
    "    label = np.argmax(mean_delay_FR,1)\n",
    "    \n",
    "    # indices of neurons, sorted according to most sensitive location. shape: n_rnn\n",
    "    ind = np.argsort(label)\n",
    "    \n",
    "    # weight matrix sorted according to most sensitive location. shape: (n_rnn, n_rnn)\n",
    "    sorted_weight_matrix = W[:,ind]\n",
    "    sorted_weight_matrix = sorted_weight_matrix[ind,:]\n",
    "    \n",
    "    # block matrix, mean weights connecting neurons most sensitive to 1 loc to neurons most sensitive to another loc. shape: (n_loc, n_loc)\n",
    "    block_dist = np.zeros([n_loc,n_loc])\n",
    "    for i in range(n_loc):\n",
    "        for j in range(n_loc):\n",
    "            from_neurons = np.where(label[ind] == i)[0]\n",
    "            to_neurons = np.where(label[ind] == j)[0]\n",
    "            block_dist[i,j] = np.nanmean(sorted_weight_matrix[from_neurons,:][:,to_neurons])\n",
    "\n",
    "    highest = np.nanmax(np.abs(block_dist))\n",
    "    fig = plt.figure()\n",
    "    diag=get_diag_meas(block_dist)\n",
    "    plt.title(f'Weight Selectivity Matrix Stim {stim} Delay {delay}')\n",
    "    plt.legend(loc='upper left', bbox_to_anchor= (1.3, 1.03),\n",
    "               title=f'diag: {round(diag,3)}')\n",
    "    sns.heatmap(block_dist,cmap='bwr',vmin=-highest, vmax=highest, )\n",
    "\n",
    "    if filename != '':\n",
    "        fig.savefig(foldername+filename,bbox_inches='tight')\n",
    "    return diag\n",
    "\n",
    "def plot_activity_heatmap(A, neuron, separability_index, linear_mixed_selectivity_index, foldername='', filename=''):\n",
    "    fig, ax = plt.subplots(1,1)\n",
    "    sns.heatmap(A)\n",
    "    ax.set_xlabel('Stim2')\n",
    "    ax.set_ylabel('Stim1')\n",
    "    ax.set_title(f'Activity Heatmap (Neuron {neuron})')\n",
    "    plt.legend(loc='upper left', bbox_to_anchor= (1.3, 1.03),\n",
    "               title=f'linear separability: {round(separability_index,3)}\\nlinear selectivity: {round(linear_mixed_selectivity_index,3)}')\n",
    "    if filename != '':\n",
    "        fig.savefig(foldername+filename,bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def get_separability_index(A):\n",
    "    U, s, VT = svd(A)\n",
    "    sum_of_squares_s = np.array([s[i]**2 for i in range(len(s))]).sum()\n",
    "    separability_index = s[0]**2/sum_of_squares_s\n",
    "    return separability_index\n",
    "\n",
    "def get_linear_mixed_selectivity_index(aov):\n",
    "    linV = aov.iloc[0,1]+aov.iloc[1,1]\n",
    "    nonlinV = aov.iloc[2,1]\n",
    "    linear_mixed_selectivity_index = (linV)/(nonlinV+linV)\n",
    "    return linear_mixed_selectivity_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8290538c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_loc = 8\n",
    "if n_loc < 10:\n",
    "    locs = [j for j in range(n_loc)]\n",
    "else:\n",
    "    locs = [int(j*n_loc/10) for j in range(10)]\n",
    "locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b232d85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tuning_curves(Z,n_loc,ind_stim_loc1,ind_stim_loc2,foldername='',filename=''):\n",
    "    \n",
    "    palette1 = cm.get_cmap('autumn',n_loc+int(8*(15/128)))\n",
    "    palette1 = [palette1(i)[:3] for i in range(n_loc)]\n",
    "    palette2 = cm.get_cmap('summer',n_loc+int(8*(15/128)))\n",
    "    palette2 = [palette2(i)[:3] for i in range(n_loc)]\n",
    "    \n",
    "    if n_loc < 10:\n",
    "        locs = [j for j in range(n_loc)]\n",
    "    else:\n",
    "        locs = [int(j*n_loc/10) for j in range(10)]\n",
    "    \n",
    "    fig,ax = fig,ax = plt.subplots(1,1)\n",
    "    for loc2 in locs:\n",
    "        df = pd.DataFrame({'first_stim':ind_stim_loc1[ind_stim_loc2==loc2],'second_stim':ind_stim_loc2[ind_stim_loc2==loc2],'activity':Z[ind_stim_loc2==loc2]})\n",
    "        x = df['first_stim']\n",
    "        y = df['activity']\n",
    "        ax.scatter(x,y,s=1, color=palette2[loc2], label=loc2)\n",
    "    ax.set_ylabel('%dth neuron'%neuron, fontsize=13)\n",
    "    ax.legend(loc='upper left', bbox_to_anchor= (1.05, 1.05), title='Stim 2')\n",
    "    ax.set_xlabel('Stim 1', fontsize=13)\n",
    "    ax.xaxis.set_label_position('top') \n",
    "    if filename != '':\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(foldername+filename+'_stim1')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    fig,ax = plt.subplots(1,1)\n",
    "    for loc1 in locs:\n",
    "        df = pd.DataFrame({'first_stim':ind_stim_loc1[ind_stim_loc1==loc1],'second_stim':ind_stim_loc2[ind_stim_loc1==loc1],'activity':Z[ind_stim_loc1==loc1]})\n",
    "        x = df['second_stim']\n",
    "        y = df['activity']\n",
    "        ax.scatter(x,y,s=1, color=palette1[loc1], label=loc1)\n",
    "    ax.set_ylabel('%dth neuron'%neuron, fontsize=13)\n",
    "    ax.legend(loc='upper left', bbox_to_anchor= (1.05, 1.05), title='Stim 1')\n",
    "    ax.set_xlabel('Stim 2', fontsize=13)\n",
    "    ax.xaxis.set_label_position('top') \n",
    "    if filename != '':\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(foldername+filename+'_stim2')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06041da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rnn = 256\n",
    "hp['n_rnn']=n_rnn\n",
    "hp['sigma_rec']=0\n",
    "hp['sigma_x']=0\n",
    "\n",
    "\n",
    "# generate 128*128 input\n",
    "trial = generate_trial(mode='fixed',n_loc=hp['n_loc'])\n",
    "x = trial.x.swapaxes(0,1)\n",
    "print('x shape: ' + str(x.shape))\n",
    "stim_loc_shape = hp['n_loc'],hp['n_loc'],1\n",
    "stim_loc_size = np.prod(stim_loc_shape)\n",
    "ind_stim_loc1, ind_stim_loc2, ind_repeat = np.unravel_index(range(stim_loc_size),stim_loc_shape)\n",
    "\n",
    "# generate 8*8 input\n",
    "n_loc=8\n",
    "trial_ = generate_trial(mode='fixed',n_loc=n_loc)\n",
    "x_ = trial_.x.swapaxes(0,1)\n",
    "print('x_ shape: ' + str(x_.shape))\n",
    "stim_loc_shape_ = n_loc,n_loc,1\n",
    "stim_loc_size_ = np.prod(stim_loc_shape_)\n",
    "ind_stim_loc1_, ind_stim_loc2_, ind_repeat_ = np.unravel_index(range(stim_loc_size_),stim_loc_shape_)\n",
    "\n",
    "# generate 10 noisy samples for each 8*8 input (for anova)\n",
    "X_n = []\n",
    "for i in range(x_.shape[0]):\n",
    "    for j in range(10):\n",
    "        X_n.append(x_[i] + np.random.RandomState(seed).randn(205, 33)*0.01)\n",
    "X_n = np.array(X_n)\n",
    "print('X_n shape: ' + str(X_n.shape))\n",
    "\n",
    "        \n",
    "for acc in [60]:\n",
    "    for seed in range(1,21):\n",
    "\n",
    "        tf.random.set_seed(seed)\n",
    "        hp['seed']=seed\n",
    "        hp['rng'] = np.random.RandomState(seed)\n",
    "\n",
    "        model_folder = '/Users/lowxizhi/Documents/fyp/codes/results/2_stim_batch_size_512/n_hidden_256/2_stim_batch_size_512_n_hidden_256_acc_%s_seed_%s_with_noise'%(acc,seed)\n",
    "        print('model folder: ' + model_folder)\n",
    "        checkpoint_path = model_folder+\"/checkpoint/cp.ckpt\"\n",
    "        isomap_folder = model_folder+'/isomap'\n",
    "        tuning_curve_folder = model_folder+'/tuning_curves/'\n",
    "        weight_matrix_folder = model_folder+'/weight_matrices/'\n",
    "\n",
    "        if not os.path.exists(weight_matrix_folder):\n",
    "            os.makedirs(weight_matrix_folder)\n",
    "            \n",
    "        cell = LeakyRNNCell2(hp)\n",
    "        rnn_layer = tf.keras.layers.RNN(cell,input_shape=((x.shape[1],x.shape[2])),\n",
    "                                        return_sequences=True)\n",
    "        model = create_model(rnn_layer)\n",
    "        model.load_weights(checkpoint_path)\n",
    "        rnn_layer = tf.keras.layers.RNN(cell,input_shape=((x.shape[1],x.shape[2])),\n",
    "                                        return_sequences=True, weights=model.layers[0].get_weights())\n",
    "        weights=model.layers[0].get_weights()\n",
    "        W = weights[1]\n",
    "\n",
    "        print('generating_hidden layer')\n",
    "        hidden = rnn_layer(x) #for linear separability\n",
    "        hidden_ = rnn_layer(x_)\n",
    "        hidden_n = rnn_layer(X_n) #for anova linear/ non-linear mixed selectivity\n",
    "        print('done')\n",
    "\n",
    "        results_dict = {}\n",
    "        for delay in range(1,3):\n",
    "            print('delay:%s'%delay)\n",
    "            delay_bins = get_delay_bins(delay=delay)\n",
    "\n",
    "            #extract mean firing rates for delay bins\n",
    "            delay_hidden = np.mean(hidden[:, delay_bins[0]:delay_bins[1], :], axis=1)\n",
    "            delay_hidden_ = np.mean(hidden_[:, delay_bins[0]:delay_bins[1], :], axis=1)\n",
    "            delay_hidden_n = np.mean(hidden_n[:, delay_bins[0]:delay_bins[1], :], axis=1)\n",
    "\n",
    "            #get weight matrices\n",
    "            ind_stim_locs = {1:ind_stim_loc1_, 2:ind_stim_loc2_}\n",
    "            for stim in [1,2]:\n",
    "                weight_matrix_file = f'delay_{delay}_stim_{stim}_weight_matrix.png'\n",
    "                diag = plot_weight_selectivity_matrix(ind_stim_locs[stim],delay_hidden_,W,n_loc,weight_matrix_folder,weight_matrix_file,stim,delay)\n",
    "                results_dict[f'delay {delay} diag {stim}'] = diag\n",
    "\n",
    "            #get separability and selectivity indices\n",
    "            separability_index_list = []\n",
    "            linear_mixed_selectivity_index_list = []\n",
    "            for neuron in range(256):\n",
    "                A = delay_hidden[:,neuron].reshape(hp['n_loc'],hp['n_loc'])\n",
    "                separability_index = get_separability_index(A)\n",
    "                separability_index_list.append(separability_index)\n",
    "\n",
    "                Z = delay_hidden_n[:,neuron]\n",
    "                df = pd.DataFrame({'first_stim':np.array([[i]*10 for i in ind_stim_loc1_]).flatten(),'second_stim':np.array([[i]*10 for i in ind_stim_loc2_]).flatten(),'activity':Z})\n",
    "                aov = pg.anova(dv='activity', between=['first_stim', 'second_stim'], data=df,\n",
    "                         detailed=True)\n",
    "                linear_mixed_selectivity_index = get_linear_mixed_selectivity_index(aov)\n",
    "                linear_mixed_selectivity_index_list.append(linear_mixed_selectivity_index)\n",
    "\n",
    "                #activity heatmap\n",
    "                plot_activity_heatmap(A,neuron, separability_index, linear_mixed_selectivity_index,tuning_curve_folder,f'delay_{delay}_heatmap_neuron_{neuron}')\n",
    "\n",
    "            df = pd.DataFrame(np.array([separability_index_list,linear_mixed_selectivity_index_list]).T, columns=['linear separability','linear selectivity'])\n",
    "            df.to_csv(tuning_curve_folder+f'delay_{delay}_separability_and_selectivity.csv')\n",
    "            df = df.dropna()\n",
    "\n",
    "            results_dict[f'delay {delay} separability mean'] = df['linear separability'].mean()\n",
    "            results_dict[f'delay {delay} separability std'] = df['linear separability'].std()\n",
    "            results_dict[f'delay {delay} selectivity mean'] = df['linear selectivity'].mean()\n",
    "            results_dict[f'delay {delay} selectivity std'] = df['linear separability'].std()\n",
    "\n",
    "            fig, ax = plt.subplots()\n",
    "            plt.plot(df['linear separability'], label=\"Linear Separability Index\")\n",
    "            plt.plot(df['linear selectivity'], label=\"Linear Mixed Selectivity Index\")\n",
    "            correlation = df['linear selectivity'].corr(df['linear separability'])\n",
    "            plt.legend(loc='upper left', bbox_to_anchor= (1.05, 1.03), title=f'Correlation = {round(correlation,3)}')\n",
    "            ax.set_xlabel('Neurons')\n",
    "            ax.set_ylabel('Index')\n",
    "            ax.set_title('Linear Separability and Linear Mixed Selectivity of Neurons')\n",
    "            fig.savefig(tuning_curve_folder+f'delay_{delay}_separability_and_selectivity_plot.png',bbox_inches='tight')\n",
    "\n",
    "            results_dict[f'delay {delay} selectivity/separability correlation'] = correlation\n",
    "            with open(os.path.join(model_folder, 'results.json'), 'w') as f:\n",
    "                json.dump(results_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dc911b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary = pd.DataFrame()\n",
    "df_indices = []\n",
    "for seed in range(1,21):\n",
    "    acc = 60\n",
    "    n_rnn = 256\n",
    "    model_folder = '/Users/lowxizhi/Documents/fyp/codes/results/2_stim_batch_size_512/n_hidden_256/2_stim_batch_size_512_n_hidden_256_acc_%s_seed_%s_with_noise'%(acc,seed)\n",
    "    tuning_curve_folder = model_folder+'/tuning_curves/'\n",
    "    with open(os.path.join(model_folder, 'results.json'), 'r') as f:\n",
    "        results_dict = json.load(f)\n",
    "    df_summary = df_summary.append(results_dict, ignore_index=True)\n",
    "    df_indices.append(seed)\n",
    "df_summary.index = df_indices\n",
    "df_summary.to_csv(f'{os.path.dirname(model_folder)}/summary_acc_{acc}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baea5416",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98072708",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rnn = 256\n",
    "hp['n_rnn']=n_rnn\n",
    "hp['sigma_rec']=0\n",
    "hp['sigma_x']=0\n",
    "\n",
    "\n",
    "# generate 128*128 input\n",
    "trial = generate_trial(mode='fixed',n_loc=hp['n_loc'])\n",
    "x = trial.x.swapaxes(0,1)\n",
    "# print('x shape: ' + str(x.shape))\n",
    "stim_loc_shape = hp['n_loc'],hp['n_loc'],1\n",
    "stim_loc_size = np.prod(stim_loc_shape)\n",
    "ind_stim_loc1, ind_stim_loc2, ind_repeat = np.unravel_index(range(stim_loc_size),stim_loc_shape)\n",
    "\n",
    "# generate 8*8 input\n",
    "n_loc=8\n",
    "trial_ = generate_trial(mode='fixed',n_loc=n_loc)\n",
    "x_ = trial_.x.swapaxes(0,1)\n",
    "# print('x_ shape: ' + str(x_.shape))\n",
    "stim_loc_shape_ = n_loc,n_loc,1\n",
    "stim_loc_size_ = np.prod(stim_loc_shape_)\n",
    "ind_stim_loc1_, ind_stim_loc2_, ind_repeat_ = np.unravel_index(range(stim_loc_size_),stim_loc_shape_)\n",
    "\n",
    "\n",
    "\n",
    "for acc in [80]:      \n",
    "    for seed in range(17,21):\n",
    "\n",
    "        # generate 10 noisy samples for each 8*8 input (for anova)\n",
    "        X_n = []\n",
    "        for i in range(x_.shape[0]):\n",
    "            for j in range(10):\n",
    "                X_n.append(x_[i] + np.random.RandomState(seed).randn(205, 33)*0.01)\n",
    "        X_n = np.array(X_n)\n",
    "#         print('X_n shape: ' + str(X_n.shape))\n",
    "\n",
    "        tf.random.set_seed(seed)\n",
    "        hp['seed']=seed\n",
    "        hp['rng'] = np.random.RandomState(seed)\n",
    "\n",
    "        model_folder = '/Users/lowxizhi/Documents/fyp/codes/results/2_stim_batch_size_512/n_hidden_%s/2_stim_batch_size_512_n_hidden_%s_acc_%s_seed_%s_with_noise'%(n_rnn,n_rnn,acc,seed)\n",
    "        print('model folder: ' + model_folder)\n",
    "        checkpoint_path = model_folder+\"/checkpoint/cp.ckpt\"\n",
    "        isomap_folder = model_folder+'/isomap'\n",
    "        tuning_curve_folder = model_folder+'/tuning_curves/'\n",
    "        selectivity_folder = model_folder+'/selectivity/'\n",
    "        weight_matrix_folder = model_folder+'/weight_matrices/'\n",
    "        \n",
    "        if not os.path.exists(isomap_folder):\n",
    "            os.makedirs(isomap_folder)\n",
    "        if not os.path.exists(tuning_curve_folder):\n",
    "            os.makedirs(tuning_curve_folder)\n",
    "        if not os.path.exists(selectivity_folder):\n",
    "            os.makedirs(selectivity_folder)\n",
    "        if not os.path.exists(weight_matrix_folder):\n",
    "            os.makedirs(weight_matrix_folder)\n",
    "\n",
    "\n",
    "        cell = LeakyRNNCell2(hp)\n",
    "        rnn_layer = tf.keras.layers.RNN(cell,input_shape=((x.shape[1],x.shape[2])),\n",
    "                                        return_sequences=True)\n",
    "        model = create_model(rnn_layer)\n",
    "        model.load_weights(checkpoint_path)\n",
    "        rnn_layer = tf.keras.layers.RNN(cell,input_shape=((x.shape[1],x.shape[2])),\n",
    "                                        return_sequences=True, weights=model.layers[0].get_weights())\n",
    "        weights=model.layers[0].get_weights()\n",
    "        W = weights[1]\n",
    "\n",
    "        print('generating_hidden layer')\n",
    "        hidden = rnn_layer(x) #for linear separability\n",
    "        hidden_ = rnn_layer(x_) #for linear separability\n",
    "        hidden_n = rnn_layer(X_n) #for anova linear/ non-linear mixed selectivity\n",
    "        print('done')\n",
    "\n",
    "        for delay in range(1,3):\n",
    "            print('delay:%s'%delay)\n",
    "            delay_bins = get_delay_bins(delay=delay)\n",
    "\n",
    "            #extract mean firing rates for delay bins\n",
    "            delay_hidden = np.mean(hidden[:, delay_bins[0]:delay_bins[1], :], axis=1)\n",
    "            delay_hidden_ = np.mean(hidden_[:, delay_bins[0]:delay_bins[1], :], axis=1)\n",
    "            delay_hidden_n = np.mean(hidden_n[:, delay_bins[0]:delay_bins[1], :], axis=1)\n",
    "            sel_and_sep = []\n",
    "            for neuron in range(n_rnn):\n",
    "                print(f\"\\nNeuron: {neuron}\")\n",
    "                Z = delay_hidden_n[:,neuron]\n",
    "                df = pd.DataFrame({'first_stim':np.array([[i]*10 for i in ind_stim_loc1_]).flatten(),'second_stim':np.array([[i]*10 for i in ind_stim_loc2_]).flatten(),'activity':Z})\n",
    "                aov = pg.anova(dv='activity', between=['first_stim', 'second_stim'], data=df,\n",
    "                         detailed=True)\n",
    "        #         print(aov)\n",
    "\n",
    "                if aov.shape == (4,7):\n",
    "                    non_selectivity = is_non_selective(aov)\n",
    "                    classical_selectivity_to_stim1 = is_classical_selective_to_stim1(aov)\n",
    "                    classical_selectivity_to_stim2 = is_classical_selective_to_stim2(aov)\n",
    "                    linear_mixed_selectivity = is_linear_mixed_selective(aov)\n",
    "                    non_linear_mixed_selectivity = is_non_linear_mixed_selective(aov)\n",
    "\n",
    "                else:\n",
    "                    non_selectivity = np.nan\n",
    "                    classical_selectivity_to_stim1 = np.nan\n",
    "                    classical_selectivity_to_stim2 = np.nan\n",
    "                    linear_mixed_selectivity = np.nan\n",
    "                    non_linear_mixed_selectivity = np.nan\n",
    "\n",
    "\n",
    "                A = delay_hidden[:,neuron].reshape(hp['n_loc'],hp['n_loc'])\n",
    "                separability_index = get_separability_index(A)\n",
    "                A_ = delay_hidden_[:,neuron].reshape(n_loc,n_loc)\n",
    "                separability_index_ = get_separability_index(A_)\n",
    "\n",
    "        #                 print('not selective:', non_selectivity)\n",
    "        #                 print('classical selective to stim 1:', classical_selectivity_to_stim1)\n",
    "        #                 print('classical selective to stim 2:', classical_selectivity_to_stim2)\n",
    "        #                 print('linear mixed selective:', linear_mixed_selectivity)\n",
    "        #                 print('non linear mixed selective:', non_linear_mixed_selectivity)\n",
    "        #                 print('separability 8 loc:',separability_index)\n",
    "        #                 print('separability 128 loc:',separability_index_)\n",
    "        #                 print(aov)\n",
    "                aov.to_csv(f'{selectivity_folder}anova_delay_{delay}_neuron_{neuron}.csv')\n",
    "\n",
    "                Z = delay_hidden[:,neuron]\n",
    "                plot_tuning_curves(Z,tuning_curve_folder,f'tuning_curve_delay_{delay}_neuron_{neuron}')\n",
    "\n",
    "                sel_and_sep.append([non_selectivity,classical_selectivity_to_stim1,classical_selectivity_to_stim2,linear_mixed_selectivity,non_linear_mixed_selectivity,separability_index,separability_index_])\n",
    "\n",
    "            sel_and_sep_df = pd.DataFrame(sel_and_sep, columns=['not selective','classical selective to stim 1','classical selective to stim 2','linear mixed selective','non linear mixed selective','separability_128locations','separability_8locations'])\n",
    "            sel_and_sep_df.to_csv(selectivity_folder+f'selectivity_and_separabiltiy_of_hidden_neurons_delay_{delay}.csv')\n",
    "\n",
    "            sel_and_sep_dict = {}\n",
    "            for col in sel_and_sep_df.columns[:5]:\n",
    "                sel_and_sep_dict[col] = str(sel_and_sep_df[col].sum())\n",
    "            print(sel_and_sep_dict)\n",
    "\n",
    "            with open(selectivity_folder+f'selectivity_and_separabiltiy_of_hidden_neurons_summary_delay_{delay}.json', 'w') as f:\n",
    "                json.dump(sel_and_sep_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427683be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for acc in [60]:      \n",
    "    for seed in range(1,21):\n",
    "\n",
    "        # generate 10 noisy samples for each 8*8 input (for anova)\n",
    "        X_n = []\n",
    "        for i in range(x_.shape[0]):\n",
    "            for j in range(10):\n",
    "                X_n.append(x_[i] + np.random.RandomState(seed).randn(205, 33)*0.01)\n",
    "        X_n = np.array(X_n)\n",
    "#         print('X_n shape: ' + str(X_n.shape))\n",
    "\n",
    "        tf.random.set_seed(seed)\n",
    "        hp['seed']=seed\n",
    "        hp['rng'] = np.random.RandomState(seed)\n",
    "\n",
    "        model_folder = '/Users/lowxizhi/Documents/fyp/codes/results/2_stim_batch_size_512/n_hidden_%s/2_stim_batch_size_512_n_hidden_%s_acc_%s_seed_%s_with_noise'%(n_rnn,n_rnn,acc,seed)\n",
    "        print('model folder: ' + model_folder)\n",
    "        checkpoint_path = model_folder+\"/checkpoint/cp.ckpt\"\n",
    "        isomap_folder = model_folder+'/isomap'\n",
    "        tuning_curve_folder = model_folder+'/tuning_curves/'\n",
    "        selectivity_folder = model_folder+'/selectivity/'\n",
    "        weight_matrix_folder = model_folder+'/weight_matrices/'\n",
    "        \n",
    "        if not os.path.exists(isomap_folder):\n",
    "            os.makedirs(isomap_folder)\n",
    "        if not os.path.exists(tuning_curve_folder):\n",
    "            os.makedirs(tuning_curve_folder)\n",
    "        if not os.path.exists(selectivity_folder):\n",
    "            os.makedirs(selectivity_folder)\n",
    "        if not os.path.exists(weight_matrix_folder):\n",
    "            os.makedirs(weight_matrix_folder)\n",
    "\n",
    "\n",
    "        cell = LeakyRNNCell2(hp)\n",
    "        rnn_layer = tf.keras.layers.RNN(cell,input_shape=((x.shape[1],x.shape[2])),\n",
    "                                        return_sequences=True)\n",
    "        model = create_model(rnn_layer)\n",
    "        model.load_weights(checkpoint_path)\n",
    "        rnn_layer = tf.keras.layers.RNN(cell,input_shape=((x.shape[1],x.shape[2])),\n",
    "                                        return_sequences=True, weights=model.layers[0].get_weights())\n",
    "        weights=model.layers[0].get_weights()\n",
    "        W = weights[1]\n",
    "\n",
    "        print('generating_hidden layer')\n",
    "        hidden = rnn_layer(x) #for linear separability\n",
    "        hidden_ = rnn_layer(x_) #for linear separability\n",
    "        hidden_n = rnn_layer(X_n) #for anova linear/ non-linear mixed selectivity\n",
    "        print('done')\n",
    "\n",
    "        for delay in range(1,3):\n",
    "            print('delay:%s'%delay)\n",
    "            delay_bins = get_delay_bins(delay=delay)\n",
    "\n",
    "            #extract mean firing rates for delay bins\n",
    "            delay_hidden = np.mean(hidden[:, delay_bins[0]:delay_bins[1], :], axis=1)\n",
    "            delay_hidden_ = np.mean(hidden_[:, delay_bins[0]:delay_bins[1], :], axis=1)\n",
    "            delay_hidden_n = np.mean(hidden_n[:, delay_bins[0]:delay_bins[1], :], axis=1)\n",
    "            sel_and_sep = []\n",
    "            for neuron in range(n_rnn):\n",
    "                print(f\"\\nNeuron: {neuron}\")\n",
    "                Z = delay_hidden_n[:,neuron]\n",
    "                df = pd.DataFrame({'first_stim':np.array([[i]*10 for i in ind_stim_loc1_]).flatten(),'second_stim':np.array([[i]*10 for i in ind_stim_loc2_]).flatten(),'activity':Z})\n",
    "                aov = pg.anova(dv='activity', between=['first_stim', 'second_stim'], data=df,\n",
    "                         detailed=True)\n",
    "        #         print(aov)\n",
    "\n",
    "                if aov.shape == (4,7):\n",
    "                    non_selectivity = is_non_selective(aov)\n",
    "                    classical_selectivity_to_stim1 = is_classical_selective_to_stim1(aov)\n",
    "                    classical_selectivity_to_stim2 = is_classical_selective_to_stim2(aov)\n",
    "                    linear_mixed_selectivity = is_linear_mixed_selective(aov)\n",
    "                    non_linear_mixed_selectivity = is_non_linear_mixed_selective(aov)\n",
    "\n",
    "                else:\n",
    "                    non_selectivity = np.nan\n",
    "                    classical_selectivity_to_stim1 = np.nan\n",
    "                    classical_selectivity_to_stim2 = np.nan\n",
    "                    linear_mixed_selectivity = np.nan\n",
    "                    non_linear_mixed_selectivity = np.nan\n",
    "\n",
    "\n",
    "                A = delay_hidden[:,neuron].reshape(hp['n_loc'],hp['n_loc'])\n",
    "                separability_index = get_separability_index(A)\n",
    "                A_ = delay_hidden_[:,neuron].reshape(n_loc,n_loc)\n",
    "                separability_index_ = get_separability_index(A_)\n",
    "\n",
    "        #                 print('not selective:', non_selectivity)\n",
    "        #                 print('classical selective to stim 1:', classical_selectivity_to_stim1)\n",
    "        #                 print('classical selective to stim 2:', classical_selectivity_to_stim2)\n",
    "        #                 print('linear mixed selective:', linear_mixed_selectivity)\n",
    "        #                 print('non linear mixed selective:', non_linear_mixed_selectivity)\n",
    "        #                 print('separability 8 loc:',separability_index)\n",
    "        #                 print('separability 128 loc:',separability_index_)\n",
    "        #                 print(aov)\n",
    "                aov.to_csv(f'{selectivity_folder}anova_delay_{delay}_neuron_{neuron}.csv')\n",
    "\n",
    "                Z = delay_hidden[:,neuron]\n",
    "                plot_tuning_curves(Z,tuning_curve_folder,f'tuning_curve_delay_{delay}_neuron_{neuron}')\n",
    "\n",
    "                sel_and_sep.append([non_selectivity,classical_selectivity_to_stim1,classical_selectivity_to_stim2,linear_mixed_selectivity,non_linear_mixed_selectivity,separability_index,separability_index_])\n",
    "\n",
    "            sel_and_sep_df = pd.DataFrame(sel_and_sep, columns=['not selective','classical selective to stim 1','classical selective to stim 2','linear mixed selective','non linear mixed selective','separability_128locations','separability_8locations'])\n",
    "            sel_and_sep_df.to_csv(selectivity_folder+f'selectivity_and_separabiltiy_of_hidden_neurons_delay_{delay}.csv')\n",
    "\n",
    "            sel_and_sep_dict = {}\n",
    "            for col in sel_and_sep_df.columns[:5]:\n",
    "                sel_and_sep_dict[col] = str(sel_and_sep_df[col].sum())\n",
    "            print(sel_and_sep_dict)\n",
    "\n",
    "            with open(selectivity_folder+f'selectivity_and_separabiltiy_of_hidden_neurons_summary_delay_{delay}.json', 'w') as f:\n",
    "                json.dump(sel_and_sep_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0fc79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rnn = 256\n",
    "hp['n_rnn']=n_rnn\n",
    "hp['sigma_rec']=0\n",
    "hp['sigma_x']=0\n",
    "\n",
    "\n",
    "# generate 128*128 input\n",
    "trial = generate_trial(mode='fixed',n_loc=hp['n_loc'])\n",
    "x = trial.x.swapaxes(0,1)\n",
    "print('x shape: ' + str(x.shape))\n",
    "stim_loc_shape = hp['n_loc'],hp['n_loc'],1\n",
    "stim_loc_size = np.prod(stim_loc_shape)\n",
    "ind_stim_loc1, ind_stim_loc2, ind_repeat = np.unravel_index(range(stim_loc_size),stim_loc_shape)\n",
    "\n",
    "# generate 8*8 input\n",
    "n_loc=8\n",
    "trial_ = generate_trial(mode='fixed',n_loc=n_loc)\n",
    "x_ = trial_.x.swapaxes(0,1)\n",
    "print('x_ shape: ' + str(x_.shape))\n",
    "stim_loc_shape_ = n_loc,n_loc,1\n",
    "stim_loc_size_ = np.prod(stim_loc_shape_)\n",
    "ind_stim_loc1_, ind_stim_loc2_, ind_repeat_ = np.unravel_index(range(stim_loc_size_),stim_loc_shape_)\n",
    "\n",
    "num_samples = 250\n",
    "\n",
    "\n",
    "# for acc in [95,80,60]:      \n",
    "#     for seed in range(1,21):\n",
    "for acc in [95]:      \n",
    "    for seed in range(19,20):\n",
    "\n",
    "        # generate 10 noisy samples for each 8*8 input (for anova)\n",
    "        X_n = []\n",
    "        for i in range(x_.shape[0]):\n",
    "            for j in range(num_samples):\n",
    "                X_n.append(x_[i] + np.random.RandomState(seed).randn(205, 33)*0.01)\n",
    "        X_n = np.array(X_n)\n",
    "        print('X_n shape: ' + str(X_n.shape))\n",
    "\n",
    "        tf.random.set_seed(seed)\n",
    "        hp['seed']=seed\n",
    "        hp['rng'] = np.random.RandomState(seed)\n",
    "\n",
    "        model_folder = '/Users/lowxizhi/Documents/fyp/codes/results/2_stim_batch_size_512/n_hidden_%s/2_stim_batch_size_512_n_hidden_%s_acc_%s_seed_%s_with_noise'%(n_rnn,n_rnn,acc,seed)\n",
    "        print('model folder: ' + model_folder)\n",
    "        checkpoint_path = model_folder+\"/checkpoint/cp.ckpt\"\n",
    "        isomap_folder = model_folder+'/isomap'\n",
    "        tuning_curve_folder = model_folder+'/tuning_curves/'\n",
    "        selectivity_folder = model_folder+'/selectivity/'\n",
    "        weight_matrix_folder = model_folder+'/weight_matrices/'\n",
    "        \n",
    "        if not os.path.exists(isomap_folder):\n",
    "            os.makedirs(isomap_folder)\n",
    "        if not os.path.exists(tuning_curve_folder):\n",
    "            os.makedirs(tuning_curve_folder)\n",
    "        if not os.path.exists(selectivity_folder):\n",
    "            os.makedirs(selectivity_folder)\n",
    "        if not os.path.exists(weight_matrix_folder):\n",
    "            os.makedirs(weight_matrix_folder)\n",
    "\n",
    "\n",
    "        cell = LeakyRNNCell2(hp)\n",
    "        rnn_layer = tf.keras.layers.RNN(cell,input_shape=((x.shape[1],x.shape[2])),\n",
    "                                        return_sequences=True)\n",
    "        model = create_model(rnn_layer)\n",
    "        model.load_weights(checkpoint_path)\n",
    "        rnn_layer = tf.keras.layers.RNN(cell,input_shape=((x.shape[1],x.shape[2])),\n",
    "                                        return_sequences=True, weights=model.layers[0].get_weights())\n",
    "        weights=model.layers[0].get_weights()\n",
    "        W = weights[1]\n",
    "\n",
    "        print('generating_hidden layer')\n",
    "        hidden = rnn_layer(x) #for linear separability\n",
    "        hidden_ = rnn_layer(x_) #for linear separability\n",
    "        hidden_n = rnn_layer(X_n) #for anova linear/ non-linear mixed selectivity\n",
    "        print('done')\n",
    "\n",
    "#         for delay in range(1,2):\n",
    "#             print('delay:%s'%delay)\n",
    "#             delay_bins = get_delay_bins(delay=delay)\n",
    "\n",
    "#             #extract mean firing rates for delay bins\n",
    "#             delay_hidden = np.mean(hidden[:, delay_bins[0]:delay_bins[1], :], axis=1)\n",
    "#             delay_hidden_ = np.mean(hidden_[:, delay_bins[0]:delay_bins[1], :], axis=1)\n",
    "#             delay_hidden_n = np.mean(hidden_n[:, delay_bins[0]:delay_bins[1], :], axis=1)\n",
    "#             sel_and_sep = []\n",
    "#             for neuron in range(n_rnn):\n",
    "#                 print(f\"\\nNeuron: {neuron}\")\n",
    "#                 Z = delay_hidden_n[:,neuron]\n",
    "#                 df = pd.DataFrame({'first_stim':np.array([[i]*num_samples for i in ind_stim_loc1_]).flatten(),'second_stim':np.array([[i]*num_samples for i in ind_stim_loc2_]).flatten(),'activity':Z})\n",
    "#                 aov = pg.anova(dv='activity', between=['first_stim', 'second_stim'], data=df,\n",
    "#                          detailed=True)\n",
    "#         #         print(aov)\n",
    "\n",
    "#                 if aov.shape == (4,7):\n",
    "#                     non_selectivity = is_non_selective(aov)\n",
    "#                     classical_selectivity_to_stim1 = is_classical_selective_to_stim1(aov)\n",
    "#                     classical_selectivity_to_stim2 = is_classical_selective_to_stim2(aov)\n",
    "#                     linear_mixed_selectivity = is_linear_mixed_selective(aov)\n",
    "#                     non_linear_mixed_selectivity = is_non_linear_mixed_selective(aov)\n",
    "\n",
    "#                 else:\n",
    "#                     non_selectivity = np.nan\n",
    "#                     classical_selectivity_to_stim1 = np.nan\n",
    "#                     classical_selectivity_to_stim2 = np.nan\n",
    "#                     linear_mixed_selectivity = np.nan\n",
    "#                     non_linear_mixed_selectivity = np.nan\n",
    "\n",
    "\n",
    "#                 A = delay_hidden[:,neuron].reshape(hp['n_loc'],hp['n_loc'])\n",
    "#                 separability_index = get_separability_index(A)\n",
    "#                 A_ = delay_hidden_[:,neuron].reshape(n_loc,n_loc)\n",
    "#                 separability_index_ = get_separability_index(A_)\n",
    "\n",
    "#                 print('not selective:', non_selectivity)\n",
    "#                 print('classical selective to stim 1:', classical_selectivity_to_stim1)\n",
    "#                 print('classical selective to stim 2:', classical_selectivity_to_stim2)\n",
    "#                 print('linear mixed selective:', linear_mixed_selectivity)\n",
    "#                 print('non linear mixed selective:', non_linear_mixed_selectivity)\n",
    "#                 print('separability 8 loc:',separability_index)\n",
    "#                 print('separability 128 loc:',separability_index_)\n",
    "#                 print(aov)\n",
    "# #                 aov.to_csv(f'{selectivity_folder}anova_neuron_{neuron}.csv')\n",
    "\n",
    "#                 Z = delay_hidden[:,neuron]\n",
    "#                 plot_tuning_curves(Z,tuning_curve_folder)\n",
    "\n",
    "#                 sel_and_sep.append([non_selectivity,classical_selectivity_to_stim1,classical_selectivity_to_stim2,linear_mixed_selectivity,non_linear_mixed_selectivity,separability_index,separability_index_])\n",
    "\n",
    "#             sel_and_sep_df = pd.DataFrame(sel_and_sep, columns=['not selective','classical selective to stim 1','classical selective to stim 2','linear mixed selective','non linear mixed selective','separability_128locations','separability_8locations'])\n",
    "#         #     sel_and_sep_df.to_csv(selectivity_folder+f'selectivity_and_separabiltiy_of_hidden_neurons_delay_{delay}.csv')\n",
    "\n",
    "#             sel_and_sep_dict = {}\n",
    "#             for col in sel_and_sep_df.columns[:5]:\n",
    "#                 sel_and_sep_dict[col] = str(sel_and_sep_df[col].sum())\n",
    "#             print(sel_and_sep_dict)\n",
    "\n",
    "#         #     with open(selectivity_folder+f'selectivity_and_separabiltiy_of_hidden_neurons_summary_delay_{delay}.json', 'w') as f:\n",
    "#         #         json.dump(sel_and_sep_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851c487b",
   "metadata": {},
   "outputs": [],
   "source": [
    "delay_hidden_n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718c8798",
   "metadata": {},
   "outputs": [],
   "source": [
    "250*64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc635b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "for delay in range(1,2):\n",
    "    print('delay:%s'%delay)\n",
    "    delay_bins = get_delay_bins(delay=delay)\n",
    "\n",
    "    #extract mean firing rates for delay bins\n",
    "    delay_hidden = np.mean(hidden[:, delay_bins[0]:delay_bins[1], :], axis=1)\n",
    "    delay_hidden_ = np.mean(hidden_[:, delay_bins[0]:delay_bins[1], :], axis=1)\n",
    "    delay_hidden_n = np.mean(hidden_n[:, delay_bins[0]:delay_bins[1], :], axis=1)\n",
    "    sel_and_sep = []\n",
    "#     for neuron in range(n_rnn):\n",
    "    for neuron in [124,125,206]:\n",
    "        print(f\"\\nNeuron: {neuron}\")\n",
    "        Z = delay_hidden_n[:,neuron]\n",
    "        df = pd.DataFrame({'first_stim':np.array([[i]*num_samples for i in ind_stim_loc1_]).flatten(),'second_stim':np.array([[i]*num_samples for i in ind_stim_loc2_]).flatten(),'activity':Z})\n",
    "        \n",
    "        aov = pg.anova(dv='activity', between=['first_stim', 'second_stim'], data=df,\n",
    "                 detailed=True)\n",
    "        print('\\nboth stim:')\n",
    "        print(aov)\n",
    "        \n",
    "        for j in range(n_loc):\n",
    "            df1 = df[df['second_stim']==j]\n",
    "            df2 = df[df['first_stim']==j]\n",
    "            \n",
    "            aov1 = pg.anova(dv='activity', between=['first_stim'], data=df1,\n",
    "                     detailed=True)\n",
    "            aov2 = pg.anova(dv='activity', between=['second_stim'], data=df2,\n",
    "                     detailed=True)\n",
    "            \n",
    "            print(f'\\n\\n\\nstim1 when second_stim={j}:')\n",
    "            print(aov1)\n",
    "            print(f'\\nstim2 when first_stim={j}:')\n",
    "            print(aov2)\n",
    "            \n",
    "        \n",
    "\n",
    "#         if aov.shape == (4,7):\n",
    "#             non_selectivity = is_non_selective(aov)\n",
    "#             classical_selectivity_to_stim1 = is_classical_selective_to_stim1(aov)\n",
    "#             classical_selectivity_to_stim2 = is_classical_selective_to_stim2(aov)\n",
    "#             linear_mixed_selectivity = is_linear_mixed_selective(aov)\n",
    "#             non_linear_mixed_selectivity = is_non_linear_mixed_selective(aov)\n",
    "\n",
    "#         else:\n",
    "#             non_selectivity = np.nan\n",
    "#             classical_selectivity_to_stim1 = np.nan\n",
    "#             classical_selectivity_to_stim2 = np.nan\n",
    "#             linear_mixed_selectivity = np.nan\n",
    "#             non_linear_mixed_selectivity = np.nan\n",
    "\n",
    "\n",
    "#         A = delay_hidden[:,neuron].reshape(hp['n_loc'],hp['n_loc'])\n",
    "#         separability_index = get_separability_index(A)\n",
    "#         A_ = delay_hidden_[:,neuron].reshape(n_loc,n_loc)\n",
    "#         separability_index_ = get_separability_index(A_)\n",
    "\n",
    "#         print('not selective:', non_selectivity)\n",
    "#         print('classical selective to stim 1:', classical_selectivity_to_stim1)\n",
    "#         print('classical selective to stim 2:', classical_selectivity_to_stim2)\n",
    "#         print('linear mixed selective:', linear_mixed_selectivity)\n",
    "#         print('non linear mixed selective:', non_linear_mixed_selectivity)\n",
    "#         print('separability 8 loc:',separability_index)\n",
    "#         print('separability 128 loc:',separability_index_)\n",
    "#         print(aov)\n",
    "# #                 aov.to_csv(f'{selectivity_folder}anova_neuron_{neuron}.csv')\n",
    "\n",
    "        Z = delay_hidden_[:,neuron]\n",
    "        plot_tuning_curves(Z,n_loc,ind_stim_loc1_,ind_stim_loc2_,tuning_curve_folder)\n",
    "\n",
    "#         sel_and_sep.append([non_selectivity,classical_selectivity_to_stim1,classical_selectivity_to_stim2,linear_mixed_selectivity,non_linear_mixed_selectivity,separability_index,separability_index_])\n",
    "\n",
    "#     sel_and_sep_df = pd.DataFrame(sel_and_sep, columns=['not selective','classical selective to stim 1','classical selective to stim 2','linear mixed selective','non linear mixed selective','separability_128locations','separability_8locations'])\n",
    "# #     sel_and_sep_df.to_csv(selectivity_folder+f'selectivity_and_separabiltiy_of_hidden_neurons_delay_{delay}.csv')\n",
    "\n",
    "#     sel_and_sep_dict = {}\n",
    "#     for col in sel_and_sep_df.columns[:5]:\n",
    "#         sel_and_sep_dict[col] = str(sel_and_sep_df[col].sum())\n",
    "#     print(sel_and_sep_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7af67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for delay in range(1,2):\n",
    "    print('delay:%s'%delay)\n",
    "    delay_bins = get_delay_bins(delay=delay)\n",
    "\n",
    "    #extract mean firing rates for delay bins\n",
    "    delay_hidden = np.mean(hidden[:, delay_bins[0]:delay_bins[1], :], axis=1)\n",
    "    delay_hidden_ = np.mean(hidden_[:, delay_bins[0]:delay_bins[1], :], axis=1)\n",
    "    delay_hidden_n = np.mean(hidden_n[:, delay_bins[0]:delay_bins[1], :], axis=1)\n",
    "    sel_and_sep = []\n",
    "#     for neuron in range(n_rnn):\n",
    "    for neuron in [124,125,206]:\n",
    "        print(f\"\\nNeuron: {neuron}\")\n",
    "        Z = delay_hidden_n[:,neuron]\n",
    "        df = pd.DataFrame({'first_stim':np.array([[i]*num_samples for i in ind_stim_loc1_]).flatten(),'second_stim':np.array([[i]*num_samples for i in ind_stim_loc2_]).flatten(),'activity':Z})\n",
    "        \n",
    "        aov = pg.anova(dv='activity', between=['first_stim', 'second_stim'], data=df,\n",
    "                 detailed=True)\n",
    "        print('\\nboth stim:')\n",
    "        print(aov)\n",
    "        \n",
    "        for j in range(n_loc):\n",
    "            df1 = df[df['second_stim']==j]\n",
    "            df2 = df[df['first_stim']==j]\n",
    "            \n",
    "            aov1 = pg.anova(dv='activity', between=['first_stim'], data=df1,\n",
    "                     detailed=True)\n",
    "            aov2 = pg.anova(dv='activity', between=['second_stim'], data=df2,\n",
    "                     detailed=True)\n",
    "            \n",
    "            print(f'\\n\\n\\nstim1 when second_stim={j}:')\n",
    "            print(aov1)\n",
    "            print(f'\\nstim2 when first_stim={j}:')\n",
    "            print(aov2)\n",
    "            \n",
    "        Z = delay_hidden[:,neuron]\n",
    "        plot_tuning_curves(Z,n_loc,ind_stim_loc1,ind_stim_loc2,tuning_curve_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89380c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "-2.653898e-04/49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a842f76b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df['second_stim']==8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc94ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232d5dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_and_sep_df[sel_and_sep_df['linear mixed selective']==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a080d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f295db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rnn=256\n",
    "for acc in [60]:   \n",
    "    for delay in range(2,3):\n",
    "        summary_sel_and_sep_df = pd.DataFrame(columns=['not selective', 'classical selective to stim 1', 'classical selective to stim 2', 'linear mixed selective', 'non linear mixed selective'])\n",
    "        for seed in range(1,21):\n",
    "            model_folder = '/Users/lowxizhi/Documents/fyp/codes/results/2_stim_batch_size_512/n_hidden_%s/2_stim_batch_size_512_n_hidden_%s_acc_%s_seed_%s_with_noise'%(n_rnn,n_rnn,acc,seed)\n",
    "            print('model folder: ' + model_folder)\n",
    "            # checkpoint_path = model_folder+\"/checkpoint/cp.ckpt\"\n",
    "            # isomap_folder = model_folder+'/isomap'\n",
    "            tuning_curve_folder = model_folder+'/tuning_curves/'\n",
    "            selectivity_folder = model_folder+'/selectivity/'\n",
    "            with open(selectivity_folder+f'selectivity_and_separabiltiy_of_hidden_neurons_summary_delay_{delay}.json', 'r') as f:\n",
    "                sel_and_sep_dict = json.load(f)\n",
    "            sel_and_sep_df = pd.read_csv(selectivity_folder+f'selectivity_and_separabiltiy_of_hidden_neurons_delay_{delay}.csv',index_col=0)\n",
    "            print(sel_and_sep_dict)\n",
    "            summary_sel_and_sep_df = summary_sel_and_sep_df.append(pd.DataFrame.from_dict(sel_and_sep_dict, orient='index').T)\n",
    "            for neuron in sel_and_sep_df[sel_and_sep_df.isna().any(axis=1)].index:\n",
    "                aov = pd.read_csv(f'{selectivity_folder}anova_delay_{delay}_neuron_{neuron}.csv',index_col=0)\n",
    "                print(aov)\n",
    "                img = mpimg.imread(tuning_curve_folder+f'tuning_curve_delay_{delay}_neuron_{neuron}_stim1.png')\n",
    "                imgplot = plt.imshow(img)\n",
    "                plt.show()\n",
    "                img = mpimg.imread(tuning_curve_folder+f'tuning_curve_delay_{delay}_neuron_{neuron}_stim2.png')\n",
    "                imgplot = plt.imshow(img)\n",
    "                plt.show()\n",
    "        summary_sel_and_sep_df.index = np.arange(1,21)\n",
    "        summary_sel_and_sep_df.to_csv(os.path.dirname(model_folder)+f'/summary_sel_and_sep_df_delay_{delay}_acc_{acc}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d76f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_and_sep_df[sel_and_sep_df.isna().any(axis=1)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca7779c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rnn=128\n",
    "for acc in [60]:   \n",
    "    for delay in range(1,2):\n",
    "        summary_sel_and_sep_df = pd.DataFrame(columns=['not selective', 'classical selective to stim 1', 'classical selective to stim 2', 'linear mixed selective', 'non linear mixed selective'])\n",
    "        for seed in range(1,21):\n",
    "            model_folder = '/Users/lowxizhi/Documents/fyp/codes/results/2_stim_batch_size_512/n_hidden_%s/2_stim_batch_size_512_n_hidden_%s_acc_%s_seed_%s_with_noise'%(n_rnn,n_rnn,acc,seed)\n",
    "            print('model folder: ' + model_folder)\n",
    "            # checkpoint_path = model_folder+\"/checkpoint/cp.ckpt\"\n",
    "            # isomap_folder = model_folder+'/isomap'\n",
    "            tuning_curve_folder = model_folder+'/tuning_curves/'\n",
    "            selectivity_folder = model_folder+'/selectivity/'\n",
    "            with open(selectivity_folder+f'selectivity_and_separabiltiy_of_hidden_neurons_summary_delay_{delay}.json', 'r') as f:\n",
    "                sel_and_sep_dict = json.load(f)\n",
    "            sel_and_sep_df = pd.read_csv(selectivity_folder+f'selectivity_and_separabiltiy_of_hidden_neurons_delay_{delay}.csv',index_col=0)\n",
    "#             print(sel_and_sep_df[sel_and_sep_df['classical selective to stim 1']!=True])\n",
    "            print(sel_and_sep_dict)\n",
    "            summary_sel_and_sep_df = summary_sel_and_sep_df.append(pd.DataFrame.from_dict(sel_and_sep_dict, orient='index').T)\n",
    "            for neuron in sel_and_sep_df[sel_and_sep_df['classical selective to stim 1']!=True].index:\n",
    "                aov = pd.read_csv(f'{selectivity_folder}anova_delay_{delay}_neuron_{neuron}.csv',index_col=0)\n",
    "                print(aov)\n",
    "                img = mpimg.imread(tuning_curve_folder+f'tuning_curve_delay_{delay}_neuron_{neuron}_stim1.png')\n",
    "                imgplot = plt.imshow(img)\n",
    "                plt.show()\n",
    "                img = mpimg.imread(tuning_curve_folder+f'tuning_curve_delay_{delay}_neuron_{neuron}_stim2.png')\n",
    "                imgplot = plt.imshow(img)\n",
    "                plt.show()\n",
    "        summary_sel_and_sep_df.index = np.arange(1,21)\n",
    "        summary_sel_and_sep_df.to_csv(os.path.dirname(model_folder)+f'/summary_sel_and_sep_df_delay_{delay}_acc_{acc}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08669cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_sel_and_sep_df_delay_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261289f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3a2bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for acc in [95,80,60]:      \n",
    "    for seed in range(1,21):\n",
    "        for delay in range(1,3):\n",
    "            \n",
    "            aov.to_csv(f'{selectivity_folder}anova_delay_{delay}_neuron_{neuron}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b626090",
   "metadata": {},
   "outputs": [],
   "source": [
    "for delay in range(1,2):\n",
    "    print('delay:%s'%delay)\n",
    "    delay_bins = get_delay_bins(delay=delay)\n",
    "\n",
    "    #extract mean firing rates for delay bins\n",
    "    delay_hidden = np.mean(hidden[:, delay_bins[0]:delay_bins[1], :], axis=1)\n",
    "    delay_hidden_ = np.mean(hidden_[:, delay_bins[0]:delay_bins[1], :], axis=1)\n",
    "    delay_hidden_n = np.mean(hidden_n[:, delay_bins[0]:delay_bins[1], :], axis=1)\n",
    "    sel_and_sep = []\n",
    "    for neuron in range(256):\n",
    "        print(f\"\\nNeuron: {neuron}\")\n",
    "        Z = delay_hidden_n[:,neuron]\n",
    "        df = pd.DataFrame({'first_stim':np.array([[i]*10 for i in ind_stim_loc1_]).flatten(),'second_stim':np.array([[i]*10 for i in ind_stim_loc2_]).flatten(),'activity':Z})\n",
    "        aov = pg.anova(dv='activity', between=['first_stim', 'second_stim'], data=df,\n",
    "                 detailed=True)\n",
    "#         print(aov)\n",
    "\n",
    "        if aov.shape == (4,7):\n",
    "            non_selectivity = is_non_selective(aov)\n",
    "            classical_selectivity_to_stim1 = is_classical_selective_to_stim1(aov)\n",
    "            classical_selectivity_to_stim2 = is_classical_selective_to_stim2(aov)\n",
    "            linear_mixed_selectivity = is_linear_mixed_selective(aov)\n",
    "            non_linear_mixed_selectivity = is_non_linear_mixed_selective(aov)\n",
    "\n",
    "        else:\n",
    "            non_selectivity = np.nan\n",
    "            classical_selectivity_to_stim1 = np.nan\n",
    "            classical_selectivity_to_stim2 = np.nan\n",
    "            linear_mixed_selectivity = np.nan\n",
    "            non_linear_mixed_selectivity = np.nan\n",
    "\n",
    "\n",
    "        A = delay_hidden[:,neuron].reshape(hp['n_loc'],hp['n_loc'])\n",
    "        separability_index = get_separability_index(A)\n",
    "        A_ = delay_hidden_[:,neuron].reshape(n_loc,n_loc)\n",
    "        separability_index_ = get_separability_index(A_)\n",
    "        \n",
    "        print('not selective:', non_selectivity)\n",
    "        print('classical selective to stim 1:', classical_selectivity_to_stim1)\n",
    "        print('classical selective to stim 2:', classical_selectivity_to_stim2)\n",
    "        print('linear mixed selective:', linear_mixed_selectivity)\n",
    "        print('non linear mixed selective:', non_linear_mixed_selectivity)\n",
    "        print('separability 8 loc:',separability_index)\n",
    "        print('separability 128 loc:',separability_index_)\n",
    "        print(aov)\n",
    "        aov.to_csv(f'{selectivity_folder}anova_neuron_{neuron}.csv')\n",
    "\n",
    "        Z = delay_hidden[:,neuron]\n",
    "        plot_tuning_curves(Z,tuning_curve_folder)\n",
    "\n",
    "        sel_and_sep.append([non_selectivity,classical_selectivity_to_stim1,classical_selectivity_to_stim2,linear_mixed_selectivity,non_linear_mixed_selectivity,separability_index,separability_index_])\n",
    "\n",
    "    sel_and_sep_df = pd.DataFrame(sel_and_sep, columns=['not selective','classical selective to stim 1','classical selective to stim 2','linear mixed selective','non linear mixed selective','separability_128locations','separability_8locations'])\n",
    "#     sel_and_sep_df.to_csv(selectivity_folder+f'selectivity_and_separabiltiy_of_hidden_neurons_delay_{delay}.csv')\n",
    "\n",
    "    sel_and_sep_dict = {}\n",
    "    for col in sel_and_sep_df.columns[:5]:\n",
    "        sel_and_sep_dict[col] = str(sel_and_sep_df[col].sum())\n",
    "    print(sel_and_sep_dict)\n",
    "\n",
    "#     with open(selectivity_folder+f'selectivity_and_separabiltiy_of_hidden_neurons_summary_delay_{delay}.json', 'w') as f:\n",
    "#         json.dump(sel_and_sep_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143e865e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    sel_and_sep_dict = {}\n",
    "    for col in sel_and_sep_df.columns[:5]:\n",
    "        sel_and_sep_dict[col] = str(sel_and_sep_df[col].sum())\n",
    "    print(sel_and_sep_dict)\n",
    "\n",
    "    with open(selectivity_folder+f'selectivity_and_separabiltiy_of_hidden_neurons_summary_delay_{delay}.json', 'w') as f:\n",
    "        json.dump(sel_and_sep_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f9e383",
   "metadata": {},
   "outputs": [],
   "source": [
    "delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193ce3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = delay_hidden_[:,neuron].reshape(n_loc,n_loc)\n",
    "separability_index = get_separability_index(A)\n",
    "print('separability:',separability_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60712de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/lowxizhi/Documents/fyp/codes/results/2_stim_batch_size_512/n_hidden_256/2_stim_batch_size_512_n_hidden_256_acc_95_seed_19_with_noise/tuning_curves/delay_2_separability_and_selectivity.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d4d3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_sep_neurons = list(np.argsort(df['linear separability']))\n",
    "max_sep_neuron = sorted_sep_neurons[-1]\n",
    "min_sep_neuron = sorted_sep_neurons[0]\n",
    "\n",
    "sorted_sel_neurons = list(np.argsort(df['linear selectivity']))\n",
    "max_sel_neuron = sorted_sel_neurons[-1]\n",
    "min_sel_neuron = sorted_sel_neurons[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2870df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron = max_sep_neuron\n",
    "print('neuron: ', neuron)\n",
    "print('linear separability: ', df['linear separability'][neuron])\n",
    "print('linear selectivity: ', df['linear selectivity'][neuron])\n",
    "Z = delay_hidden[:,neuron]\n",
    "plot_tuning_curves(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea796138",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron = min_sep_neuron\n",
    "print('neuron: ', neuron)\n",
    "print('linear separability: ', df['linear separability'][neuron])\n",
    "print('linear selectivity: ', df['linear selectivity'][neuron])\n",
    "Z = delay_hidden[:,neuron]\n",
    "plot_tuning_curves(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6348fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron = max_sel_neuron\n",
    "print('neuron: ', neuron)\n",
    "print('linear separability: ', df['linear separability'][neuron])\n",
    "print('linear selectivity: ', df['linear selectivity'][neuron])\n",
    "Z = delay_hidden[:,neuron]\n",
    "plot_tuning_curves(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b222fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron = min_sel_neuron\n",
    "print('neuron: ', neuron)\n",
    "print('linear separability: ', df['linear separability'][neuron])\n",
    "print('linear selectivity: ', df['linear selectivity'][neuron])\n",
    "Z = delay_hidden[:,neuron]\n",
    "plot_tuning_curves(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bec04be",
   "metadata": {},
   "source": [
    "### consolidate all isomaps into a isomap summary folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5166b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rnn = 64\n",
    "bs = 64\n",
    "isomap_summary_folder = f'/Volumes/Seagate Backup Plus Drive/fyp/results/2_stim_batch_size_{bs}/n_hidden_{n_rnn}/isomaps/'\n",
    "for acc in [60,80,95]:\n",
    "    isomap_dest_folder = isomap_summary_folder+f'acc_{acc}/'\n",
    "    if not os.path.exists(isomap_dest_folder):\n",
    "        os.makedirs(isomap_dest_folder)\n",
    "    for seed in range(1,21):\n",
    "        model_folder = f'/Volumes/Seagate Backup Plus Drive/fyp/results/2_stim_batch_size_{bs}/n_hidden_{n_rnn}/2_stim_batch_size_{bs}_n_hidden_{n_rnn}_acc_{acc}_seed_{seed}_with_noise/'\n",
    "        isomap_folder = model_folder+'isomap/'\n",
    "        if os.path.exists(isomap_folder):\n",
    "            for file in os.listdir(isomap_folder):\n",
    "                isomap_dest_file = isomap_dest_folder+file.split('.')[0]+f'_{seed}.'+file.split('.')[1]\n",
    "                shutil.copyfile(isomap_folder+file, isomap_dest_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb420caa",
   "metadata": {},
   "source": [
    "### plot isomaps in performance progression\n",
    "- seed 2 and 11 have non torus in lower performance and torus in higher performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fa71e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "isomap_summary_folder = '/Volumes/Seagate Backup Plus Drive/fyp/results/2_stim_batch_size_512/n_hidden_256/isomaps/'\n",
    "for file in os.listdir(isomap_folder)[4:]:\n",
    "    for start in [1,6,11,16]:\n",
    "        fig,ax = plt.subplots(5,3, figsize=(15,25))\n",
    "        for i, seed in enumerate(range(start,start+5)):\n",
    "            for j, acc in enumerate([60,80,95]):\n",
    "                isomap_dest_folder = isomap_summary_folder+f'acc_{acc}/'\n",
    "                isomap_dest_file = isomap_dest_folder+file.split('.')[0]+f'_{seed}.'+file.split('.')[1]\n",
    "                # reading png image\n",
    "                im = img.imread(isomap_dest_file)\n",
    "                ax[i,j].imshow(im)\n",
    "                ax[i,j].axis('off')\n",
    "        #         if i == 0:\n",
    "        #             ax[i,j].set_title(f'Accuracy ~ {acc}%',fontsize=20)\n",
    "        fig.savefig(isomap_summary_folder+f'summary_{start}to{start+5}_{file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2b1da7",
   "metadata": {},
   "source": [
    "### plot all isomaps in performance progression for run 2 and 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c94908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "\n",
    "for seed in [2,11]:\n",
    "    isomap_summary_folder = '/Volumes/Seagate Backup Plus Drive/fyp/results/2_stim_batch_size_512/n_hidden_256/isomaps/'\n",
    "    fig,ax = plt.subplots(4,3, figsize=(15,20))\n",
    "    for i, file in enumerate(['trained_delay2_hidden_single_distractor.png','trained_delay2_hidden_single_target.png','trained_delay2_hidden_target_isomap.png','trained_delay2_hidden_distractor_isomap.png']):\n",
    "        for j, acc in enumerate([60,80,95]):\n",
    "            isomap_dest_folder = isomap_summary_folder+f'acc_{acc}/'\n",
    "            isomap_dest_file = isomap_dest_folder+file.split('.')[0]+f'_{seed}.'+file.split('.')[1]\n",
    "            # reading png image\n",
    "            im = img.imread(isomap_dest_file)\n",
    "            ax[i,j].imshow(im)\n",
    "            ax[i,j].axis('off')\n",
    "    #         if i == 0:\n",
    "    #             ax[i,j].set_title(f'Accuracy ~ {acc}%',fontsize=20)\n",
    "    fig.savefig(isomap_summary_folder+f'summary_{seed}_isomaps.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de68b14",
   "metadata": {},
   "source": [
    "### Learning Curve for runs 2 and 11\n",
    "- steep drop then continuous rise in accuracy for run 11 when it finds torus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4a8380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "learning_curve_summary_folder = '/Volumes/Seagate Backup Plus Drive/fyp/results/2_stim_batch_size_512/n_hidden_256/learning_curves/'\n",
    "\n",
    "for seed in range(1,21):\n",
    "    yvalues = []\n",
    "    for j, acc in enumerate([60,80,95]):\n",
    "        model_folder = f'/Volumes/Seagate Backup Plus Drive/fyp/results/2_stim_batch_size_512/n_hidden_256/2_stim_batch_size_512_n_hidden_256_acc_{acc}_seed_{seed}_with_noise/'\n",
    "        df = pd.read_csv(model_folder+'log.csv')\n",
    "        yvalues.append(df.custom_response_accuracy)\n",
    "    yvalues = list(pd.concat(yvalues))\n",
    "    xvalues = np.arange(len(yvalues))\n",
    "    yvalues = [yvalues[i] for i in range(len(yvalues)) if xvalues[i]%500==0]\n",
    "    xvalues = [xvalues[i] for i in range(len(xvalues)) if xvalues[i]%500==0]\n",
    "    \n",
    "    fig,ax = plt.subplots(1,1, figsize=(10,7))\n",
    "    ax.plot(xvalues,yvalues)\n",
    "    plt.xticks(fontsize= 10)\n",
    "    plt.yticks(fontsize= 10)\n",
    "    ax.set_xlabel('Epochs', fontsize='15')\n",
    "    ax.set_ylabel('Accuracy', fontsize='15')\n",
    "    \n",
    "    if not os.path.exists(learning_curve_summary_folder):\n",
    "        os.makedirs(learning_curve_summary_folder)\n",
    "    fig.savefig(learning_curve_summary_folder+f'learning_curve_{seed}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79851798",
   "metadata": {},
   "source": [
    "### Model performances for different n_rnn. Pick 10 runs with >85% to compare\n",
    "\n",
    "- 256 (80): 1,2,3,4,5,6,7,8,9,10\n",
    "- 128 (95): 2,3,5,6,7,10,13,14,18,20\n",
    "- 64 (95): 2,3,5,7,11,13,14,17,18,20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4882efef",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rnn=256\n",
    "for acc in [95,80,60]:\n",
    "    print('acc: ', acc)\n",
    "    for seed in range(1,21):\n",
    "        model_folder = '/Volumes/Seagate Backup Plus Drive/fyp/results/2_stim_batch_size_512/n_hidden_%s/2_stim_batch_size_512_n_hidden_%s_acc_%s_seed_%s_with_noise'%(n_rnn,n_rnn,acc,seed)\n",
    "        if os.path.exists(model_folder+f'/performance.json'):\n",
    "            with open(model_folder+f'/performance.json', 'r') as f:\n",
    "                performance_dict = json.load(f)\n",
    "                print(seed, performance_dict['trained accuracy on 16384 trials'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263f4df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rnn=128\n",
    "for acc in [95,80,60]:\n",
    "    print('acc: ', acc)\n",
    "    for seed in range(1,21):\n",
    "        model_folder = '/Users/lowxizhi/Documents/fyp/codes/results/2_stim_batch_size_512/n_hidden_%s/2_stim_batch_size_512_n_hidden_%s_acc_%s_seed_%s_with_noise'%(n_rnn,n_rnn,acc,seed)\n",
    "        with open(model_folder+f'/performance.json', 'r') as f:\n",
    "            performance_dict = json.load(f)\n",
    "            print(seed, performance_dict['trained accuracy on 16384 trials'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0da7cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rnn=64\n",
    "for acc in [95,80,60]:\n",
    "    print('acc: ', acc)\n",
    "    for seed in range(1,21):\n",
    "        model_folder = '/Users/lowxizhi/Documents/fyp/codes/results/2_stim_batch_size_64/n_hidden_%s/2_stim_batch_size_64_n_hidden_%s_acc_%s_seed_%s_with_noise'%(n_rnn,n_rnn,acc,seed)\n",
    "        if os.path.exists(model_folder+f'/performance.json'):\n",
    "            with open(model_folder+f'/performance.json', 'r') as f:\n",
    "                performance_dict = json.load(f)\n",
    "                print(seed, performance_dict['trained accuracy on 16384 trials'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11eb1e0",
   "metadata": {},
   "source": [
    "- 256 (80): 1,2,3,4,5,6,7,8,9,10\n",
    "- 128 (95): 2,3,5,6,7,10,13,14,18,20\n",
    "- 64 (95): 2,3,5,7,11,13,14,17,18,20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce9ca9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "\n",
    "acc=95\n",
    "n_rnn=64\n",
    "bs=64\n",
    "run_list = [13,14,17,18,20]\n",
    "fig,ax = plt.subplots(5,4, figsize=(15,20))\n",
    "for i, seed in enumerate(run_list):\n",
    "    isomap_summary_folder = f'/Volumes/Seagate Backup Plus Drive/fyp/results/2_stim_batch_size_{bs}/n_hidden_{n_rnn}/isomaps/'\n",
    "    for j, file in enumerate(['trained_delay2_hidden_single_distractor.png','trained_delay2_hidden_single_target.png','trained_delay2_hidden_target_isomap.png','trained_delay2_hidden_distractor_isomap.png']):\n",
    "        isomap_dest_folder = isomap_summary_folder+f'acc_{acc}/'\n",
    "        isomap_dest_file = isomap_dest_folder+file.split('.')[0]+f'_{seed}.'+file.split('.')[1]\n",
    "        im = img.imread(isomap_dest_file)\n",
    "        ax[i,j].imshow(im)\n",
    "        ax[i,j].axis('off')\n",
    "\n",
    "    fig.savefig(isomap_summary_folder+f'summary_acc_{acc}_seed_{str(run_list)}_isomaps.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625e63f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_rnn=64\n",
    "bs = 64\n",
    "n_rnn = 256\n",
    "for acc in [95,80,60]:\n",
    "    print('acc: ', acc)\n",
    "    for seed in range(1,21):\n",
    "        model_folder = f'/Volumes/Seagate Backup Plus Drive/fyp/results/3_stim_batch_size_{bs}/n_hidden_{n_rnn}/3_stim_batch_size_{bs}_n_hidden_{n_rnn}_acc_{acc}_seed_{seed}_with_noise/'\n",
    "        if os.path.exists(model_folder+f'/performance.json'):\n",
    "            with open(model_folder+f'/performance.json', 'r') as f:\n",
    "                performance_dict = json.load(f)\n",
    "                print(seed, performance_dict['trained accuracy on 32768 trials'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4eea8e2",
   "metadata": {},
   "source": [
    "### Neurons with high separability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f6bfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for neuron in sorted_sep_neurons[-10:]:\n",
    "    print('neuron: ', neuron)\n",
    "    print('linear separability: ', df['linear separability'][neuron])\n",
    "    print('linear selectivity: ', df['linear selectivity'][neuron])\n",
    "    Z = delay_hidden[:,neuron]\n",
    "    plot_tuning_curves(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fb62e3",
   "metadata": {},
   "source": [
    "### Neurons with low separability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e392d65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for neuron in sorted_sep_neurons[:10]:\n",
    "    print('neuron: ', neuron)\n",
    "    print('linear separability: ', df['linear separability'][neuron])\n",
    "    print('linear selectivity: ', df['linear selectivity'][neuron])\n",
    "    Z = delay_hidden[:,neuron]\n",
    "    plot_tuning_curves(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b06ce82",
   "metadata": {},
   "source": [
    "### Neurons with high selectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78303bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for neuron in sorted_sel_neurons[-10:]:\n",
    "    print('neuron: ', neuron)\n",
    "    print('linear separability: ', df['linear separability'][neuron])\n",
    "    print('linear selectivity: ', df['linear selectivity'][neuron])\n",
    "    Z = delay_hidden[:,neuron]\n",
    "    plot_tuning_curves(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86767ed8",
   "metadata": {},
   "source": [
    "### Neurons with low selectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ca6023",
   "metadata": {},
   "outputs": [],
   "source": [
    "for neuron in sorted_sel_neurons[:10]:\n",
    "    print('neuron: ', neuron)\n",
    "    print('linear separability: ', df['linear separability'][neuron])\n",
    "    print('linear selectivity: ', df['linear selectivity'][neuron])\n",
    "    Z = delay_hidden[:,neuron]\n",
    "    plot_tuning_curves(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7c9012",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron = df[df['linear selectivity']==df['linear selectivity'].max()].index[0]\n",
    "print('neuron: ', neuron)\n",
    "print('linear separability: ', df['linear separability'][neuron])\n",
    "print('linear selectivity: ', df['linear selectivity'][neuron])\n",
    "Z = delay_hidden[:,neuron]\n",
    "plot_tuning_curves(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20c6a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron = df[df['linear selectivity']==df['linear selectivity'].min()].index[0]\n",
    "print('neuron: ', neuron)\n",
    "print('linear separability: ', df['linear separability'][neuron])\n",
    "print('linear selectivity: ', df['linear selectivity'][neuron])\n",
    "Z = delay_hidden[:,neuron]\n",
    "plot_tuning_curves(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeecae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for neuron in sorted_sel_neurons[-4:-2]:\n",
    "    print('neuron: ', neuron)\n",
    "    print('linear separability: ', df['linear separability'][neuron])\n",
    "    print('linear selectivity: ', df['linear selectivity'][neuron])\n",
    "    Z = delay_hidden[:,neuron]\n",
    "    plot_tuning_curves(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7d70bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b861d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sin(x):\n",
    "    return math.sin(x)\n",
    "def cos(x):\n",
    "    return math.cos(x)\n",
    "def sin2(x):\n",
    "    return math.sin(2*x)\n",
    "def cos2(x):\n",
    "    return math.cos(2*x)\n",
    "def sin05(x):\n",
    "    return math.sin(0.5*x)\n",
    "def cos05(x):\n",
    "    return math.cos(0.5*x)\n",
    "\n",
    "\n",
    "# x = np.arange(100)\n",
    "# y = np.arange(100)\n",
    "\n",
    "# h = [[0]]*100\n",
    "# for i in range(100):\n",
    "#     for j in range(100):\n",
    "#         h[i][j] = g(x[i])*f(x[i]) + a(x[i])*b(x[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8a7eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=0\n",
    "# generate 128*128 input\n",
    "trial = generate_trial(mode='fixed',n_loc=hp['n_loc'])\n",
    "x = trial.x.swapaxes(0,1)\n",
    "print('x shape: ' + str(x.shape))\n",
    "stim_loc_shape = hp['n_loc'],hp['n_loc'],1\n",
    "stim_loc_size = np.prod(stim_loc_shape)\n",
    "ind_stim_loc1, ind_stim_loc2, ind_repeat = np.unravel_index(range(stim_loc_size),stim_loc_shape)\n",
    "\n",
    "# generate 8*8 input\n",
    "n_loc=8\n",
    "trial_ = generate_trial(mode='fixed',n_loc=n_loc)\n",
    "x_ = trial_.x.swapaxes(0,1)\n",
    "print('x_ shape: ' + str(x_.shape))\n",
    "stim_loc_shape_ = n_loc,n_loc,1\n",
    "stim_loc_size_ = np.prod(stim_loc_shape_)\n",
    "ind_stim_loc1_, ind_stim_loc2_, ind_repeat_ = np.unravel_index(range(stim_loc_size_),stim_loc_shape_)\n",
    "\n",
    "# generate 10 noisy samples for each 8*8 input (for anova)\n",
    "X_n = []\n",
    "for i in range(x_.shape[0]):\n",
    "    for j in range(10):\n",
    "        X_n.append(x_[i] + np.random.RandomState(seed).randn(205, 33)*0.01)\n",
    "X_n = np.array(X_n)\n",
    "print('X_n shape: ' + str(X_n.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81517917",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_stim_loc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494ad3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_loc=128\n",
    "\n",
    "stim_loc_shape = n_loc,n_loc,1\n",
    "stim_loc_size = np.prod(stim_loc_shape)\n",
    "ind_stim_loc1, ind_stim_loc2, ind_repeat = np.unravel_index(range(stim_loc_size),stim_loc_shape)\n",
    "\n",
    "ind_stim_loc1_n = []\n",
    "ind_stim_loc2_n = []\n",
    "for i in range(stim_loc_size):\n",
    "    noise1 = np.random.RandomState().rand()*0.01\n",
    "    for j in range(10):\n",
    "        ind_stim_loc1_n.append(ind_stim_loc1[i]+np.random.RandomState().rand()*0.001)\n",
    "        ind_stim_loc2_n.append(ind_stim_loc2[i]+np.random.RandomState().rand()*0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b15de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_non_selective(aov):\n",
    "    p1 = aov.iloc[0,5]\n",
    "    p2 = aov.iloc[1,5]\n",
    "    p1p2 = aov.iloc[2,5]\n",
    "    if p1 >= 0.05 and p2 >= 0.05 and p1p2>=0.05:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def is_classical_selective_to_stim1(aov):\n",
    "    p1 = aov.iloc[0,5]\n",
    "    ms_1 = aov.iloc[0,3]\n",
    "    sum_ms = aov.iloc[:,3].loc[aov.iloc[:,5]<0.05].sum()\n",
    "\n",
    "    if p1<0.05 and ms_1/sum_ms > 0.99:\n",
    "#     if p1 < 0.05 and p2 >= 0.05 and p1p2>=0.05:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def is_classical_selective_to_stim2(aov):\n",
    "    p2 = aov.iloc[1,5]\n",
    "    ms_2 = aov.iloc[1,3]\n",
    "    sum_ms = aov.iloc[:,3].loc[aov.iloc[:,5]<0.05].sum()\n",
    "    if p2<0.05 and ms_2/sum_ms > 0.99:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def is_linear_mixed_selective(aov):\n",
    "    p1 = aov.iloc[0,5]\n",
    "    p2 = aov.iloc[1,5]\n",
    "    ms_1 = aov.iloc[0,3]\n",
    "    ms_2 = aov.iloc[1,3]\n",
    "    sum_ms = aov.iloc[:,3].loc[aov.iloc[:,5]<0.05].sum()\n",
    "    if not is_classical_selective_to_stim1(aov) and not is_classical_selective_to_stim2(aov) and p1<0.05 and p2<0.05 and (ms_1+ms_2)/sum_ms>0.99:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def is_non_linear_mixed_selective(aov):\n",
    "    p1p2 = aov.iloc[2,5]\n",
    "    if not is_non_selective(aov) and not is_classical_selective_to_stim1(aov) and not is_classical_selective_to_stim2(aov) and not is_linear_mixed_selective(aov) and p1p2<0.05:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def is_separable(separability_index):\n",
    "    if separability_index==1:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92301407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tuning_curves(Z,foldername='',filename=''):\n",
    "    fig,ax = fig,ax = plt.subplots(1,1)\n",
    "#     for loc2 in range(hp['n_loc']):\n",
    "    for loc2 in [j*40 for j in range(int(hp['n_loc']/40))]:\n",
    "        df = pd.DataFrame({'first_stim':ind_stim_loc1[ind_stim_loc2==loc2],'second_stim':ind_stim_loc2[ind_stim_loc2==loc2],'activity':Z[ind_stim_loc2==loc2]})\n",
    "        x = df['first_stim']\n",
    "        y = df['activity']\n",
    "        ax.plot(x,y, color=palette2[loc2], label=loc2)\n",
    "#     ax.set_ylabel('%dth neuron'%neuron, fontsize=13)\n",
    "    ax.legend(loc='upper left', bbox_to_anchor= (1.05, 1.05), title='Stim 2')\n",
    "    ax.set_xlabel('Stim 1', fontsize=13)\n",
    "    ax.xaxis.set_label_position('top') \n",
    "    if filename != '':\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(foldername+filename+'_stim1')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    fig,ax = plt.subplots(1,1)\n",
    "#     for loc1 in range(hp['n_loc']):\n",
    "    for loc1 in [j*40 for j in range(int(hp['n_loc']/40))]:\n",
    "        df = pd.DataFrame({'first_stim':ind_stim_loc1[ind_stim_loc1==loc1],'second_stim':ind_stim_loc2[ind_stim_loc1==loc1],'activity':Z[ind_stim_loc1==loc1]})\n",
    "        x = df['second_stim']\n",
    "        y = df['activity']\n",
    "        ax.plot(x,y, color=palette1[loc1], label=loc1)\n",
    "#     ax.set_ylabel('%dth neuron'%neuron, fontsize=13)\n",
    "    ax.legend(loc='upper left', bbox_to_anchor= (1.05, 1.05), title='Stim 1')\n",
    "    ax.set_xlabel('Stim 2', fontsize=13)\n",
    "    ax.xaxis.set_label_position('top') \n",
    "    if filename != '':\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(foldername+filename+'_stim2')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77890395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataframe(df_):\n",
    "    df = df_.copy()\n",
    "    \n",
    "    df['x1*x2'] = df['x1']*df['x2']\n",
    "    \n",
    "    df['sin(x1)'] = df['x1'].apply(sin)\n",
    "    df['cos(x2)'] = df['x2'].apply(cos)\n",
    "    df['sin(x2)'] = df['x2'].apply(sin)\n",
    "    df['cos(x1)'] = df['x1'].apply(cos)\n",
    "    \n",
    "    df['sin(2*x1)'] = df['x1'].apply(sin2)\n",
    "    df['cos(2*x2)'] = df['x2'].apply(cos2)\n",
    "    df['sin(2*x2)'] = df['x2'].apply(sin2)\n",
    "    df['cos(2*x1)'] = df['x1'].apply(cos2)\n",
    "    \n",
    "    df['sin(x1)+cos(x2)'] = df['x1'].apply(sin)+df['x2'].apply(cos)\n",
    "    df['sin(2*x1)+cos(2*x2)'] = df['x1'].apply(sin2)+df['x2'].apply(cos2)\n",
    "    df['sin(x1)+cos(2*x2)'] = df['x1'].apply(sin)+df['x2'].apply(cos2)\n",
    "    df['sin(2*x1)+cos(x2)'] = df['x1'].apply(sin2)+df['x2'].apply(cos)\n",
    "    \n",
    "    df['sin(x1)*cos(x2)'] = df['x1'].apply(sin)*df['x2'].apply(cos)\n",
    "    df['sin(2*x1)*cos(2*x2)'] = df['x1'].apply(sin2)*df['x2'].apply(cos2)\n",
    "    df['sin(x1)*cos(2*x2)'] = df['sin(x1)']*df['cos(2*x2)']\n",
    "    df['sin(x2)*cos(2*x1)'] = df['sin(x2)']+df['cos(2*x1)']\n",
    "    df['sin(2*x1)*cos(x2)'] = df['sin(2*x1)']*df['cos(x2)']\n",
    "    df['sin(2*x2)*cos(x1)'] = df['sin(2*x2)']*df['cos(x1)']\n",
    "    \n",
    "    df['sin(x1)+sin(x1)*cos(x2)'] = df['sin(x1)'] + df['sin(x1)*cos(x2)']\n",
    "    df['sin(2*x1)+sin(x1)*cos(x2)'] = df['sin(2*x1)'] + df['sin(x1)*cos(x2)']\n",
    "    df['1+sin(x1)*cos(x2)'] = 1 + df['sin(x1)*cos(x2)']\n",
    "    df['100+sin(x1)*cos(x2)'] = 100 + df['sin(x1)*cos(x2)']\n",
    "    df['sin(x1)+cos(x2)+sin(x1)*cos(x2)'] = df['sin(x1)+cos(x2)'] + df['sin(x1)*cos(x2)']\n",
    "    df['sin(2*x1)+cos(2*x2)+sin(2*x1)*cos(2*x2)'] = df['sin(2*x1)+cos(2*x2)'] + df['sin(2*x1)*cos(2*x2)']\n",
    "    \n",
    "    df['sin(x1)*cos(2*x2) + sin(2*x1)*cos(x2)'] = df['sin(x1)*cos(2*x2)'] + df['sin(2*x1)*cos(x2)']\n",
    "    df['sin(x2)*cos(2*x1) + sin(2*x2)*cos(x1)'] = df['sin(x2)*cos(2*x1)'] + df['sin(2*x2)*cos(x1)']\n",
    "    df['sin(x1)*cos(2*x2) + sin(2*x1)*cos(x2) + sin(x2)*cos(2*x1)'] = df['sin(x1)*cos(2*x2)'] + df['sin(2*x1)*cos(x2)'] + df['sin(x2)*cos(2*x1)']\n",
    "    df['sin(x1)*cos(2*x2) + sin(2*x1)*cos(x2) + sin(x2)*cos(2*x1) + sin(2*x2)*cos(x1)'] = df['sin(x1)*cos(2*x2)'] + df['sin(2*x1)*cos(x2)'] + df['sin(x2)*cos(2*x1)'] + + df['sin(2*x2)*cos(x1)']\n",
    "    \n",
    "    df['sin(x1*x2)'] = df['x1*x2'].apply(sin)\n",
    "    df['cos(x1*x2)'] = df['x1*x2'].apply(cos)\n",
    "    df['sin(2*x1*x2)'] = df['x1*x2'].apply(sin2)\n",
    "    df['cos(2*x1*x2)'] = df['x1*x2'].apply(cos2)\n",
    "    return df\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['first_stim'] = np.array([[i]*10 for i in ind_stim_loc1]).flatten()\n",
    "df['second_stim'] = np.array([[i]*10 for i in ind_stim_loc2]).flatten()\n",
    "df['x1'] = [(ind_stim_loc1_n_/n_loc)*4*np.pi for ind_stim_loc1_n_ in ind_stim_loc1_n]\n",
    "df['x2'] = [(ind_stim_loc2_n_/n_loc)*4*np.pi for ind_stim_loc2_n_ in ind_stim_loc2_n]\n",
    "df = generate_dataframe(df)\n",
    "\n",
    "df1 = pd.DataFrame()\n",
    "df1['x1'] = [(ind_stim_loc1_/n_loc)*4*np.pi for ind_stim_loc1_ in ind_stim_loc1]\n",
    "df1['x2'] = [(ind_stim_loc2_/n_loc)*4*np.pi for ind_stim_loc2_ in ind_stim_loc2]\n",
    "df1 = generate_dataframe(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a55585",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df.columns)):\n",
    "    print(i, df.columns[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a1dbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for activity in [df.columns[col] for col in [5,6,13,17,23,24,25,26,27,29,30,31,32,33]]:\n",
    "    print('\\n'+activity)\n",
    "    aov = pg.anova(dv=activity, between=['first_stim', 'second_stim'], data=df,\n",
    "             detailed=True)\n",
    "    print(aov)\n",
    "#     for selectivity_type in ['pure_selectivity_to_1', 'pure_selectivity_to_2', 'linear_mixed_selectivity', 'non_linear_mixed_selectivity']:\n",
    "#         print(selectivity_type, ' selectivity:', round(get_selectivity_index(aov, selectivity_type),3), ' p:', round(get_anova_p_value(aov, selectivity_type),3))\n",
    "        \n",
    "#     A = np.array(df1[activity]).reshape(n_loc,n_loc)\n",
    "#     separability_index = get_separability_index(A)\n",
    "#     print('separability',separability_index)\n",
    "    \n",
    "    print('pure selective to stim 1:', is_classical_selective_to_stim1(aov))\n",
    "    print('pure selective to stim 2:', is_classical_selective_to_stim2(aov))\n",
    "    print('linear mixed selective:', is_linear_mixed_selective(aov))\n",
    "    print('non linear mixed selective:', is_non_linear_mixed_selective(aov))\n",
    "#     if is_non_linear_mixed_selective(aov):\n",
    "    A = np.array(df1[activity]).reshape(n_loc,n_loc)\n",
    "    separability_index = get_separability_index(A)\n",
    "    print('separability',separability_index)\n",
    "    print('separability',is_separable(separability_index))\n",
    "        \n",
    "    plot_tuning_curves(df1[activity])\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f440bc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test separability measure\n",
    "- add constant\n",
    "- add sin(x)\n",
    "- add sin(x)+cos(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb7f7c6",
   "metadata": {},
   "source": [
    "## use ring activity as input to functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b93636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_x_loc(x_loc, n_eachring = 32):\n",
    "    \"\"\"Input activity given location.\"\"\"\n",
    "    pref  = np.arange(0,2*np.pi,2*np.pi/n_eachring)\n",
    "    dist = get_dist(x_loc-pref)  # periodic boundary\n",
    "    dist /= np.pi/8\n",
    "    return 0.8*np.exp(-dist**2/2)\n",
    "\n",
    "def add_x_noise(x, seed=0,sigma_x = 0.01):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    x += rng.randn(len(x))*sigma_x\n",
    "    return x\n",
    "\n",
    "def get_dist(original_dist):\n",
    "    '''Get the distance in periodic boundary conditions'''\n",
    "    return np.minimum(abs(original_dist),2*np.pi-abs(original_dist))\n",
    "\n",
    "n_loc = 8\n",
    "n_stim_loc1, n_stim_loc2, repeat = stim_loc_shape = n_loc, n_loc, 1\n",
    "stim_loc_size = np.prod(stim_loc_shape)\n",
    "ind_stim_loc1, ind_stim_loc2, ind_repeat = np.unravel_index(range(stim_loc_size),stim_loc_shape)\n",
    "stim1_locs = 2*np.pi*ind_stim_loc1/n_stim_loc1\n",
    "stim2_locs = 2*np.pi*ind_stim_loc2/n_stim_loc2\n",
    "\n",
    "X = []\n",
    "for i in range(stim_loc_size):\n",
    "    for i in range(10):\n",
    "        x1 = add_x_loc(stim1_locs[i])\n",
    "        x1 = add_x_noise(x1)\n",
    "        x2 = add_x_loc(stim2_locs[i])\n",
    "        x2 = add_x_noise(x2)\n",
    "        X.append(np.append(x1,x2))\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfa0dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
