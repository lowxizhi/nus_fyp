{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c6a677",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import decomposition, manifold\n",
    "from matplotlib import cm\n",
    "import scipy.io\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from scipy.stats import ortho_group\n",
    "import argparse\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TimeDistributed, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.python.ops.rnn_cell_impl import RNNCell\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "n_eachring = 32\n",
    "n_input, n_output = 1+n_eachring, 1+n_eachring\n",
    "batch_size_train = 64\n",
    "hp = {\n",
    "        # batch size for training\n",
    "        'batch_size_train': batch_size_train,\n",
    "        # batch size for validation\n",
    "        'batch_size_val': batch_size_train,\n",
    "        # Type of loss functions\n",
    "        'loss_type': 'lsq',\n",
    "        # Optimizer\n",
    "        'optimizer': 'adam',\n",
    "        # Type of activation runctions, relu, softplus, tanh, elu\n",
    "        'activation': 'relu',\n",
    "        # Time constant (ms)\n",
    "        'tau': 100,\n",
    "        # discretization time step (ms)\n",
    "        'dt': 20,\n",
    "        # discretization time step/time constant\n",
    "        'alpha': 0.2,\n",
    "        # recurrent noise\n",
    "#         'sigma_rec': 0.05,\n",
    "        'sigma_rec': 0,\n",
    "        # input noise\n",
    "#         'sigma_x': 0.01,\n",
    "        'sigma_x': 0,\n",
    "        # leaky_rec weight initialization, diag, randortho, randgauss\n",
    "        'w_rec_init': 'randortho',\n",
    "        # a default weak regularization prevents instability\n",
    "        'l1_h': 0,\n",
    "        # l2 regularization on activity\n",
    "        'l2_h': 0,\n",
    "        # l2 regularization on weight\n",
    "        'l1_weight': 0,\n",
    "        # l2 regularization on weight\n",
    "        'l2_weight': 0,\n",
    "        # l2 regularization on deviation from initialization\n",
    "        'l2_weight_init': 0,\n",
    "        # Stopping performance\n",
    "        'target_perf': 1.,\n",
    "        # number of units each ring\n",
    "        'n_eachring': n_eachring,\n",
    "        # first input index for rule units\n",
    "        'rule_start': 1+n_eachring,\n",
    "        # number of input units\n",
    "        'n_input': n_input,\n",
    "        # number of output units\n",
    "        'n_output': n_output,\n",
    "        # number of recurrent units\n",
    "        'n_rnn': 256,\n",
    "        # learning rate\n",
    "        'learning_rate': 0.0001,\n",
    "        # number of display epochs\n",
    "        'steps_per_epoch': 1,\n",
    "        # number of fixed locations for isomap\n",
    "        'n_loc': 128,\n",
    "        # accuracy threshold to stop training\n",
    "        'accuracy_threshold': 0.9,\n",
    "        }\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "n_loc = 128\n",
    "n_stim_loc1, n_stim_loc2, repeat = stim_loc_shape = n_loc, n_loc, 1\n",
    "stim_loc_size = np.prod(stim_loc_shape)\n",
    "ind_stim_loc1, ind_stim_loc2, ind_repeat = np.unravel_index(range(stim_loc_size),stim_loc_shape)\n",
    "stim1_locs = 2*np.pi*ind_stim_loc1/n_stim_loc1\n",
    "stim2_locs = 2*np.pi*ind_stim_loc2/n_stim_loc2\n",
    "\n",
    "palette1 = cm.get_cmap('autumn',n_loc+15)\n",
    "palette1 = [palette1(i)[:3] for i in range(n_loc)]\n",
    "palette2 = cm.get_cmap('summer',n_loc+15)\n",
    "palette2 = [palette2(i)[:3] for i in range(n_loc)]\n",
    "\n",
    "color1=np.array(palette1)[ind_stim_loc1]\n",
    "color2=np.array(palette2)[ind_stim_loc2]\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "class LeakyRNNCell2(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self,hp,**kwargs):\n",
    "\n",
    "        self.units = hp['n_rnn']\n",
    "        self.state_size = hp['n_rnn']\n",
    "        \n",
    "        activation = hp['activation']\n",
    "        if activation == 'softplus':\n",
    "            self._activation = tf.nn.softplus\n",
    "            self._w_in_start = 1.0\n",
    "            self._w_rec_start = 0.5\n",
    "        elif activation == 'tanh':\n",
    "            self._activation = tf.tanh\n",
    "            self._w_in_start = 1.0\n",
    "            self._w_rec_start = 1.0\n",
    "        elif activation == 'relu':\n",
    "            self._activation = tf.nn.relu\n",
    "            self._w_in_start = 1.0\n",
    "            self._w_rec_start = 0.5\n",
    "        elif activation == 'power':\n",
    "            self._activation = lambda x: tf.square(tf.nn.relu(x))\n",
    "            self._w_in_start = 1.0\n",
    "            self._w_rec_start = 0.01\n",
    "        elif activation == 'retanh':\n",
    "            self._activation = lambda x: tf.tanh(tf.nn.relu(x))\n",
    "            self._w_in_start = 1.0\n",
    "            self._w_rec_start = 0.5\n",
    "        else:\n",
    "            raise ValueError('Unknown activation')\n",
    "            \n",
    "\n",
    "        self.rng = hp['rng']\n",
    "        self.seed = hp['seed']\n",
    "        self._alpha = hp['alpha']\n",
    "        self._sigma = np.sqrt(2 / self._alpha) * hp['sigma_rec']\n",
    "        \n",
    "        super(LeakyRNNCell2, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        w_in0 = (self.rng.randn(input_shape[-1], self.units) /\n",
    "                 np.sqrt(input_shape[-1]) * self._w_in_start)\n",
    "        self.kernel = self.add_weight(shape=(input_shape[-1], self.units), dtype=tf.float32,\n",
    "                                      initializer=tf.constant_initializer(w_in0),\n",
    "                                      name='kernel')\n",
    "        w_rec0 = self._w_rec_start*ortho_group.rvs(dim=self.units, random_state=self.rng)\n",
    "        self.recurrent_kernel = self.add_weight(shape=(self.units, self.units), dtype=tf.float32,\n",
    "                                                initializer=tf.constant_initializer(w_rec0),\n",
    "                                                name='recurrent_kernel')\n",
    "        matrix0 = np.concatenate((w_in0, w_rec0), axis=0)\n",
    "    \n",
    "#         self.kernel = self.add_weight(\n",
    "#                 name='kernel',\n",
    "#                 shape=[input_shape[-1] + self.units, self.units], \n",
    "#                 dtype=tf.float32,\n",
    "#                 initializer=tf.constant_initializer(matrix0))\n",
    "        \n",
    "        self._bias = self.add_weight(\n",
    "                name='bias',\n",
    "                shape=[self.units],\n",
    "                dtype=tf.float32,\n",
    "                initializer=tf.zeros_initializer())\n",
    "        \n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, states):\n",
    "        prev_output = states[0]\n",
    "#         h = tf.keras.backend.dot(array_ops.concat([inputs, prev_output], 1), self.kernel)\n",
    "        h = tf.keras.backend.dot(inputs, self.kernel)\n",
    "        h = h + tf.keras.backend.dot(prev_output, self.recurrent_kernel)\n",
    "        h = tf.nn.bias_add(h, self._bias)\n",
    "        noise = tf.random.normal(tf.shape(prev_output), mean=0, stddev=self._sigma, seed=self.seed)\n",
    "        h = h + noise\n",
    "        output = self._activation(h)\n",
    "        output = (1-self._alpha) * prev_output + self._alpha * output\n",
    "        return output, [output]\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "def get_dist(original_dist):\n",
    "    '''Get the distance in periodic boundary conditions'''\n",
    "    return np.minimum(abs(original_dist),2*np.pi-abs(original_dist))\n",
    "\n",
    "class Trial(object):\n",
    "    \"\"\"Class representing a batch of trials.\"\"\"\n",
    "\n",
    "    def __init__(self, config, tdim, batch_size):\n",
    "        \"\"\"A batch of trials.\n",
    "\n",
    "        Args:\n",
    "            config: dictionary of configurations\n",
    "            tdim: int, number of time steps\n",
    "            batch_size: int, batch size\n",
    "        \"\"\"\n",
    "        self.float_type = 'float32' # This should be the default\n",
    "        self.config = config\n",
    "        self.dt = self.config['dt']\n",
    "\n",
    "        self.n_eachring = self.config['n_eachring']\n",
    "        self.n_input = self.config['n_input']\n",
    "        self.n_output = self.config['n_output']\n",
    "        self.pref  = np.arange(0,2*np.pi,2*np.pi/self.n_eachring) # preferences\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.tdim = tdim\n",
    "        self.x = np.zeros((tdim, batch_size, self.n_input), dtype=self.float_type)\n",
    "        self.y = np.zeros((tdim, batch_size, self.n_output), dtype=self.float_type)\n",
    "        if self.config['loss_type'] == 'lsq':\n",
    "            self.y[:,:,:] = 0.05\n",
    "        # y_loc is the stimulus location of the output, -1 for fixation, (0,2 pi) for response\n",
    "        self.y_loc = -np.ones((tdim, batch_size)      , dtype=self.float_type)\n",
    "\n",
    "        self._sigma_x = config['sigma_x']*np.sqrt(2/config['alpha'])\n",
    "\n",
    "    def expand(self, var):\n",
    "        \"\"\"Expand an int/float to list.\"\"\"\n",
    "        if not hasattr(var, '__iter__'):\n",
    "            var = [var] * self.batch_size\n",
    "        return var\n",
    "\n",
    "    def add(self, loc_type, locs=None, ons=None, offs=None, strengths=1, mods=None):\n",
    "        \"\"\"Add an input or stimulus output.\n",
    "\n",
    "        Args:\n",
    "            loc_type: str (fix_in, stim, fix_out, out), type of information to be added\n",
    "            locs: array of list of float (batch_size,), locations to be added, only for loc_type=stim or out\n",
    "            ons: int or list, index of onset time\n",
    "            offs: int or list, index of offset time\n",
    "            strengths: float or list, strength of input or target output\n",
    "            mods: int or list, modalities of input or target output\n",
    "        \"\"\"\n",
    "\n",
    "        ons = self.expand(ons)\n",
    "        offs = self.expand(offs)\n",
    "        strengths = self.expand(strengths)\n",
    "        mods = self.expand(mods)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            if loc_type == 'fix_in':\n",
    "                self.x[ons[i]: offs[i], i, 0] = 1\n",
    "            elif loc_type == 'stim':\n",
    "                # Assuming that mods[i] starts from 1\n",
    "                self.x[ons[i]: offs[i], i, 1+(mods[i]-1)*self.n_eachring:1+mods[i]*self.n_eachring]                     += self.add_x_loc(locs[i])*strengths[i]\n",
    "            elif loc_type == 'fix_out':\n",
    "                # Notice this shouldn't be set at 1, because the output is logistic and saturates at 1\n",
    "                if self.config['loss_type'] == 'lsq':\n",
    "                    self.y[ons[i]: offs[i], i, 0] = 0.8\n",
    "                else:\n",
    "                    self.y[ons[i]: offs[i], i, 0] = 1.0\n",
    "            elif loc_type == 'out':\n",
    "                if self.config['loss_type'] == 'lsq':\n",
    "                    self.y[ons[i]: offs[i], i, 1:] += self.add_y_loc(locs[i])*strengths[i]  #target response\n",
    "                else:\n",
    "                    y_tmp = self.add_y_loc(locs[i])\n",
    "                    y_tmp /= np.sum(y_tmp)\n",
    "                    self.y[ons[i]: offs[i], i, 1:] += y_tmp\n",
    "                self.y_loc[ons[i]: offs[i], i] = locs[i] #location\n",
    "            else:\n",
    "                raise ValueError('Unknown loc_type')\n",
    "\n",
    "    def add_x_noise(self):\n",
    "        \"\"\"Add input noise.\"\"\"\n",
    "        self.x += self.config['rng'].randn(*self.x.shape)*self._sigma_x\n",
    "\n",
    "    def add_c_mask(self, pre_offs, post_ons):\n",
    "        \"\"\"Add a cost mask.\n",
    "\n",
    "        Usually there are two periods, pre and post response\n",
    "        Scale the mask weight for the post period so in total it's as important\n",
    "        as the pre period\n",
    "        \"\"\"\n",
    "\n",
    "        pre_on   = int(100/self.dt) # never check the first 100ms\n",
    "        pre_offs = self.expand(pre_offs)\n",
    "        post_ons = self.expand(post_ons)\n",
    "\n",
    "        if self.config['loss_type'] == 'lsq':\n",
    "            c_mask = np.zeros((self.tdim, self.batch_size, self.n_output), dtype=self.float_type)\n",
    "            for i in range(self.batch_size):\n",
    "                # Post response periods usually have the same length across tasks\n",
    "                c_mask[post_ons[i]:, i, :] = 5.\n",
    "                # Pre-response periods usually have different lengths across tasks\n",
    "                # To keep cost comparable across tasks\n",
    "                # Scale the cost mask of the pre-response period by a factor\n",
    "                c_mask[pre_on:pre_offs[i], i, :] = 1.\n",
    "\n",
    "            # self.c_mask[:, :, 0] *= self.n_eachring # Fixation is important\n",
    "            c_mask[:, :, 0] *= 2. # Fixation is important\n",
    "\n",
    "            self.c_mask = c_mask.reshape((self.tdim*self.batch_size, self.n_output))\n",
    "        else:\n",
    "            c_mask = np.zeros((self.tdim, self.batch_size), dtype=self.float_type)\n",
    "            for i in range(self.batch_size):\n",
    "                # Post response periods usually have the same length across tasks\n",
    "                # Having it larger than 1 encourages the network to achieve higher performance\n",
    "                c_mask[post_ons[i]:, i] = 5.\n",
    "                # Pre-response periods usually have different lengths across tasks\n",
    "                # To keep cost comparable across tasks\n",
    "                # Scale the cost mask of the pre-response period by a factor\n",
    "                c_mask[pre_on:pre_offs[i], i] = 1.\n",
    "\n",
    "            self.c_mask = c_mask.reshape((self.tdim*self.batch_size,))\n",
    "            self.c_mask /= self.c_mask.mean()\n",
    "\n",
    "    def add_rule(self, rule, on=None, off=None, strength=1.):\n",
    "        \"\"\"Add rule input.\"\"\"\n",
    "        if isinstance(rule, int):\n",
    "            self.x[on:off, :, self.config['rule_start']+rule] = strength\n",
    "        else:\n",
    "            ind_rule = get_rule_index(rule, self.config)\n",
    "            self.x[on:off, :, ind_rule] = strength\n",
    "\n",
    "    def add_x_loc(self, x_loc):\n",
    "        \"\"\"Input activity given location.\"\"\"\n",
    "        dist = get_dist(x_loc-self.pref)  # periodic boundary\n",
    "        dist /= np.pi/8\n",
    "        return 0.8*np.exp(-dist**2/2)\n",
    "\n",
    "    def add_y_loc(self, y_loc):\n",
    "        \"\"\"Target response given location.\"\"\"\n",
    "        dist = get_dist(y_loc-self.pref)  # periodic boundary\n",
    "        if self.config['loss_type'] == 'lsq':\n",
    "            dist /= np.pi/8\n",
    "            y = 0.8*np.exp(-dist**2/2)\n",
    "        else:\n",
    "            # One-hot output\n",
    "            y = np.zeros_like(dist)\n",
    "            ind = np.argmin(dist)\n",
    "            y[ind] = 1.\n",
    "        return y\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "\n",
    "def plot_loss_over_epochs(history, foldername=''):\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.plot(history.history['loss'],label=\"Training set loss\")\n",
    "    # plt.plot(history.history['val_loss'],label=\"Validation set loss\")\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    plt.savefig('%sloss_over_epochs.png'%foldername)\n",
    "\n",
    "def get_delay_bins(delay):\n",
    "    dt=20\n",
    "    stim1_ons = int(500/dt)\n",
    "    stim1_offs = stim1_ons + int(300/dt)\n",
    "    stim2_ons =stim1_offs + int(1000/dt)\n",
    "    stim2_offs = stim2_ons + int(300/dt)\n",
    "    fix_offs  = stim2_offs + int(1000/dt)\n",
    "\n",
    "    baseline = (0,stim1_ons)\n",
    "\n",
    "    if delay == 1:\n",
    "            delay_bins = (stim2_ons - int(500/dt),stim2_ons)\n",
    "\n",
    "    elif delay == 2:\n",
    "            delay_bins = (fix_offs - int(500/dt),fix_offs)\n",
    "\n",
    "    return delay_bins\n",
    "\n",
    "def plot_tuning_curves(Z,foldername='',filename=''):\n",
    "    input\n",
    "    fig,ax = plt.subplots(min(Z.shape[1],10),1,figsize=(5,min(Z.shape[1],10)*3))\n",
    "    for i in range(min(Z.shape[1],10)):\n",
    "        if Z.shape[1]<10:\n",
    "            neuron = i\n",
    "        else:\n",
    "            neuron = i*int(Z.shape[1]/10)\n",
    "        for loc1 in [j*10 for j in range(int(128/10))]:\n",
    "            df = pd.DataFrame({'first_stim':ind_stim_loc1[ind_stim_loc1==loc1],'second_stim':ind_stim_loc2[ind_stim_loc1==loc1],'activity':Z[:,neuron][ind_stim_loc1==loc1]})\n",
    "            x = df['second_stim']\n",
    "            y = df['activity']\n",
    "            ax[i].scatter(x,y,s=1, color=palette1[loc1], label=loc1)\n",
    "        ax[i].set_ylabel('%dth neuron'%neuron, fontsize=13)\n",
    "    ax[0].legend(loc='upper left', bbox_to_anchor= (1.05, 1.05), title='Stim 1')\n",
    "    ax[0].set_xlabel('Stim 2', fontsize=13)\n",
    "    ax[0].xaxis.set_label_position('top') \n",
    "    if filename != '':\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(foldername+filename+'_stim2')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "        \n",
    "    fig,ax = plt.subplots(min(Z.shape[1],10),1,figsize=(5,min(Z.shape[1],10)*3))\n",
    "    for i in range(min(Z.shape[1],10)):\n",
    "        if Z.shape[1]<10:\n",
    "            neuron = i\n",
    "        else:\n",
    "            neuron = i*int(Z.shape[1]/10)\n",
    "        for loc2 in [j*10 for j in range(int(128/10))]:\n",
    "            df = pd.DataFrame({'first_stim':ind_stim_loc1[ind_stim_loc2==loc2],'second_stim':ind_stim_loc2[ind_stim_loc2==loc2],'activity':Z[:,neuron][ind_stim_loc2==loc2]})\n",
    "            x = df['first_stim']\n",
    "            y = df['activity']\n",
    "            ax[i].scatter(x,y,s=1, color=palette2[loc2], label=loc2)\n",
    "        ax[i].set_ylabel('%dth neuron'%neuron, fontsize=13)\n",
    "    ax[0].legend(loc='upper left', bbox_to_anchor= (1.05, 1.05), title='Stim 2')\n",
    "    ax[0].set_xlabel('Stim 1', fontsize=13)\n",
    "    ax[0].xaxis.set_label_position('top') \n",
    "    if filename != '':\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(foldername+filename+'_stim1')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "def fit_isomap(data_to_use, n_neighbors = 15, target_dim = 3):\n",
    "    iso_instance = manifold.Isomap(n_neighbors = n_neighbors, n_components = target_dim)\n",
    "    proj = iso_instance.fit_transform(data_to_use)\n",
    "    return proj\n",
    "\n",
    "def set_axes_equal(ax):\n",
    "    '''Make axes of 3D plot have equal scale so that spheres appear as spheres,\n",
    "    cubes as cubes, etc..  This is one possible solution to Matplotlib's\n",
    "    ax.set_aspect('equal') and ax.axis('equal') not working for 3D.\n",
    "\n",
    "    Input\n",
    "      ax: a matplotlib axis, e.g., as output from plt.gca().\n",
    "    '''\n",
    "\n",
    "    x_limits = ax.get_xlim3d()\n",
    "    y_limits = ax.get_ylim3d()\n",
    "    z_limits = ax.get_zlim3d()\n",
    "\n",
    "    x_range = abs(x_limits[1] - x_limits[0])\n",
    "    x_middle = np.mean(x_limits)\n",
    "    y_range = abs(y_limits[1] - y_limits[0])\n",
    "    y_middle = np.mean(y_limits)\n",
    "    z_range = abs(z_limits[1] - z_limits[0])\n",
    "    z_middle = np.mean(z_limits)\n",
    "\n",
    "    # The plot bounding box is a sphere in the sense of the infinity\n",
    "    # norm, hence I call half the max range the plot radius.\n",
    "    plot_radius = 0.5*max([x_range, y_range, z_range])\n",
    "\n",
    "    ax.set_xlim3d([x_middle - plot_radius, x_middle + plot_radius])\n",
    "    ax.set_ylim3d([y_middle - plot_radius, y_middle + plot_radius])\n",
    "    ax.set_zlim3d([z_middle - plot_radius, z_middle + plot_radius])\n",
    "\n",
    "def plot_isomap(data_plot, color, annotate=False):\n",
    "    fig = plt.figure(figsize=(16,16),dpi=200)\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    if annotate:\n",
    "        ax.scatter(data_plot[:,0], data_plot[:,1], data_plot[:,2], \n",
    "            s=5, alpha=1, edgecolor='face',c=color)\n",
    "        label = 0\n",
    "        for xyz in zip(data_plot[:,0], data_plot[:,1], data_plot[:,2]):\n",
    "            x, y, z = xyz\n",
    "            ax.text(x, y, z, '%s' % (label), size=5, zorder=1, color='k')\n",
    "            label += 1\n",
    "    else:\n",
    "        ax.scatter(data_plot[:,0], data_plot[:,1], data_plot[:,2], \n",
    "            s=20, alpha=1, edgecolor='face',c=color)\n",
    "    ax.grid(False)\n",
    "    ax.xaxis.pane.fill = False\n",
    "    ax.yaxis.pane.fill = False\n",
    "    ax.zaxis.pane.fill = False\n",
    "    ax.xaxis.pane.set_edgecolor('w')\n",
    "    ax.yaxis.pane.set_edgecolor('w')\n",
    "    ax.zaxis.pane.set_edgecolor('w')\n",
    "    return fig, ax\n",
    "\n",
    "def plot_single_distractor_or_target(palette, xlim, ylim, zlim, label_plot, proj_plot, annotate=False, filename=''):\n",
    "\n",
    "    color=np.array(palette)[label_plot]\n",
    "\n",
    "    h0_longest,h1_longest,h2_longest = run_ripser(proj_plot,figure_dir+'ripser'+figure_subscript)\n",
    "    fig, ax = plot_isomap(data_plot=proj_plot, color=color, annotate=annotate)\n",
    "    plt.setp(ax, xlim=xlim, ylim=ylim, zlim=zlim)\n",
    "    fig.tight_layout()\n",
    "    if filename is not None:\n",
    "        fig.savefig(filename)\n",
    "    plt.show()\n",
    "    plt.close(fig) \n",
    "    \n",
    "    return h1_longest\n",
    "\n",
    "def plot_all_isomap_figures(proj,foldername='',filename=''):\n",
    "    fig,ax = plot_isomap(data_plot=proj, color=color1)\n",
    "    set_axes_equal(ax)\n",
    "    fig.tight_layout()\n",
    "    if filename is not None:\n",
    "        fig.savefig(foldername+filename+'_target_isomap.png')\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "    fig,ax = plot_isomap(data_plot=proj, color=color2)\n",
    "    set_axes_equal(ax)\n",
    "    fig.tight_layout()\n",
    "    if filename is not None:\n",
    "        fig.savefig(foldername+filename+'_distractor_isomap.png')\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "    xlim=fig.gca().get_xlim()\n",
    "    ylim=fig.gca().get_ylim()\n",
    "    zlim=fig.gca().get_zlim()\n",
    "\n",
    "    num=0\n",
    "    indices = ind_stim_loc1==num\n",
    "    label_plot = ind_stim_loc2[indices]\n",
    "    proj_plot = proj[indices,:]\n",
    "    plot_single_distractor_or_target(palette = palette2, xlim = xlim, ylim = ylim, zlim = zlim, label_plot=label_plot, proj_plot = proj_plot, filename = foldername+filename+'_single_target.png')\n",
    "\n",
    "    num=0\n",
    "    indices = ind_stim_loc2==num\n",
    "    label_plot = ind_stim_loc1[indices]\n",
    "    proj_plot = proj[indices,:]\n",
    "    plot_single_distractor_or_target(palette = palette1, xlim = xlim, ylim = ylim, zlim = zlim, label_plot=label_plot, proj_plot = proj_plot, filename = foldername+filename+'_single_distractor.png')\n",
    "\n",
    "\n",
    "# In[109]:\n",
    "\n",
    "\n",
    "def get_c_mask(batch_dim):\n",
    "\n",
    "    dt = hp['dt']\n",
    "    pre_on   = int(100/dt)\n",
    "    \n",
    "    stim1_strengths = 1\n",
    "    stim2_strengths = 1\n",
    "    stim1_ons = int(500/dt)\n",
    "    stim1_offs = stim1_ons + int(300/dt)\n",
    "    stim2_ons =stim1_offs + int(1000/dt)\n",
    "    stim2_offs = stim2_ons + int(300/dt)\n",
    "    fix_offs  = stim2_offs + int(1000/dt)\n",
    "    output_2_on = fix_offs + int(500/dt)\n",
    "    tdim = output_2_on + int(500/dt)\n",
    "    check_ons = fix_offs + int(100/dt)\n",
    "    \n",
    "    pre_offs=fix_offs\n",
    "    post_ons=check_ons\n",
    "    \n",
    "    c_mask = np.zeros(batch_dim, dtype='float32')\n",
    "    for i in range(batch_dim[0]):\n",
    "        c_mask[i, post_ons:, :] = 5.\n",
    "        c_mask[i, pre_on:pre_offs, :] = 1.\n",
    "    c_mask[:, :, 0] *= 2. # Fixation is important\n",
    "    c_mask = c_mask.reshape((batch_dim[0]*batch_dim[1], batch_dim[2]))\n",
    "    \n",
    "    return c_mask\n",
    "    \n",
    "\n",
    "def custom_mse(y_true, y_hat):\n",
    "\n",
    "    n_output = hp['n_output']\n",
    "    y_true_shaped = tf.reshape(y_true, (-1, n_output))\n",
    "    y_hat_shaped = tf.reshape(y_hat, (-1, n_output))\n",
    "    cost_lsq = K.mean(K.square((y_true_shaped - y_hat_shaped) * c_mask_shaped_tf))\n",
    "    \n",
    "#     cost = self.cost_lsq + self.cost_reg\n",
    "    cost = cost_lsq\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def popvec(y):\n",
    "    \"\"\"Population vector read out.\n",
    "\n",
    "    Assuming the last dimension is the dimension to be collapsed\n",
    "\n",
    "    Args:\n",
    "        y: population output on a ring network. Numpy array (Batch, Units)\n",
    "\n",
    "    Returns:\n",
    "        Readout locations: Numpy array (Batch,)\n",
    "    \"\"\"\n",
    "    pref = np.arange(0, 2*np.pi, 2*np.pi/y.shape[-1])  # preferences\n",
    "    temp_sum = y.sum(axis=-1)\n",
    "    temp_cos = np.sum(y*np.cos(pref), axis=-1)/temp_sum\n",
    "    temp_sin = np.sum(y*np.sin(pref), axis=-1)/temp_sum\n",
    "    loc = np.arctan2(temp_sin, temp_cos)\n",
    "    return np.mod(loc, 2*np.pi)\n",
    "\n",
    "def custom_response_accuracy(y_true, y_hat):\n",
    "    if type(y_true) is not np.ndarray:\n",
    "        y_true = y_true.numpy()\n",
    "    if type(y_hat) is not np.ndarray:\n",
    "        y_hat = y_hat.numpy()\n",
    "    y_hat_loc1 = popvec(np.mean(y_hat[:, 155:180, 1:], axis=1))\n",
    "    y_true_loc1 = popvec(np.mean(y_true[:, 155:180, 1:], axis=1))\n",
    "    original_dist1 = y_true_loc1 - y_hat_loc1\n",
    "    dist1 = np.minimum(abs(original_dist1), 2*np.pi-abs(original_dist1))\n",
    "    corr_loc1 = dist1 < 2*np.pi/hp['n_loc']\n",
    "\n",
    "    y_hat_loc2 = popvec(np.mean(y_hat[:, 180:, 1:], axis=1))\n",
    "    y_true_loc2 = popvec(np.mean(y_true[:, 180:, 1:], axis=1))\n",
    "    original_dist2 = y_true_loc2 - y_hat_loc2\n",
    "    dist2 = np.minimum(abs(original_dist2), 2*np.pi-abs(original_dist2))\n",
    "    corr_loc2 = dist2 < 2*np.pi/hp['n_loc']\n",
    "    \n",
    "    return np.sum(corr_loc1*0.5+corr_loc2*0.5)/corr_loc1.shape[0]\n",
    "\n",
    "def custom_response_loss(y_true, y_hat):\n",
    "    if type(y_true) is not np.ndarray:\n",
    "        y_true = y_true.numpy()\n",
    "    if type(y_hat) is not np.ndarray:\n",
    "        y_hat = y_hat.numpy()\n",
    "    y_hat_loc1 = popvec(np.mean(y_hat[:, 155:180, 1:], axis=1))\n",
    "    y_true_loc1 = popvec(np.mean(y_true[:, 155:180, 1:], axis=1))\n",
    "    original_dist1 = y_true_loc1 - y_hat_loc1\n",
    "    dist1 = np.minimum(abs(original_dist1), 2*np.pi-abs(original_dist1))\n",
    "\n",
    "    y_hat_loc2 = popvec(np.mean(y_hat[:, 180:, 1:], axis=1))\n",
    "    y_true_loc2 = popvec(np.mean(y_true[:, 180:, 1:], axis=1))\n",
    "    original_dist2 = y_true_loc2 - y_hat_loc2\n",
    "    dist2 = np.minimum(abs(original_dist2), 2*np.pi-abs(original_dist2))\n",
    "    \n",
    "    return np.sum(dist1*0.5+dist2*0.5)/dist1.shape[0]*(360/(2*np.pi))\n",
    "\n",
    "def custom_perf(y_true, y_hat):\n",
    "\n",
    "    if type(y_true) is not np.ndarray:\n",
    "        y_true = y_true.numpy()\n",
    "    if type(y_hat) is not np.ndarray:\n",
    "        y_hat = y_hat.numpy()\n",
    "\n",
    "    y_true = y_true[:,-1,:]\n",
    "    y_hat = y_hat[:,-1,:]\n",
    "\n",
    "    y_hat_loc = popvec(y_hat[..., 1:])\n",
    "    y_true_loc = popvec(y_true[..., 1:])\n",
    "    y_hat_fix = y_hat[..., 0]\n",
    "    fixating = y_hat_fix > 0.5 \n",
    "\n",
    "    original_dist = y_true_loc - y_hat_loc\n",
    "    dist = np.minimum(abs(original_dist), 2*np.pi-abs(original_dist))\n",
    "    corr_loc = dist < 2*np.pi/hp['n_loc']\n",
    "\n",
    "\n",
    "    # Should fixate?\n",
    "    should_fix = y_true_loc < 0\n",
    "\n",
    "    # performance\n",
    "    perf = should_fix * fixating + (1-should_fix) * corr_loc * (1-fixating) \n",
    "    return np.mean(perf)\n",
    "# In[83]:\n",
    "\n",
    "def generate_trial(mode='random',batch_size=hp['batch_size_train'],**kwargs):\n",
    "    dt = hp['dt']\n",
    "    if mode == 'random':\n",
    "        rng = hp['rng']\n",
    "        stim1_locs = rng.uniform(0, 2*np.pi, (batch_size,))\n",
    "        stim2_locs = rng.uniform(0, 2*np.pi, (batch_size,))\n",
    "\n",
    "        stims_mean = rng.uniform(0.8,1.2,(batch_size,))\n",
    "        stims_coh  = rng.choice([0.,0.08,0.16,0.32],(batch_size,))\n",
    "        stims_sign = rng.choice([1,-1], (batch_size,))\n",
    "\n",
    "        stim1_strengths = stims_mean + stims_coh*stims_sign\n",
    "        stim2_strengths = stims_mean - stims_coh*stims_sign\n",
    "\n",
    "    elif mode == 'fixed':\n",
    "        n_loc = kwargs['n_loc']\n",
    "        batch_size = n_loc*n_loc\n",
    "        n_stim_loc1, n_stim_loc2, repeat = stim_loc_shape = n_loc, n_loc, 1\n",
    "        stim_loc_size = np.prod(stim_loc_shape)\n",
    "        ind_stim_loc1, ind_stim_loc2, ind_repeat = np.unravel_index(range(stim_loc_size),stim_loc_shape)\n",
    "        stim1_locs = 2*np.pi*ind_stim_loc1/n_stim_loc1\n",
    "        stim2_locs = 2*np.pi*ind_stim_loc2/n_stim_loc2\n",
    "        \n",
    "        stim1_strengths = 1\n",
    "        stim2_strengths = 1\n",
    "        \n",
    "    else:\n",
    "        raise ValueError('Unknown mode: ' + str(mode))\n",
    "        \n",
    "    stim1_ons = int(500/dt)\n",
    "    stim1_offs = stim1_ons + int(300/dt)\n",
    "    stim2_ons =stim1_offs + int(1000/dt)\n",
    "    stim2_offs = stim2_ons + int(300/dt)\n",
    "    fix_offs  = stim2_offs + int(1000/dt)\n",
    "    output_2_on = fix_offs + int(500/dt)\n",
    "    tdim = output_2_on + int(500/dt)\n",
    "\n",
    "    check_ons = fix_offs + int(100/dt)\n",
    "\n",
    "    trial = Trial(hp, tdim, batch_size)\n",
    "    trial.add('fix_in', offs=fix_offs)\n",
    "    trial.add('stim', stim1_locs, ons=stim1_ons, offs=stim1_offs, strengths=stim1_strengths, mods=1)\n",
    "    trial.add('stim', stim2_locs, ons=stim2_ons, offs=stim2_offs, strengths=stim2_strengths, mods=1)\n",
    "    trial.add('fix_out', offs=fix_offs)\n",
    "    stim_locs = [stim1_locs[i] for i in range(batch_size)]\n",
    "    stim_locs2 = [stim2_locs[i] for i in range(batch_size)]\n",
    "    trial.add('out',stim_locs,ons=fix_offs,offs=output_2_on)\n",
    "    trial.add('out',stim_locs2,ons=output_2_on)\n",
    "\n",
    "    trial.add_c_mask(pre_offs=fix_offs,post_ons=check_ons)\n",
    "    trial.epochs = {'fix1':(None,stim1_ons),\n",
    "                    'stim1':(stim1_ons,stim1_offs),\n",
    "                    'delay1':(stim1_offs,stim2_ons),\n",
    "                    'stim2':(stim2_ons,stim2_offs),\n",
    "                    'delay2':(stim2_offs,fix_offs),\n",
    "                    'go1':(fix_offs,output_2_on),\n",
    "                    'go2':(output_2_on,None)}\n",
    "    return trial\n",
    "\n",
    "def train_generator():\n",
    "    for i in range(hp['steps_per_epoch']*hp['n_epochs']):\n",
    "        trial = generate_trial(mode='random',batch_size=hp['batch_size_train'])\n",
    "        x = trial.x.swapaxes(0,1)\n",
    "        y = trial.y.swapaxes(0,1)\n",
    "        yield x, y\n",
    "\n",
    "def val_generator():\n",
    "    for i in range(hp['steps_per_epoch']*hp['n_epochs']):\n",
    "        trial = generate_trial(mode='random',batch_size=hp['batch_size_val'])\n",
    "        x = trial.x.swapaxes(0,1)\n",
    "        y = trial.y.swapaxes(0,1)\n",
    "        yield x, y\n",
    "\n",
    "\n",
    "# In[84]:\n",
    "\n",
    "\n",
    "def create_model(rnn_layer):\n",
    "    model = Sequential()\n",
    "    model.add(rnn_layer)\n",
    "    model.add(TimeDistributed(Dense(33, activation='sigmoid')))\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp['learning_rate']),\n",
    "        loss=custom_mse,\n",
    "        metrics=[custom_perf, custom_response_accuracy,custom_response_loss],\n",
    "        run_eagerly=True)\n",
    "    # model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp['learning_rate']),\n",
    "    #     loss=custom_mse,\n",
    "    #     metrics=[custom_perf],\n",
    "    #     run_eagerly=True)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# In[85]:\n",
    "\n",
    "\n",
    "def generate_hidden_layer_plots(rnn_layer, superscript):\n",
    "    hidden = rnn_layer(x)\n",
    "\n",
    "    for delay in range(1,3):\n",
    "        print('delay:%s'%delay)\n",
    "        delay_bins = get_delay_bins(delay=delay)\n",
    "        #extract mean firing rates for delay bins\n",
    "        delay_hidden = np.mean(hidden[:, delay_bins[0]:delay_bins[1], :], axis=1)\n",
    "    #     delay_hidden = hidden[:, delay_bins[1], :]\n",
    "        plot_tuning_curves(delay_hidden,tuning_curve_folder,'%s_delay%d_hidden'%(superscript,delay))\n",
    "\n",
    "        proj = fit_isomap(data_to_use=delay_hidden)\n",
    "        plot_all_isomap_figures(proj,isomap_folder,'%s_delay%d_hidden'%(superscript,delay))\n",
    "\n",
    "def save_hp(hp, model_dir):\n",
    "    \"\"\"Save the hyper-parameter file of model save_name\"\"\"\n",
    "    hp_copy = hp.copy()\n",
    "    hp_copy.pop('rng', None)\n",
    "    with open(os.path.join(model_dir, 'hp.json'), 'w') as f:\n",
    "        json.dump(hp_copy, f)\n",
    "\n",
    "class NBatchLogger(tf.keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    A Logger that log average performance per `display` steps.\n",
    "    \"\"\"\n",
    "    def __init__(self, display):\n",
    "        self.step = 0\n",
    "        self.display = display\n",
    "        self.metric_cache = {}\n",
    "        self.t_start = time.time()\n",
    "        \n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        keys = list(logs.keys())\n",
    "        for k in keys:\n",
    "            self.metric_cache[k] = self.metric_cache.get(k, 0) + logs[k]\n",
    "        if self.step % self.display == 0:\n",
    "            metrics_log = ''\n",
    "            for (k, v) in self.metric_cache.items():\n",
    "                val = v / self.display\n",
    "                if abs(val) > 1e-3:\n",
    "                    metrics_log += ' - %s: %.4f' % (k, val)\n",
    "                else:\n",
    "                    metrics_log += ' - %s: %.4e' % (k, val)\n",
    "            print('time: {} | trial: {} | batch: {} ... {}'.format(time.time()-self.t_start, self.step * hp['batch_size_train'], self.step,\n",
    "                                          metrics_log))\n",
    "            self.metric_cache.clear()\n",
    "        self.step += 1\n",
    "\n",
    "class AccuracyThresholdCallback(tf.keras.callbacks.Callback): \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        acc = logs.get('custom_response_accuracy')\n",
    "        if(acc > hp['accuracy_threshold']):\n",
    "            print(\"\\nReached %2.2f%% accuracy!\" %(acc*100))   \n",
    "            self.model.stop_training = True\n",
    "\n",
    "class printeverybatch(tf.keras.Model):\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        tf.print('train batch:')\n",
    "        tf.print(x[0,90:105,0])\n",
    "        tf.print(y[0,180:,0])\n",
    "        tf.print()\n",
    "        tf.print(x[0,90:105,1])\n",
    "        tf.print(y[0,180:,1])\n",
    "        return super().train_step(data)\n",
    "\n",
    "    def test_step(self, data):\n",
    "        x, y = data\n",
    "        tf.print('val batch:')\n",
    "        tf.print(x[0,90:105,0])\n",
    "        tf.print(y[0,180:,0])\n",
    "        tf.print()\n",
    "        tf.print(x[0,90:105,1])\n",
    "        tf.print(y[0,180:,1])\n",
    "        return super().test_step(data)\n",
    "# # In[108]:\n",
    "\n",
    "# def parse_args():\n",
    "#     parser = argparse.ArgumentParser(\"HP for training\")\n",
    "#     parser.add_argument(\"--seed\",type=int,default = 0, help = \"Seed number\")\n",
    "#     parser.add_argument(\"--n_rnn\",type=int,default = 256, help = \"Number of hidden neurons\")\n",
    "#     parser.add_argument(\"--batch_size_train\",type=int,default = 512, help = \"Training Batch Size\")\n",
    "#     parser.add_argument(\"--accuracy_threshold\",type=float,default = 0.9, help = \"Accuracy Threshold To Stop Training\")\n",
    "#     parser.add_argument(\"--with_noise\",type=str,default = 'True', help = \"Whether to add input and recurrent noise\",choices=('True','False'))\n",
    "#     parser.add_argument(\"--load_model\",type=str,default = 'False', help = \"Whether to load model\",choices=('True','False'))\n",
    "\n",
    "#     return parser.parse_args()\n",
    "\n",
    "# arglist = parse_args()\n",
    "# seed = arglist.seed\n",
    "# n_rnn = arglist.n_rnn\n",
    "# batch_size_train = arglist.batch_size_train\n",
    "# accuracy_threshold = arglist.accuracy_threshold\n",
    "# with_noise = arglist.with_noise == 'True'\n",
    "# load_model = arglist.load_model == 'True'\n",
    "\n",
    "# print(f\"seed: {seed}, n_rnn: {n_rnn}, batch_size_train: {batch_size_train}, accuracy_threshold: {accuracy_threshold}, with_noise: {with_noise}, load_model: {load_model}\")\n",
    "\n",
    "# tf.random.set_seed(seed)\n",
    "\n",
    "# trial = generate_trial(mode='fixed',n_loc=hp['n_loc'])\n",
    "# x = trial.x.swapaxes(0,1)\n",
    "# y = trial.y.swapaxes(0,1)\n",
    "\n",
    "# hp['n_rnn']=n_rnn\n",
    "# hp['batch_size_train']=batch_size_train\n",
    "# hp['batch_size_val']=batch_size_train\n",
    "# hp['accuracy_threshold']=accuracy_threshold\n",
    "# hp['n_epochs']=int(100000000/batch_size_train)\n",
    "# hp['n_display']=int(32000/batch_size_train)\n",
    "# hp['n_patience']=hp['n_display']*100\n",
    "# hp['seed']=seed\n",
    "# hp['rng'] = np.random.RandomState(seed)\n",
    "\n",
    "# c_mask_shaped = get_c_mask((hp['batch_size_train'], x.shape[1], x.shape[2]))\n",
    "# c_mask_shaped_tf = tf.convert_to_tensor(c_mask_shaped, dtype=tf.float32)\n",
    "\n",
    "# model_folder = '/hpctmp/e0316055/2_stim_batch_size_%s/n_hidden_%s/2_stim_batch_size_%s_n_hidden_%s_acc_%s_seed_%s'%(batch_size_train,n_rnn,batch_size_train,n_rnn,int(accuracy_threshold*100),seed)\n",
    "# if with_noise:\n",
    "#     hp['sigma_rec']=0.05\n",
    "#     hp['sigma_x']=0.01\n",
    "#     model_folder += '_with_noise'\n",
    "# else:\n",
    "#     model_folder += '_with_noise'\n",
    "# print('model folder: ' + model_folder)\n",
    "\n",
    "# main_checkpoint_path = os.path.dirname(model_folder)+\"/checkpoint_seed_%s/cp.ckpt\"%seed\n",
    "# checkpoint_path = model_folder+\"/checkpoint/cp.ckpt\"\n",
    "# isomap_folder = model_folder+'/isomap/'\n",
    "# tuning_curve_folder = model_folder+'/tuning_curves/'\n",
    "# loss_curve_folder = model_folder+'/'\n",
    "\n",
    "# if not os.path.exists(model_folder):\n",
    "#     os.makedirs(model_folder)\n",
    "#     os.makedirs(tuning_curve_folder)\n",
    "#     os.makedirs(isomap_folder)\n",
    "\n",
    "# save_hp(hp, model_folder)\n",
    "# # Display hp\n",
    "# for key, val in hp.items():\n",
    "#     print('{:20s} = '.format(key) + str(val))\n",
    "\n",
    "# dataset_train = tf.data.Dataset.from_generator(train_generator, \n",
    "#                                 output_types=(np.float32,np.float32), \n",
    "#                                 output_shapes=((hp['batch_size_train'],x.shape[1],x.shape[2]),(hp['batch_size_train'],y.shape[1],y.shape[2])))\n",
    "# # dataset_val = tf.data.Dataset.from_generator(val_generator,\n",
    "# #                                 output_types=(np.float32,np.float32), \n",
    "# #                                 output_shapes=((hp['batch_size_val'],x.shape[1],x.shape[2]),(hp['batch_size_val'],y.shape[1],y.shape[2])))\n",
    "\n",
    "# cell = LeakyRNNCell2(hp)\n",
    "# rnn_layer = tf.keras.layers.RNN(cell,input_shape=((x.shape[1],x.shape[2])),\n",
    "#                                 return_sequences=True)\n",
    "\n",
    "# # print('generating untrained plots......')\n",
    "# # generate_hidden_layer_plots(rnn_layer, superscript='untrained')\n",
    "\n",
    "# model = create_model(rnn_layer)\n",
    "\n",
    "# if load_model and os.path.exists(os.path.dirname(main_checkpoint_path)):\n",
    "#     print('loading previous model......')\n",
    "#     model.load_weights(main_checkpoint_path)\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "# performance_dict = {}\n",
    "# performance_dict['untrained accuracy on %s trials'%x.shape[0]] = round(custom_response_accuracy(y, model.predict(x)),4)\n",
    "# performance_dict['untrained loss on %s trials'%x.shape[0]] = round(custom_response_loss(y, model.predict(x)),4)\n",
    "# performance_dict['untrained perf on %s trials'%x.shape[0]] = round(custom_perf(y, model.predict(x)),4)\n",
    "# print(performance_dict)\n",
    "\n",
    "# display_callback = NBatchLogger(display=hp['n_display'])\n",
    "# threshold_callback = AccuracyThresholdCallback()\n",
    "# history_logger_callback = tf.keras.callbacks.CSVLogger(loss_curve_folder+'log.csv', separator=\",\", append=True)\n",
    "# cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "#                                                 save_weights_only=True,\n",
    "#                                                 save_best_only=True,\n",
    "#                                                 monitor='loss',\n",
    "#                                                 verbose=0)\n",
    "# es_callback = tf.keras.callbacks.EarlyStopping(restore_best_weights=True, \n",
    "#                                                 patience=hp['n_patience'],\n",
    "#                                                 monitor='loss')\n",
    "\n",
    "# history = model.fit(dataset_train, epochs=hp['n_epochs'], steps_per_epoch=hp['steps_per_epoch'], verbose=0,\n",
    "#           callbacks=[cp_callback, history_logger_callback, display_callback, threshold_callback])\n",
    "\n",
    "# shutil.copytree(os.path.dirname(checkpoint_path), os.path.dirname(main_checkpoint_path), dirs_exist_ok=True)\n",
    "\n",
    "# plot_loss_over_epochs(history, foldername=loss_curve_folder)\n",
    "\n",
    "\n",
    "# hp['sigma_rec']=0\n",
    "# hp['sigma_x']=0\n",
    "# cell = LeakyRNNCell2(hp)\n",
    "# rnn_layer = tf.keras.layers.RNN(cell,input_shape=((x.shape[1],x.shape[2])),\n",
    "#                                 return_sequences=True)\n",
    "# model = create_model(rnn_layer)\n",
    "# model.load_weights(checkpoint_path)\n",
    "# rnn_layer = tf.keras.layers.RNN(cell,input_shape=((x.shape[1],x.shape[2])),\n",
    "#                                 return_sequences=True, weights=model.layers[0].get_weights())\n",
    "# print('generating trained plots......')\n",
    "# generate_hidden_layer_plots(rnn_layer, superscript='trained')\n",
    "\n",
    "# performance_dict['trained accuracy on %s trials'%x.shape[0]] = round(custom_response_accuracy(y, model.predict(x)),4)\n",
    "# performance_dict['trained loss on %s trials'%x.shape[0]] = round(custom_response_loss(y, model.predict(x)),4)\n",
    "# performance_dict['trained perf on %s trials'%x.shape[0]] = round(custom_perf(y, model.predict(x)),4)\n",
    "# print(performance_dict)\n",
    "\n",
    "# with open(os.path.join(model_folder, 'performance.json'), 'w') as f:\n",
    "#     json.dump(performance_dict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dcda8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rnn = 128\n",
    "hp['n_rnn']=n_rnn\n",
    "hp['sigma_rec']=0\n",
    "hp['sigma_x']=0\n",
    "\n",
    "\n",
    "# # generate 128*128 input\n",
    "trial = generate_trial(mode='fixed',n_loc=hp['n_loc'])\n",
    "x = trial.x.swapaxes(0,1)\n",
    "y = trial.y.swapaxes(0,1)\n",
    "print('x shape: ' + str(x.shape))\n",
    "stim_loc_shape = hp['n_loc'],hp['n_loc'],1\n",
    "stim_loc_size = np.prod(stim_loc_shape)\n",
    "ind_stim_loc1, ind_stim_loc2, ind_repeat = np.unravel_index(range(stim_loc_size),stim_loc_shape)\n",
    "\n",
    "# generate 8*8 input\n",
    "# n_loc=32\n",
    "# trial_ = generate_trial(mode='fixed',n_loc=n_loc)\n",
    "# x_ = trial_.x.swapaxes(0,1)\n",
    "# print('x_ shape: ' + str(x_.shape))\n",
    "# stim_loc_shape_ = n_loc,n_loc,1\n",
    "# stim_loc_size_ = np.prod(stim_loc_shape_)\n",
    "# ind_stim_loc1_, ind_stim_loc2_, ind_repeat_ = np.unravel_index(range(stim_loc_size_),stim_loc_shape_)\n",
    "\n",
    "# num_samples = 250\n",
    "\n",
    "\n",
    "# for acc in [95,80,60]:      \n",
    "#     for seed in range(1,21):\n",
    "for seed in range(2,3):\n",
    "    for acc in [95]:      \n",
    "    \n",
    "\n",
    "#         # generate 10 noisy samples for each 8*8 input (for anova)\n",
    "#         X_n = []\n",
    "#         for i in range(x_.shape[0]):\n",
    "#             for j in range(num_samples):\n",
    "#                 X_n.append(x_[i] + np.random.RandomState(seed).randn(205, 33)*0.01)\n",
    "#         X_n = np.array(X_n)\n",
    "#         print('X_n shape: ' + str(X_n.shape))\n",
    "\n",
    "        tf.random.set_seed(seed)\n",
    "        hp['seed']=seed\n",
    "        hp['rng'] = np.random.RandomState(seed)\n",
    "        model_folder = '/Volumes/Seagate Backup Plus Drive/fyp/results/2_stim_batch_size_512/n_hidden_%s/2_stim_batch_size_512_n_hidden_%s_acc_%s_seed_%s_with_noise'%(n_rnn,n_rnn,acc,seed)\n",
    "        print('model folder: ' + model_folder)\n",
    "        checkpoint_path = model_folder+\"/checkpoint/cp.ckpt\"\n",
    "        isomap_folder = model_folder+'/isomap'\n",
    "        tuning_curve_folder = model_folder+'/tuning_curves/'\n",
    "        selectivity_folder = model_folder+'/selectivity/'\n",
    "        weight_matrix_folder = model_folder+'/weight_matrices/'\n",
    "        \n",
    "        if not os.path.exists(isomap_folder):\n",
    "            os.makedirs(isomap_folder)\n",
    "        if not os.path.exists(tuning_curve_folder):\n",
    "            os.makedirs(tuning_curve_folder)\n",
    "        if not os.path.exists(selectivity_folder):\n",
    "            os.makedirs(selectivity_folder)\n",
    "        if not os.path.exists(weight_matrix_folder):\n",
    "            os.makedirs(weight_matrix_folder)\n",
    "\n",
    "\n",
    "        cell = LeakyRNNCell2(hp)\n",
    "        rnn_layer = tf.keras.layers.RNN(cell,input_shape=((x.shape[1],x.shape[2])),\n",
    "                                        return_sequences=True)\n",
    "        model = create_model(rnn_layer)\n",
    "        model.load_weights(checkpoint_path)\n",
    "        rnn_layer = tf.keras.layers.RNN(cell,input_shape=((x.shape[1],x.shape[2])),\n",
    "                                        return_sequences=True, weights=model.layers[0].get_weights())\n",
    "        weights=model.layers[0].get_weights()\n",
    "\n",
    "#         print('generating_hidden layer')\n",
    "#         hidden = rnn_layer(x) #for linear separability\n",
    "# #         hidden_ = rnn_layer(x_) #for linear separability\n",
    "# #         hidden_n = rnn_layer(X_n) #for anova linear/ non-linear mixed selectivity\n",
    "#         print('done')\n",
    "    \n",
    "#         for delay in range(2,3):\n",
    "#             print('delay:%s'%delay)\n",
    "#             delay_bins = get_delay_bins(delay=delay)\n",
    "#             #extract mean firing rates for delay bins\n",
    "#             delay_hidden = np.mean(hidden[:, delay_bins[0]:delay_bins[1], :], axis=1)\n",
    "# #             delay_hidden = np.mean(hidden_[:, delay_bins[0]:delay_bins[1], :], axis=1)\n",
    "\n",
    "#         #     delay_hidden = hidden[:, delay_bins[1], :]\n",
    "\n",
    "#             proj = fit_isomap(data_to_use=delay_hidden)\n",
    "#             with open(isomap_folder+f'/proj.npy', 'wb') as f:\n",
    "#                 np.save(f, proj)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334f348f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_df=pd.read_csv(selectivity_folder+'/selectivity_and_separabiltiy_of_hidden_neurons_delay_2.csv',index_col=0).iloc[:,:5]\n",
    "sel_df.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecda184a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NL_neurons = sel_df[sel_df['non linear mixed selective']==True].index\n",
    "NL_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f791de",
   "metadata": {},
   "outputs": [],
   "source": [
    "L_neurons = sel_df[sel_df['linear mixed selective']==True].index\n",
    "L_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e2e26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weights[0].shape,weights[1].shape,weights[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb6c22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_rm_NL = [np.copy(arr) for arr in weights]\n",
    "\n",
    "for i in range(weights_rm_NL[0].shape[0]):\n",
    "    for j in range(weights_rm_NL[0].shape[1]):\n",
    "        if j in NL_neurons:\n",
    "            weights_rm_NL[0][i][j] = 0\n",
    "            \n",
    "for i in range(weights_rm_NL[1].shape[0]):\n",
    "    for j in range(weights_rm_NL[1].shape[1]):\n",
    "        if i in NL_neurons or j in NL_neurons:\n",
    "            weights_rm_NL[1][i][j] = 0\n",
    "            \n",
    "for i in range(weights_rm_NL[2].shape[0]):\n",
    "    if i in NL_neurons:\n",
    "        weights_rm_NL[2][i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f1d78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_rm_L = [np.copy(arr) for arr in weights]\n",
    "\n",
    "for i in range(weights_rm_L[0].shape[0]):\n",
    "    for j in range(weights_rm_L[0].shape[1]):\n",
    "        if j in L_neurons:\n",
    "            weights_rm_L[0][i][j] = 0\n",
    "            \n",
    "for i in range(weights_rm_L[1].shape[0]):\n",
    "    for j in range(weights_rm_L[1].shape[1]):\n",
    "        if i in L_neurons or j in L_neurons:\n",
    "            weights_rm_L[1][i][j] = 0\n",
    "            \n",
    "for i in range(weights_rm_L[2].shape[0]):\n",
    "    if i in L_neurons:\n",
    "        weights_rm_L[2][i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287ea213",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_isomap(data_plot, color, annotate=False):\n",
    "    fig = plt.figure(figsize=(16,16),dpi=200)\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    if annotate:\n",
    "        ax.scatter(data_plot[:,0], data_plot[:,1], data_plot[:,2], \n",
    "            s=5, alpha=1, edgecolor='face',c=color)\n",
    "        label = 0\n",
    "        for xyz in zip(data_plot[:,0], data_plot[:,1], data_plot[:,2]):\n",
    "            x, y, z = xyz\n",
    "            ax.text(x, y, z, '%s' % (label), size=5, zorder=1, color='k')\n",
    "            label += 1\n",
    "    else:\n",
    "        ax.scatter(data_plot[:,0], data_plot[:,1], data_plot[:,2], \n",
    "            s=20, alpha=1, edgecolor='face',c=color)\n",
    "    ax.grid(False)\n",
    "    ax.xaxis.pane.fill = False\n",
    "    ax.yaxis.pane.fill = False\n",
    "    ax.zaxis.pane.fill = False\n",
    "    ax.xaxis.pane.set_edgecolor('w')\n",
    "    ax.yaxis.pane.set_edgecolor('w')\n",
    "    ax.zaxis.pane.set_edgecolor('w')\n",
    "    return fig, ax\n",
    "\n",
    "def plot_single_distractor_or_target(palette, xlim, ylim, zlim, label_plot, proj_plot, annotate=False, filename=''):\n",
    "\n",
    "    color=np.array(palette)[label_plot]\n",
    "\n",
    "    # h0_longest,h1_longest,h2_longest = run_ripser(proj_plot,figure_dir+'ripser'+figure_subscript)\n",
    "    fig, ax = plot_isomap(data_plot=proj_plot, color=color, annotate=annotate)\n",
    "    plt.setp(ax, xlim=xlim, ylim=ylim, zlim=zlim)\n",
    "    fig.tight_layout()\n",
    "    if filename is not None:\n",
    "        fig.savefig(filename)\n",
    "    plt.show()\n",
    "    plt.close(fig) \n",
    "\n",
    "\n",
    "def plot_all_isomap_figures(proj,foldername='',filename=''):\n",
    "    fig,ax = plot_isomap(data_plot=proj, color=color1)\n",
    "    set_axes_equal(ax)\n",
    "    fig.tight_layout()\n",
    "    if filename is not None:\n",
    "        fig.savefig(foldername+filename+'_target_isomap.png')\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "    fig,ax = plot_isomap(data_plot=proj, color=color2)\n",
    "    set_axes_equal(ax)\n",
    "    fig.tight_layout()\n",
    "    if filename is not None:\n",
    "        fig.savefig(foldername+filename+'_distractor_isomap.png')\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "    xlim=fig.gca().get_xlim()\n",
    "    ylim=fig.gca().get_ylim()\n",
    "    zlim=fig.gca().get_zlim()\n",
    "\n",
    "    num=0\n",
    "    indices = ind_stim_loc1==num\n",
    "    label_plot = ind_stim_loc2[indices]\n",
    "    proj_plot = proj[indices,:]\n",
    "    plot_single_distractor_or_target(palette = palette2, xlim = xlim, ylim = ylim, zlim = zlim, label_plot=label_plot, proj_plot = proj_plot, filename = foldername+filename+'_single_target.png')\n",
    "\n",
    "    num=0\n",
    "    indices = ind_stim_loc2==num\n",
    "    label_plot = ind_stim_loc1[indices]\n",
    "    proj_plot = proj[indices,:]\n",
    "    plot_single_distractor_or_target(palette = palette1, xlim = xlim, ylim = ylim, zlim = zlim, label_plot=label_plot, proj_plot = proj_plot, filename = foldername+filename+'_single_distractor.png')\n",
    "\n",
    "def generate_hidden_layer_plots(rnn_layer, superscript):\n",
    "    hidden = rnn_layer(x)\n",
    "\n",
    "    for delay in range(1,3):\n",
    "        print('delay:%s'%delay)\n",
    "        delay_bins = get_delay_bins(delay=delay)\n",
    "        #extract mean firing rates for delay bins\n",
    "        delay_hidden = np.mean(hidden[:, delay_bins[0]:delay_bins[1], :], axis=1)\n",
    "    #     delay_hidden = hidden[:, delay_bins[1], :]\n",
    "#         plot_tuning_curves(delay_hidden,tuning_curve_folder,'%s_delay%d_hidden'%(superscript,delay))\n",
    "\n",
    "        proj = fit_isomap(data_to_use=delay_hidden)\n",
    "        plot_all_isomap_figures(proj,isomap_folder,'/%s_delay%d_hidden'%(superscript,delay))\n",
    "\n",
    "        \n",
    "rnn_layer = tf.keras.layers.RNN(cell,input_shape=((x.shape[1],x.shape[2])),\n",
    "                                return_sequences=True, weights=weights_rm_NL)\n",
    "\n",
    "print('generating trained plots......')\n",
    "generate_hidden_layer_plots(rnn_layer, superscript='trained_rm_NL_neurons')\n",
    "performance_dict = {}\n",
    "performance_dict['trained accuracy on %s trials'%x.shape[0]] = round(custom_response_accuracy(y, model.predict(x)),4)\n",
    "performance_dict['trained loss on %s trials'%x.shape[0]] = round(custom_response_loss(y, model.predict(x)),4)\n",
    "performance_dict['trained perf on %s trials'%x.shape[0]] = round(custom_perf(y, model.predict(x)),4)\n",
    "print(performance_dict)\n",
    "\n",
    "with open(os.path.join(model_folder, 'performance_rm_NL_neurons.json'), 'w') as f:\n",
    "    json.dump(performance_dict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a491f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_layer = tf.keras.layers.RNN(cell,input_shape=((x.shape[1],x.shape[2])),\n",
    "                                return_sequences=True, weights=weights_rm_L)\n",
    "print('generating trained plots......')\n",
    "generate_hidden_layer_plots(rnn_layer, superscript='trained_rm_L_neurons')\n",
    "performance_dict = {}\n",
    "performance_dict['trained accuracy on %s trials'%x.shape[0]] = round(custom_response_accuracy(y, model.predict(x)),4)\n",
    "performance_dict['trained loss on %s trials'%x.shape[0]] = round(custom_response_loss(y, model.predict(x)),4)\n",
    "performance_dict['trained perf on %s trials'%x.shape[0]] = round(custom_perf(y, model.predict(x)),4)\n",
    "print(performance_dict)\n",
    "\n",
    "with open(os.path.join(model_folder, 'performance_rm_L_neurons.json'), 'w') as f:\n",
    "    json.dump(performance_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d99ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(0)\n",
    "batch_size=512\n",
    "\n",
    "n_segments = 3\n",
    "\n",
    "stim1_segment = rng.randint(16)\n",
    "stim2_segments = [rng.choice(list(set(np.arange(16))-set([stim1_segment]))) for i in range(n_segments)]\n",
    "stim1_locs = rng.uniform(2*np.pi*stim1_segment/16, 2*np.pi*(stim1_segment+1)/16, (batch_size,))\n",
    "stim2_locs = np.array([rng.choice([rng.uniform(2*np.pi*stim2_segment/16, 2*np.pi*(stim2_segment+1)/16) for stim2_segment in stim2_segments]) for i in range(batch_size)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac1ace6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_segments=1\n",
    "\n",
    "segment_mapping = {}\n",
    "for i in range(16):\n",
    "    segment_list = [j for j in range(16) if j != i]\n",
    "    rng.shuffle(segment_list)\n",
    "    segment_mapping[i] = segment_list[:n_segments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63912f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b8a68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stim1_locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4ab5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "stim1_locs = rng.uniform(0, 2*np.pi, (batch_size,))\n",
    "stim_dist = rng.uniform((0+constraint_factor)*np.pi, (2-constraint_factor)*np.pi,(batch_size,))*rng.choice([-1,1],(batch_size,))\n",
    "stim2_locs = (stim1_locs+stim_dist)%(2*np.pi)\n",
    "\n",
    "stims_mean = rng.uniform(0.8,1.2,(batch_size,))\n",
    "stims_coh  = rng.choice([0.,0.08,0.16,0.32],(batch_size,))\n",
    "stims_sign = rng.choice([1,-1], (batch_size,))\n",
    "\n",
    "stim1_strengths = stims_mean + stims_coh*stims_sign\n",
    "stim2_strengths = stims_mean - stims_coh*stims_sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439eb32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt=20\n",
    "stim1_ons = int(500/dt)\n",
    "stim1_offs = stim1_ons + int(300/dt)\n",
    "fix_offs  = stim1_offs + int(1000/dt)\n",
    "tdim = fix_offs + int(500/dt)\n",
    "check_ons = fix_offs + int(100/dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b724830",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_offs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b74571",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76de9bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stim1_ons = int(500/dt)\n",
    "stim1_offs = stim1_ons + int(300/dt)\n",
    "stim2_ons =stim1_offs + int(1000/dt)\n",
    "stim2_offs = stim2_ons + int(300/dt)\n",
    "fix_offs  = stim2_offs + int(1000/dt)\n",
    "output_2_on = fix_offs + int(500/dt)\n",
    "tdim = output_2_on + int(500/dt)\n",
    "\n",
    "check_ons = fix_offs + int(100/dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f033f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_offs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d870688",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_2_on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0724bfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "1943040/33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0b7390",
   "metadata": {},
   "outputs": [],
   "source": [
    "33*115*128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878b6cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "58880/115"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c69166d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
