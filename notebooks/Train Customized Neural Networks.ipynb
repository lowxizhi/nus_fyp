{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa1353e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pingouin as pg\n",
    "from sklearn import decomposition, manifold\n",
    "from matplotlib import cm\n",
    "import scipy.io\n",
    "from tensorflow.keras.layers import TimeDistributed,Dense,LSTM, SimpleRNN\n",
    "from tensorflow.keras.models import Sequential\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "from tensorflow.python.ops.rnn_cell_impl import RNNCell\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import nn_ops\n",
    "from scipy.stats import ortho_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1dabca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install matplotlib -y\n",
    "# !conda install pandas -y\n",
    "# !conda install scikit-learn -y\n",
    "# !conda install pingouin -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0dae0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe25d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdd183d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_x_loc(x_loc, n_eachring = 32):\n",
    "    \"\"\"Input activity given location.\"\"\"\n",
    "    pref  = np.arange(0,2*np.pi,2*np.pi/n_eachring)\n",
    "    dist = get_dist(x_loc-pref)  # periodic boundary\n",
    "    dist /= np.pi/8\n",
    "    return 0.8*np.exp(-dist**2/2)\n",
    "\n",
    "def add_x_noise(x, seed=0,sigma_x = 0.01):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    x += rng.randn(len(x1))*sigma_x\n",
    "    return x\n",
    "\n",
    "def get_dist(original_dist):\n",
    "    '''Get the distance in periodic boundary conditions'''\n",
    "    return np.minimum(abs(original_dist),2*np.pi-abs(original_dist))\n",
    "\n",
    "n_loc = 128\n",
    "n_stim_loc1, n_stim_loc2, repeat = stim_loc_shape = n_loc, n_loc, 1\n",
    "stim_loc_size = np.prod(stim_loc_shape)\n",
    "ind_stim_loc1, ind_stim_loc2, ind_repeat = np.unravel_index(range(stim_loc_size),stim_loc_shape)\n",
    "stim1_locs = 2*np.pi*ind_stim_loc1/n_stim_loc1\n",
    "stim2_locs = 2*np.pi*ind_stim_loc2/n_stim_loc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776eb0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_n = []\n",
    "for i in range(stim_loc_size):\n",
    "    for seed in range(10):\n",
    "        x1 = add_x_loc(stim1_locs[i])\n",
    "        x2 = add_x_loc(stim2_locs[i])\n",
    "        x1 = add_x_noise(x1, seed=seed)\n",
    "        x2 = add_x_noise(x2, seed=seed)\n",
    "        X_n.append(np.append(x1,x2))\n",
    "X_n = np.array(X_n)\n",
    "\n",
    "X = []\n",
    "for i in range(stim_loc_size):\n",
    "    x1 = add_x_loc(stim1_locs[i])\n",
    "    x2 = add_x_loc(stim2_locs[i])\n",
    "    X.append(np.append(x1,x2))\n",
    "X = np.array(X)\n",
    "\n",
    "\n",
    "\n",
    "seed = 0\n",
    "rng1 = np.random.RandomState(seed)\n",
    "rng1.shuffle(stim1_locs)\n",
    "rng1.shuffle(stim2_locs)\n",
    "\n",
    "X_train = []\n",
    "for i in range(stim_loc_size):\n",
    "    x1 = add_x_loc(stim1_locs[i])\n",
    "    x2 = add_x_loc(stim2_locs[i])\n",
    "    X_train.append(np.append(x1,x2))\n",
    "X_train = np.array(X_train)\n",
    "\n",
    "rng1.shuffle(stim1_locs)\n",
    "rng1.shuffle(stim2_locs)\n",
    "\n",
    "X_test = []\n",
    "for i in range(stim_loc_size):\n",
    "    x1 = add_x_loc(stim1_locs[i])\n",
    "    x2 = add_x_loc(stim2_locs[i])\n",
    "    X_test.append(np.append(x1,x2))\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0683d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = X_n[:,:32]\n",
    "X2 = X_n[:,32:]\n",
    "Zeros = np.zeros(X1.shape)\n",
    "inputs_n = np.array([X1,Zeros,X2,Zeros,Zeros,Zeros])\n",
    "outputs_n = np.array([Zeros,Zeros,Zeros,Zeros,X1,X2])\n",
    "inputs_n = inputs_n.transpose((1, 0, 2))\n",
    "outputs_n = outputs_n.transpose((1, 0, 2))\n",
    "\n",
    "X1 = X[:,:32]\n",
    "X2 = X[:,32:]\n",
    "Zeros = np.zeros(X1.shape)\n",
    "inputs = np.array([X1,Zeros,X2,Zeros,Zeros,Zeros])\n",
    "outputs = np.array([Zeros,Zeros,Zeros,Zeros,X1,X2])\n",
    "inputs = inputs.transpose((1, 0, 2))\n",
    "outputs = outputs.transpose((1, 0, 2))\n",
    "\n",
    "X1 = X_train[:,:32]\n",
    "X2 = X_train[:,32:]\n",
    "Zeros = np.zeros(X1.shape)\n",
    "inputs_train = np.array([X1,Zeros,X2,Zeros,Zeros,Zeros])\n",
    "outputs_train = np.array([Zeros,Zeros,Zeros,Zeros,X1,X2])\n",
    "inputs_train = inputs_train.transpose((1, 0, 2))\n",
    "outputs_train = outputs_train.transpose((1, 0, 2))\n",
    "\n",
    "X1 = X_test[:,:32]\n",
    "X2 = X_test[:,32:]\n",
    "Zeros = np.zeros(X1.shape)\n",
    "inputs_test = np.array([X1,Zeros,X2,Zeros,Zeros,Zeros])\n",
    "outputs_test = np.array([Zeros,Zeros,Zeros,Zeros,X1,X2])\n",
    "inputs_test = inputs_test.transpose((1, 0, 2))\n",
    "outputs_test = outputs_test.transpose((1, 0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2327e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "palette1 = cm.get_cmap('autumn',n_loc+15)\n",
    "palette1 = [palette1(i)[:3] for i in range(n_loc)]\n",
    "color1=np.array(palette1)[ind_stim_loc1]\n",
    "\n",
    "palette2 = cm.get_cmap('summer',n_loc+15)\n",
    "palette2 = [palette2(i)[:3] for i in range(n_loc)]\n",
    "color2=np.array(palette2)[ind_stim_loc2]\n",
    "\n",
    "\n",
    "    \n",
    "def fit_isomap(data_to_use, n_neighbors = 15, target_dim = 3):\n",
    "    iso_instance = manifold.Isomap(n_neighbors = n_neighbors, n_components = target_dim)\n",
    "    proj = iso_instance.fit_transform(data_to_use)\n",
    "    return proj\n",
    "\n",
    "def set_axes_equal(ax):\n",
    "    '''Make axes of 3D plot have equal scale so that spheres appear as spheres,\n",
    "    cubes as cubes, etc..  This is one possible solution to Matplotlib's\n",
    "    ax.set_aspect('equal') and ax.axis('equal') not working for 3D.\n",
    "\n",
    "    Input\n",
    "      ax: a matplotlib axis, e.g., as output from plt.gca().\n",
    "    '''\n",
    "\n",
    "    x_limits = ax.get_xlim3d()\n",
    "    y_limits = ax.get_ylim3d()\n",
    "    z_limits = ax.get_zlim3d()\n",
    "\n",
    "    x_range = abs(x_limits[1] - x_limits[0])\n",
    "    x_middle = np.mean(x_limits)\n",
    "    y_range = abs(y_limits[1] - y_limits[0])\n",
    "    y_middle = np.mean(y_limits)\n",
    "    z_range = abs(z_limits[1] - z_limits[0])\n",
    "    z_middle = np.mean(z_limits)\n",
    "\n",
    "    # The plot bounding box is a sphere in the sense of the infinity\n",
    "    # norm, hence I call half the max range the plot radius.\n",
    "    plot_radius = 0.5*max([x_range, y_range, z_range])\n",
    "\n",
    "    ax.set_xlim3d([x_middle - plot_radius, x_middle + plot_radius])\n",
    "    ax.set_ylim3d([y_middle - plot_radius, y_middle + plot_radius])\n",
    "    ax.set_zlim3d([z_middle - plot_radius, z_middle + plot_radius])\n",
    "\n",
    "def plot_isomap(data_plot, color, annotate=False):\n",
    "    fig = plt.figure(figsize=(16,16),dpi=200)\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    if annotate:\n",
    "        ax.scatter(data_plot[:,0], data_plot[:,1], data_plot[:,2], \n",
    "            s=5, alpha=1, edgecolor='face',c=color)\n",
    "        label = 0\n",
    "        for xyz in zip(data_plot[:,0], data_plot[:,1], data_plot[:,2]):\n",
    "            x, y, z = xyz\n",
    "            ax.text(x, y, z, '%s' % (label), size=5, zorder=1, color='k')\n",
    "            label += 1\n",
    "    else:\n",
    "        ax.scatter(data_plot[:,0], data_plot[:,1], data_plot[:,2], \n",
    "            s=20, alpha=1, edgecolor='face',c=color)\n",
    "    ax.grid(False)\n",
    "    ax.xaxis.pane.fill = False\n",
    "    ax.yaxis.pane.fill = False\n",
    "    ax.zaxis.pane.fill = False\n",
    "    ax.xaxis.pane.set_edgecolor('w')\n",
    "    ax.yaxis.pane.set_edgecolor('w')\n",
    "    ax.zaxis.pane.set_edgecolor('w')\n",
    "    return fig, ax\n",
    "\n",
    "def plot_single_distractor_or_target(palette, xlim, ylim, zlim, label_plot, proj_plot, annotate=False, filename=''):\n",
    "\n",
    "    color=np.array(palette)[label_plot]\n",
    "\n",
    "    # h0_longest,h1_longest,h2_longest = run_ripser(proj_plot,figure_dir+'ripser'+figure_subscript)\n",
    "    fig, ax = plot_isomap(data_plot=proj_plot, color=color, annotate=annotate)\n",
    "    plt.setp(ax, xlim=xlim, ylim=ylim, zlim=zlim)\n",
    "    fig.tight_layout()\n",
    "    if filename is not None:\n",
    "        fig.savefig(filename)\n",
    "    plt.show()\n",
    "    plt.close(fig) \n",
    "\n",
    "\n",
    "def plot_all_isomap_figures(proj,filename=''):\n",
    "    fig,ax = plot_isomap(data_plot=proj, color=color1)\n",
    "    set_axes_equal(ax)\n",
    "    fig.tight_layout()\n",
    "    if filename is not None:\n",
    "        fig.savefig('target_isomap_'+filename)\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "    fig,ax = plot_isomap(data_plot=proj, color=color2)\n",
    "    set_axes_equal(ax)\n",
    "    fig.tight_layout()\n",
    "    if filename is not None:\n",
    "        fig.savefig('distractor_isomap_'+filename)\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "    xlim=fig.gca().get_xlim()\n",
    "    ylim=fig.gca().get_ylim()\n",
    "    zlim=fig.gca().get_zlim()\n",
    "\n",
    "    num=0\n",
    "    indices = ind_stim_loc1==num\n",
    "    label_plot = ind_stim_loc2[indices]\n",
    "    proj_plot = proj[indices,:]\n",
    "    plot_single_distractor_or_target(palette = palette2, xlim = xlim, ylim = ylim, zlim = zlim, label_plot=label_plot, proj_plot = proj_plot, filename = 'single_target_'+filename)\n",
    "\n",
    "    num=0\n",
    "    indices = ind_stim_loc2==num\n",
    "    label_plot = ind_stim_loc1[indices]\n",
    "    proj_plot = proj[indices,:]\n",
    "    plot_single_distractor_or_target(palette = palette1, xlim = xlim, ylim = ylim, zlim = zlim, label_plot=label_plot, proj_plot = proj_plot, filename = 'single_distractor_'+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660da3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def popvec(y):\n",
    "    \"\"\"Population vector read out.\n",
    "\n",
    "    Assuming the last dimension is the dimension to be collapsed\n",
    "\n",
    "    Args:\n",
    "        y: population output on a ring network. Numpy array (Batch, Units)\n",
    "\n",
    "    Returns:\n",
    "        Readout locations: Numpy array (Batch,)\n",
    "    \"\"\"\n",
    "    pref = np.arange(0, 2*np.pi, 2*np.pi/y.shape[-1])  # preferences\n",
    "    temp_sum = y.sum(axis=-1)\n",
    "    temp_cos = np.sum(y*np.cos(pref), axis=-1)/temp_sum\n",
    "    temp_sin = np.sum(y*np.sin(pref), axis=-1)/temp_sum\n",
    "    loc = np.arctan2(temp_sin, temp_cos)\n",
    "    return np.mod(loc, 2*np.pi)\n",
    "\n",
    "def get_model_performance(model):\n",
    "    y_hat = model.predict(inputs)\n",
    "\n",
    "    y_hat_loc1 = popvec(y_hat[:,-2,:])\n",
    "    outputs_loc1 = popvec(outputs[:,-2,:])\n",
    "    original_dist1 = outputs_loc1 - y_hat_loc1\n",
    "    dist1 = np.minimum(abs(original_dist1), 2*np.pi-abs(original_dist1))\n",
    "    corr_loc1 = dist1 < 2*np.pi/128\n",
    "\n",
    "    y_hat_loc2 = popvec(y_hat[:,-1,:])\n",
    "    outputs_loc2 = popvec(outputs[:,-1,:])\n",
    "    original_dist2 = outputs_loc2 - y_hat_loc2\n",
    "    dist2 = np.minimum(abs(original_dist2), 2*np.pi-abs(original_dist2))\n",
    "    corr_loc2 = dist2 < 2*np.pi/128\n",
    "\n",
    "    return np.sum(corr_loc1*0.5+corr_loc2*0.5)/len(inputs)\n",
    "\n",
    "\n",
    "def plot_loss_over_epochs(history, foldername=''):\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.plot(history.history['loss'],label=\"Training set loss\")\n",
    "    plt.plot(history.history['val_loss'],label=\"Validation set loss\")\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    plt.savefig('%sloss_over_epochs.png'%foldername)\n",
    "\n",
    "def get_anova_stats(foldername, rnn_layer):\n",
    "    hidden = rnn_layer(inputs_n)\n",
    "\n",
    "    delay1_hidden = hidden[:,1,:]\n",
    "    delay2_hidden = hidden[:,3,:]\n",
    "\n",
    "    neuron = 0\n",
    "    df = pd.DataFrame({'first_stim':np.array([[i]*10 for i in ind_stim_loc1]).flatten(),'second_stim':np.array([[i]*10 for i in ind_stim_loc2]).flatten(),'delay1_activity':delay1_hidden[:,neuron],'delay2_activity':delay2_hidden[:,neuron]})\n",
    "    aov = pg.anova(dv='delay1_activity', between=['first_stim', 'second_stim'], data=df,\n",
    "              detailed=True)\n",
    "    print(aov)\n",
    "    aov.to_csv(foldername+\"delay1_anova.csv\")\n",
    "    aov = pg.anova(dv='delay2_activity', between=['first_stim', 'second_stim'], data=df,\n",
    "              detailed=True)\n",
    "    print(aov)\n",
    "    aov.to_csv(foldername+\"delay2_anova.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eca8b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tuning_curves(Z,foldername='',filename=''):    \n",
    "    fig,ax = plt.subplots(min(Z.shape[1],10),1,figsize=(5,min(Z.shape[1],10)*3))\n",
    "    for i in range(min(Z.shape[1],10)):\n",
    "        if Z.shape[1]<10:\n",
    "            neuron = i\n",
    "        else:\n",
    "            neuron = i*int(Z.shape[1]/10)\n",
    "        for loc1 in [j*10 for j in range(int(128/10))]:\n",
    "            df = pd.DataFrame({'first_stim':ind_stim_loc1[ind_stim_loc1==loc1],'second_stim':ind_stim_loc2[ind_stim_loc1==loc1],'activity':Z[:,neuron][ind_stim_loc1==loc1]})\n",
    "            x = df['second_stim']\n",
    "            y = df['activity']\n",
    "            ax[i].scatter(x,y,s=1, color=palette1[loc1], label=loc1)\n",
    "        ax[i].set_ylabel('%dth neuron'%neuron, fontsize=13)\n",
    "    ax[0].legend(loc='upper left', bbox_to_anchor= (1.05, 1.05), title='Stim 1')\n",
    "    ax[0].set_xlabel('Stim 2', fontsize=13)\n",
    "    ax[0].xaxis.set_label_position('top') \n",
    "    if filename != '':\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(foldername+filename+'_stim2')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "        \n",
    "    fig,ax = plt.subplots(min(Z.shape[1],10),1,figsize=(5,min(Z.shape[1],10)*3))\n",
    "    for i in range(min(Z.shape[1],10)):\n",
    "        if Z.shape[1]<10:\n",
    "            neuron = i\n",
    "        else:\n",
    "            neuron = i*int(Z.shape[1]/10)\n",
    "        for loc2 in [j*10 for j in range(int(128/10))]:\n",
    "            df = pd.DataFrame({'first_stim':ind_stim_loc1[ind_stim_loc2==loc2],'second_stim':ind_stim_loc2[ind_stim_loc2==loc2],'activity':Z[:,neuron][ind_stim_loc2==loc2]})\n",
    "            x = df['first_stim']\n",
    "            y = df['activity']\n",
    "            ax[i].scatter(x,y,s=1, color=palette2[loc2], label=loc2)\n",
    "        ax[i].set_ylabel('%dth neuron'%neuron, fontsize=13)\n",
    "    ax[0].legend(loc='upper left', bbox_to_anchor= (1.05, 1.05), title='Stim 2')\n",
    "    ax[0].set_xlabel('Stim 1', fontsize=13)\n",
    "    ax[0].xaxis.set_label_position('top') \n",
    "    if filename != '':\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(foldername+filename+'_stim1')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6094a8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Create folders to save model and training info\n",
    "# model_folder = 'simple_rnn_%s_n_hidden_%s'%(datetime.now(pytz.timezone('Asia/Singapore')).strftime(\"%d_%m_%Y_%H_%M_%S\"),n_hidden)\n",
    "# checkpoint_path = model_folder+\"/cp.ckpt\"\n",
    "# saved_model_folder = model_folder+'/saved_model'\n",
    "# loss_curve_folder = model_folder+'/'\n",
    "# anova_folder = model_folder+'/'\n",
    "\n",
    "# os.makedirs(model_folder)\n",
    "\n",
    "n_hidden = 8\n",
    "\n",
    "### Create the model\n",
    "model = Sequential()\n",
    "simplernn = SimpleRNN(n_hidden, input_shape=(inputs_train.shape[1:]), return_sequences=True)\n",
    "model.add(simplernn)\n",
    "model.add(TimeDistributed(Dense(32)))\n",
    "# model.summary()\n",
    "\n",
    "### Check performance and selectivity before training\n",
    "print(\"n_hidden %s untrained performance: %s\"%(n_hidden, get_model_performance(model)))\n",
    "\n",
    "hidden = simplernn(inputs)\n",
    "delay1_hidden = hidden[:,1,:]\n",
    "delay2_hidden = hidden[:,3,:]\n",
    "plot_tuning_curves(delay1_hidden,'untrained_delay1_hidden')\n",
    "plot_tuning_curves(delay2_hidden,'untrained_delay2_hidden')\n",
    "\n",
    "get_anova_stats(anova_folder+'untrained_', simplernn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4ee6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create callbacks to saves the model's weights and earlystopping\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                save_weights_only=True,\n",
    "                                                verbose=1)\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(restore_best_weights=True, patience=10)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),loss=tf.keras.losses.mse)\n",
    "\n",
    "### Training the model\n",
    "history = model.fit(inputs_train, outputs_train, validation_data=(inputs_test,outputs_test), batch_size=64, epochs=500, verbose=2,\n",
    "          callbacks=[cp_callback, es_callback])\n",
    "plot_loss_over_epochs(history, foldername=loss_curve_folder)\n",
    "model.save(saved_model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359c3598",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(saved_model_folder)\n",
    "\n",
    "### Check performance and selectivity after training\n",
    "print(\"n_hidden %s trained performance: %s\"%(n_hidden, get_model_performance(model)))\n",
    "\n",
    "simplernn = SimpleRNN(n_hidden, input_shape=(inputs_train.shape[1:]), return_sequences=True, weights=model.layers[0].get_weights())\n",
    "hidden = simplernn(inputs)\n",
    "\n",
    "delay1_hidden = hidden[:,1,:]\n",
    "delay2_hidden = hidden[:,3,:]\n",
    "plot_tuning_curves(delay1_hidden,'trained_delay1_hidden')\n",
    "plot_tuning_curves(delay2_hidden,'trained_delay2_hidden')\n",
    "\n",
    "get_anova_stats(anova_folder, simplernn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045aa401",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinimalRNNCell(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, units, **kwargs):\n",
    "        self.units = units\n",
    "        self.state_size = units\n",
    "        super(MinimalRNNCell, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(shape=(input_shape[-1], self.units),\n",
    "                                      initializer='uniform',\n",
    "                                      name='kernel')\n",
    "        self.recurrent_kernel = self.add_weight(\n",
    "            shape=(self.units, self.units),\n",
    "            initializer='uniform',\n",
    "            name='recurrent_kernel')\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, states):\n",
    "        prev_output = states[0]\n",
    "        h = tf.keras.backend.dot(inputs, self.kernel)\n",
    "        output = h + tf.keras.backend.dot(prev_output, self.recurrent_kernel)\n",
    "        return output, [output]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0c0860",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create folders to save model and training info\n",
    "n_hidden = 8\n",
    "\n",
    "model_folder = 'minimal_rnn_%s_n_hidden_%s'%(datetime.now(pytz.timezone('Asia/Singapore')).strftime(\"%d_%m_%Y_%H_%M_%S\"),n_hidden)\n",
    "checkpoint_path = model_folder+\"/cp.ckpt\"\n",
    "saved_model_folder = model_folder+'/saved_model'\n",
    "loss_curve_folder = model_folder+'/'\n",
    "anova_folder = model_folder+'/'\n",
    "\n",
    "os.makedirs(model_folder)\n",
    "\n",
    "cell = MinimalRNNCell(n_hidden)\n",
    "rnn_layer = tf.keras.layers.RNN(cell,input_shape=(inputs_train.shape[1:]),\n",
    "                                return_sequences=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763bff5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = rnn_layer(inputs)\n",
    "delay1_hidden = hidden[:,1,:]\n",
    "delay2_hidden = hidden[:,3,:]\n",
    "plot_tuning_curves(delay1_hidden,'untrained_delay1_hidden')\n",
    "plot_tuning_curves(delay2_hidden,'untrained_delay2_hidden')\n",
    "\n",
    "get_anova_stats(anova_folder+'untrained_', rnn_layer)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(rnn_layer)\n",
    "model.add(TimeDistributed(Dense(32)))\n",
    "model.summary()\n",
    "\n",
    "### Check performance and selectivity before training\n",
    "print(\"n_hidden %s untrained performance: %s\"%(n_hidden, get_model_performance(model)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bac63ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create callbacks to saves the model's weights and earlystopping\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                save_weights_only=True,\n",
    "                                                verbose=1)\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(restore_best_weights=True, patience=10)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),loss=tf.keras.losses.mse)\n",
    "\n",
    "### Training the model\n",
    "history = model.fit(inputs_train, outputs_train, validation_data=(inputs_test,outputs_test), batch_size=64, epochs=500, verbose=2,\n",
    "          callbacks=[cp_callback, es_callback])\n",
    "plot_loss_over_epochs(history, foldername=loss_curve_folder)\n",
    "model.save(saved_model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fd361c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.models.load_model(saved_model_folder)\n",
    "\n",
    "### Check performance and selectivity after training\n",
    "print(\"n_hidden %s trained performance: %s\"%(n_hidden, get_model_performance(model)))\n",
    "\n",
    "# simplernn = SimpleRNN(n_hidden, input_shape=(inputs_train.shape[1:]), return_sequences=True, weights=model.layers[0].get_weights())\n",
    "rnn_layer = tf.keras.layers.RNN(cell,input_shape=(inputs_train.shape[1:]),\n",
    "                                return_sequences=True, weights=model.layers[0].get_weights())\n",
    "hidden = rnn_layer(inputs)\n",
    "\n",
    "delay1_hidden = hidden[:,1,:]\n",
    "delay2_hidden = hidden[:,3,:]\n",
    "plot_tuning_curves(delay1_hidden,'trained_delay1_hidden')\n",
    "plot_tuning_curves(delay2_hidden,'trained_delay2_hidden')\n",
    "\n",
    "get_anova_stats(anova_folder, rnn_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f579de8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4b0fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create folders to save model and training info\n",
    "n_hidden = 8\n",
    "\n",
    "model_folder = 'leaky2_rnn_%s_n_hidden_%s'%(datetime.now(pytz.timezone('Asia/Singapore')).strftime(\"%d_%m_%Y_%H_%M_%S\"),n_hidden)\n",
    "checkpoint_path = model_folder+\"/cp.ckpt\"\n",
    "saved_model_folder = model_folder+'/saved_model'\n",
    "loss_curve_folder = anova_folder = tuning_curve_folder = model_folder+'/'\n",
    "\n",
    "os.makedirs(model_folder)\n",
    "\n",
    "cell = LeakyRNNCell2(n_hidden)\n",
    "rnn_layer = tf.keras.layers.RNN(cell,input_shape=(inputs_train.shape[1:]),\n",
    "                                return_sequences=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bd530f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = rnn_layer(inputs)\n",
    "delay1_hidden = hidden[:,1,:]\n",
    "delay2_hidden = hidden[:,3,:]\n",
    "plot_tuning_curves(delay1_hidden,tuning_curve_folder,'untrained_delay1_hidden')\n",
    "plot_tuning_curves(delay2_hidden,tuning_curve_folder,'untrained_delay2_hidden')\n",
    "\n",
    "get_anova_stats(anova_folder+'untrained_', rnn_layer)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(rnn_layer)\n",
    "model.add(TimeDistributed(Dense(32)))\n",
    "model.summary()\n",
    "\n",
    "### Check performance and selectivity before training\n",
    "print(\"n_hidden %s untrained performance: %s\"%(n_hidden, get_model_performance(model)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d1765c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create callbacks to saves the model's weights and earlystopping\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                save_weights_only=True,\n",
    "                                                verbose=1)\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(restore_best_weights=True, patience=10)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),loss=tf.keras.losses.mse)\n",
    "\n",
    "### Training the model\n",
    "history = model.fit(inputs_train, outputs_train, validation_data=(inputs_test,outputs_test), batch_size=64, epochs=500, verbose=2,\n",
    "          callbacks=[cp_callback, es_callback])\n",
    "plot_loss_over_epochs(history, foldername=loss_curve_folder)\n",
    "model.save(saved_model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1107bca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.models.load_model(saved_model_folder)\n",
    "\n",
    "### Check performance and selectivity after training\n",
    "print(\"n_hidden %s trained performance: %s\"%(n_hidden, get_model_performance(model)))\n",
    "\n",
    "# simplernn = SimpleRNN(n_hidden, input_shape=(inputs_train.shape[1:]), return_sequences=True, weights=model.layers[0].get_weights())\n",
    "rnn_layer = tf.keras.layers.RNN(cell,input_shape=(inputs_train.shape[1:]),\n",
    "                                return_sequences=True, weights=model.layers[0].get_weights())\n",
    "hidden = rnn_layer(inputs)\n",
    "\n",
    "delay1_hidden = hidden[:,1,:]\n",
    "delay2_hidden = hidden[:,3,:]\n",
    "plot_tuning_curves(delay1_hidden,tuning_curve_folder,'trained_delay1_hidden')\n",
    "plot_tuning_curves(delay2_hidden,tuning_curve_folder,'trained_delay2_hidden')\n",
    "\n",
    "get_anova_stats(anova_folder, rnn_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5255e30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeakyRNNCell2(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, units, **kwargs):\n",
    "        alpha = 0.2\n",
    "        sigma_rec = 0\n",
    "        self.units = units\n",
    "        self.state_size = units\n",
    "        self._activation = tf.nn.relu\n",
    "        self._w_in_start = 1.0\n",
    "        self._w_rec_start = 0.5\n",
    "        self.rng = np.random.RandomState()\n",
    "        self._alpha = alpha\n",
    "        self._sigma = np.sqrt(2 / alpha) * sigma_rec\n",
    "        super(LeakyRNNCell2, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        w_in0 = (self.rng.randn(input_shape[-1], self.units) /\n",
    "                 np.sqrt(input_shape[-1]) * self._w_in_start)\n",
    "        self.kernel = self.add_weight(shape=(input_shape[-1], self.units), dtype=tf.float32,\n",
    "                                      initializer=tf.constant_initializer(w_in0),\n",
    "                                      name='kernel')\n",
    "        w_rec0 = self._w_rec_start*ortho_group.rvs(dim=self.units, random_state=self.rng)\n",
    "        self.recurrent_kernel = self.add_weight(shape=(self.units, self.units), dtype=tf.float32,\n",
    "                                                initializer=tf.constant_initializer(w_rec0),\n",
    "                                                name='recurrent_kernel')\n",
    "        matrix0 = np.concatenate((w_in0, w_rec0), axis=0)\n",
    "    \n",
    "#         self.kernel = self.add_weight(\n",
    "#                 name='kernel',\n",
    "#                 shape=[input_shape[-1] + self.units, self.units], \n",
    "#                 dtype=tf.float32,\n",
    "#                 initializer=tf.constant_initializer(matrix0))\n",
    "        \n",
    "        self._bias = self.add_weight(\n",
    "                name='bias',\n",
    "                shape=[self.units],\n",
    "                dtype=tf.float32,\n",
    "                initializer=tf.zeros_initializer())\n",
    "        \n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, states):\n",
    "        prev_output = states[0]\n",
    "        h = tf.keras.backend.dot(inputs, self.kernel)\n",
    "#         h = tf.keras.backend.dot(array_ops.concat([inputs, prev_output], 1), self.kernel)\n",
    "        h = h + tf.keras.backend.dot(prev_output, self.recurrent_kernel)\n",
    "        h = tf.nn.bias_add(h, self._bias)\n",
    "        noise = tf.random.normal(tf.shape(prev_output), mean=0, stddev=self._sigma)\n",
    "        h = h + noise\n",
    "        output = self._activation(h)\n",
    "        output = (1-self._alpha) * prev_output + self._alpha * output\n",
    "        return output, [output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7e6351",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    \"\"\"The model.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 model_dir,\n",
    "                 hp=None,\n",
    "                 sigma_rec=None,\n",
    "                 dt=None):\n",
    "        \"\"\"\n",
    "        Initializing the model with information from hp\n",
    "\n",
    "        Args:\n",
    "            model_dir: string, directory of the model\n",
    "            hp: a dictionary or None\n",
    "            sigma_rec: if not None, overwrite the sigma_rec passed by hp\n",
    "        \"\"\"\n",
    "\n",
    "        # Reset tensorflow graphs\n",
    "        tf.reset_default_graph()  # must be in the beginning\n",
    "\n",
    "        if hp is None:\n",
    "            hp = tools.load_hp(model_dir)\n",
    "            if hp is None:\n",
    "                raise ValueError(\n",
    "                    'No hp found for model_dir {:s}'.format(model_dir))\n",
    "\n",
    "        tf.set_random_seed(hp['seed'])\n",
    "        self.rng = np.random.RandomState(hp['seed'])\n",
    "\n",
    "        if sigma_rec is not None:\n",
    "            print('Overwrite sigma_rec with {:0.3f}'.format(sigma_rec))\n",
    "            hp['sigma_rec'] = sigma_rec\n",
    "\n",
    "        if dt is not None:\n",
    "            print('Overwrite original dt with {:0.1f}'.format(dt))\n",
    "            hp['dt'] = dt\n",
    "\n",
    "        hp['alpha'] = 1.0*hp['dt']/hp['tau']\n",
    "\n",
    "        # Input, target output, and cost mask\n",
    "        # Shape: [Time, Batch, Num_units]\n",
    "        if hp['in_type'] != 'normal':\n",
    "            raise ValueError('Only support in_type ' + hp['in_type'])\n",
    "\n",
    "        self._build(hp)\n",
    "\n",
    "        self.model_dir = model_dir\n",
    "        self.hp = hp\n",
    "\n",
    "    def _build(self, hp):\n",
    "        if 'use_separate_input' in hp and hp['use_separate_input']:\n",
    "            self._build_seperate(hp)\n",
    "        else:\n",
    "            self._build_fused(hp)\n",
    "\n",
    "        self.var_list = tf.trainable_variables()\n",
    "        self.weight_list = [v for v in self.var_list if is_weight(v)]\n",
    "\n",
    "        if 'use_separate_input' in hp and hp['use_separate_input']:\n",
    "            self._set_weights_separate(hp)\n",
    "        else:\n",
    "            self._set_weights_fused(hp)\n",
    "\n",
    "        # Regularization terms\n",
    "        self.cost_reg = tf.constant(0.)\n",
    "        if hp['l1_h'] > 0:\n",
    "            self.cost_reg += tf.reduce_mean(tf.abs(self.h)) * hp['l1_h']\n",
    "        if hp['l2_h'] > 0:\n",
    "            self.cost_reg += tf.nn.l2_loss(self.h) * hp['l2_h']\n",
    "\n",
    "        if hp['l1_weight'] > 0:\n",
    "            self.cost_reg += hp['l1_weight'] * tf.add_n(\n",
    "                [tf.reduce_mean(tf.abs(v)) for v in self.weight_list])\n",
    "        if hp['l2_weight'] > 0:\n",
    "            self.cost_reg += hp['l2_weight'] * tf.add_n(\n",
    "                [tf.nn.l2_loss(v) for v in self.weight_list])\n",
    "\n",
    "        # Create an optimizer.\n",
    "        if 'optimizer' not in hp or hp['optimizer'] == 'adam':\n",
    "            self.opt = tf.train.AdamOptimizer(\n",
    "                learning_rate=hp['learning_rate'])\n",
    "        elif hp['optimizer'] == 'sgd':\n",
    "            self.opt = tf.train.GradientDescentOptimizer(\n",
    "                learning_rate=hp['learning_rate'])\n",
    "        # Set cost\n",
    "        self.set_optimizer()\n",
    "\n",
    "        # Variable saver\n",
    "        # self.saver = tf.train.Saver(self.var_list)\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "    def _build_fused(self, hp):\n",
    "        n_input = hp['n_input']\n",
    "        n_rnn = hp['n_rnn']\n",
    "        n_output = hp['n_output']\n",
    "\n",
    "        self.x = tf.placeholder(\"float\", [None, None, n_input])\n",
    "        self.y = tf.placeholder(\"float\", [None, None, n_output])\n",
    "        if hp['loss_type'] == 'lsq':\n",
    "            self.c_mask = tf.placeholder(\"float\", [None, n_output])\n",
    "        else:\n",
    "            # Mask on time\n",
    "            self.c_mask = tf.placeholder(\"float\", [None])\n",
    "\n",
    "        # Activation functions\n",
    "        if hp['activation'] == 'power':\n",
    "            f_act = lambda x: tf.square(tf.nn.relu(x))\n",
    "        elif hp['activation'] == 'retanh':\n",
    "            f_act = lambda x: tf.tanh(tf.nn.relu(x))\n",
    "        elif hp['activation'] == 'relu+':\n",
    "            f_act = lambda x: tf.nn.relu(x + tf.constant(1.))\n",
    "        else:\n",
    "            f_act = getattr(tf.nn, hp['activation'])\n",
    "\n",
    "        # Recurrent activity\n",
    "        if hp['rnn_type'] == 'LeakyRNN':\n",
    "            n_in_rnn = self.x.get_shape().as_list()[-1]\n",
    "            cell = LeakyRNNCell(n_rnn, \n",
    "                                n_in_rnn,\n",
    "                                hp['alpha'],\n",
    "                                sigma_rec=hp['sigma_rec'],\n",
    "                                activation=hp['activation'],\n",
    "                                w_rec_init=hp['w_rec_init'],\n",
    "                                rng=self.rng)\n",
    "        elif hp['rnn_type'] == 'LeakyGRU':\n",
    "            cell = LeakyGRUCell(\n",
    "                n_rnn, hp['alpha'],\n",
    "                sigma_rec=hp['sigma_rec'], activation=f_act)\n",
    "        elif hp['rnn_type'] == 'LSTM':\n",
    "            cell = tf.contrib.rnn.LSTMCell(n_rnn, activation=f_act)\n",
    "\n",
    "        elif hp['rnn_type'] == 'GRU':\n",
    "            cell = tf.contrib.rnn.GRUCell(n_rnn, activation=f_act)\n",
    "        else:\n",
    "            raise NotImplementedError(\"\"\"rnn_type must be one of LeakyRNN,\n",
    "                    LeakyGRU, EILeakyGRU, LSTM, GRU\n",
    "                    \"\"\")\n",
    "\n",
    "        # Dynamic rnn with time major\n",
    "        self.h, states = rnn.dynamic_rnn(\n",
    "            cell, self.x, dtype=tf.float32, time_major=True)\n",
    "\n",
    "        # Output\n",
    "        with tf.variable_scope(\"output\"):\n",
    "            # Using default initialization `glorot_uniform_initializer`\n",
    "            w_out = tf.get_variable(\n",
    "                'weights',\n",
    "                [n_rnn, n_output],\n",
    "                dtype=tf.float32\n",
    "            )\n",
    "            b_out = tf.get_variable(\n",
    "                'biases',\n",
    "                [n_output],\n",
    "                dtype=tf.float32,\n",
    "                initializer=tf.constant_initializer(0.0, dtype=tf.float32)\n",
    "            )\n",
    "\n",
    "        h_shaped = tf.reshape(self.h, (-1, n_rnn))\n",
    "        y_shaped = tf.reshape(self.y, (-1, n_output))\n",
    "        # y_hat_ shape (n_time*n_batch, n_unit)\n",
    "        y_hat_ = tf.matmul(h_shaped, w_out) + b_out\n",
    "        if hp['loss_type'] == 'lsq':\n",
    "            # Least-square loss\n",
    "            y_hat = tf.sigmoid(y_hat_)\n",
    "            self.cost_lsq = tf.reduce_mean(\n",
    "                tf.square((y_shaped - y_hat) * self.c_mask))\n",
    "        else:\n",
    "            y_hat = tf.nn.softmax(y_hat_)\n",
    "            # Cross-entropy loss\n",
    "            self.cost_lsq = tf.reduce_mean(\n",
    "                self.c_mask * tf.nn.softmax_cross_entropy_with_logits(\n",
    "                    labels=y_shaped, logits=y_hat_))\n",
    "\n",
    "        self.y_hat = tf.reshape(y_hat,\n",
    "                                (-1, tf.shape(self.h)[1], n_output))\n",
    "        y_hat_fix, y_hat_ring = tf.split(\n",
    "            self.y_hat, [1, n_output - 1], axis=-1)\n",
    "        self.y_hat_loc = tf_popvec(y_hat_ring)\n",
    "\n",
    "    def _set_weights_fused(self, hp):\n",
    "        \"\"\"Set model attributes for several weight variables.\"\"\"\n",
    "        n_input = hp['n_input']\n",
    "        n_rnn = hp['n_rnn']\n",
    "        n_output = hp['n_output']\n",
    "\n",
    "        for v in self.var_list:\n",
    "            if 'rnn' in v.name:\n",
    "                if 'kernel' in v.name or 'weight' in v.name:\n",
    "                    # TODO(gryang): For GRU, fix\n",
    "                    self.w_rec = v[n_input:, :]\n",
    "                    self.w_in = v[:n_input, :]\n",
    "                else:\n",
    "                    self.b_rec = v\n",
    "            else:\n",
    "                assert 'output' in v.name\n",
    "                if 'kernel' in v.name or 'weight' in v.name:\n",
    "                    self.w_out = v\n",
    "                else:\n",
    "                    self.b_out = v\n",
    "\n",
    "        # check if the recurrent and output connection has the correct shape\n",
    "        if self.w_out.shape != (n_rnn, n_output):\n",
    "            raise ValueError('Shape of w_out should be ' +\n",
    "                             str((n_rnn, n_output)) + ', but found ' +\n",
    "                             str(self.w_out.shape))\n",
    "        if self.w_rec.shape != (n_rnn, n_rnn):\n",
    "            raise ValueError('Shape of w_rec should be ' +\n",
    "                             str((n_rnn, n_rnn)) + ', but found ' +\n",
    "                             str(self.w_rec.shape))\n",
    "        if self.w_in.shape != (n_input, n_rnn):\n",
    "            raise ValueError('Shape of w_in should be ' +\n",
    "                             str((n_input, n_rnn)) + ', but found ' +\n",
    "                             str(self.w_in.shape))\n",
    "\n",
    "    def _build_seperate(self, hp):\n",
    "        # Input, target output, and cost mask\n",
    "        # Shape: [Time, Batch, Num_units]\n",
    "        n_input = hp['n_input']\n",
    "        n_rnn = hp['n_rnn']\n",
    "        n_output = hp['n_output']\n",
    "\n",
    "        self.x = tf.placeholder(\"float\", [None, None, n_input])\n",
    "        self.y = tf.placeholder(\"float\", [None, None, n_output])\n",
    "        self.c_mask = tf.placeholder(\"float\", [None, n_output])\n",
    "\n",
    "        sensory_inputs, rule_inputs = tf.split(\n",
    "            self.x, [hp['rule_start'], hp['n_rule']], axis=-1)\n",
    "\n",
    "        sensory_rnn_inputs = tf.layers.dense(sensory_inputs, n_rnn, name='sen_input')\n",
    "\n",
    "        if 'mix_rule' in hp and hp['mix_rule'] is True:\n",
    "            # rotate rule matrix\n",
    "            kernel_initializer = tf.orthogonal_initializer()\n",
    "            rule_inputs = tf.layers.dense(\n",
    "                rule_inputs, hp['n_rule'], name='mix_rule',\n",
    "                use_bias=False, trainable=False,\n",
    "                kernel_initializer=kernel_initializer)\n",
    "\n",
    "        rule_rnn_inputs = tf.layers.dense(rule_inputs, n_rnn, name='rule_input', use_bias=False)\n",
    "\n",
    "        rnn_inputs = sensory_rnn_inputs + rule_rnn_inputs\n",
    "\n",
    "        # Recurrent activity\n",
    "        cell = LeakyRNNCellSeparateInput(\n",
    "            n_rnn, hp['alpha'],\n",
    "            sigma_rec=hp['sigma_rec'],\n",
    "            activation=hp['activation'],\n",
    "            w_rec_init=hp['w_rec_init'],\n",
    "            rng=self.rng)\n",
    "\n",
    "        # Dynamic rnn with time major\n",
    "        self.h, states = rnn.dynamic_rnn(\n",
    "            cell, rnn_inputs, dtype=tf.float32, time_major=True)\n",
    "\n",
    "        # Output\n",
    "        h_shaped = tf.reshape(self.h, (-1, n_rnn))\n",
    "        y_shaped = tf.reshape(self.y, (-1, n_output))\n",
    "        # y_hat shape (n_time*n_batch, n_unit)\n",
    "        y_hat = tf.layers.dense(\n",
    "            h_shaped, n_output, activation=tf.nn.sigmoid, name='output')\n",
    "        # Least-square loss\n",
    "        self.cost_lsq = tf.reduce_mean(\n",
    "            tf.square((y_shaped - y_hat) * self.c_mask))\n",
    "\n",
    "        self.y_hat = tf.reshape(y_hat,\n",
    "                                (-1, tf.shape(self.h)[1], n_output))\n",
    "        y_hat_fix, y_hat_ring = tf.split(\n",
    "            self.y_hat, [1, n_output - 1], axis=-1)\n",
    "        self.y_hat_loc = tf_popvec(y_hat_ring)\n",
    "\n",
    "    def _set_weights_separate(self, hp):\n",
    "        \"\"\"Set model attributes for several weight variables.\"\"\"\n",
    "        n_input = hp['n_input']\n",
    "        n_rnn = hp['n_rnn']\n",
    "        n_output = hp['n_output']\n",
    "\n",
    "        for v in self.var_list:\n",
    "            if 'rnn' in v.name:\n",
    "                if 'kernel' in v.name or 'weight' in v.name:\n",
    "                    self.w_rec = v\n",
    "                else:\n",
    "                    self.b_rec = v\n",
    "            elif 'sen_input' in v.name:\n",
    "                if 'kernel' in v.name or 'weight' in v.name:\n",
    "                    self.w_sen_in = v\n",
    "                else:\n",
    "                    self.b_in = v\n",
    "            elif 'rule_input' in v.name:\n",
    "                self.w_rule = v\n",
    "            else:\n",
    "                assert 'output' in v.name\n",
    "                if 'kernel' in v.name or 'weight' in v.name:\n",
    "                    self.w_out = v\n",
    "                else:\n",
    "                    self.b_out = v\n",
    "\n",
    "        # check if the recurrent and output connection has the correct shape\n",
    "        if self.w_out.shape != (n_rnn, n_output):\n",
    "            raise ValueError('Shape of w_out should be ' +\n",
    "                             str((n_rnn, n_output)) + ', but found ' +\n",
    "                             str(self.w_out.shape))\n",
    "        if self.w_rec.shape != (n_rnn, n_rnn):\n",
    "            raise ValueError('Shape of w_rec should be ' +\n",
    "                             str((n_rnn, n_rnn)) + ', but found ' +\n",
    "                             str(self.w_rec.shape))\n",
    "        if self.w_sen_in.shape != (hp['rule_start'], n_rnn):\n",
    "            raise ValueError('Shape of w_sen_in should be ' +\n",
    "                             str((hp['rule_start'], n_rnn)) + ', but found ' +\n",
    "                             str(self.w_sen_in.shape))\n",
    "        if self.w_rule.shape != (hp['n_rule'], n_rnn):\n",
    "            raise ValueError('Shape of w_in should be ' +\n",
    "                             str((hp['n_rule'], n_rnn)) + ', but found ' +\n",
    "                             str(self.w_rule.shape))\n",
    "\n",
    "    def initialize(self):\n",
    "        \"\"\"Initialize the model for training.\"\"\"\n",
    "        sess = tf.get_default_session()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def restore(self, load_dir=None):\n",
    "        \"\"\"restore the model\"\"\"\n",
    "        sess = tf.get_default_session()\n",
    "        if load_dir is None:\n",
    "            load_dir = self.model_dir\n",
    "        save_path = os.path.join(load_dir, 'model.ckpt')\n",
    "        try:\n",
    "            self.saver.restore(sess, save_path)\n",
    "        except:\n",
    "            # Some earlier checkpoints only stored trainable variables\n",
    "            self.saver = tf.train.Saver(self.var_list)\n",
    "            self.saver.restore(sess, save_path)\n",
    "        print(\"Model restored from file: %s\" % save_path)\n",
    "\n",
    "    def save(self):\n",
    "        \"\"\"Save the model.\"\"\"\n",
    "        sess = tf.get_default_session()\n",
    "        save_path = os.path.join(self.model_dir, 'model.ckpt')\n",
    "        self.saver.save(sess, save_path)\n",
    "        print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "    def set_optimizer(self, extra_cost=None, var_list=None):\n",
    "        \"\"\"Recompute the optimizer to reflect the latest cost function.\n",
    "\n",
    "        This is useful when the cost function is modified throughout training\n",
    "\n",
    "        Args:\n",
    "            extra_cost : tensorflow variable,\n",
    "            added to the lsq and regularization cost\n",
    "        \"\"\"\n",
    "        cost = self.cost_lsq + self.cost_reg\n",
    "        if extra_cost is not None:\n",
    "            cost += extra_cost\n",
    "\n",
    "        if var_list is None:\n",
    "            var_list = self.var_list\n",
    "\n",
    "        print('Variables being optimized:')\n",
    "        for v in var_list:\n",
    "            print(v)\n",
    "\n",
    "        self.grads_and_vars = self.opt.compute_gradients(cost, var_list)\n",
    "        # gradient clipping\n",
    "        capped_gvs = [(tf.clip_by_value(grad, -1., 1.), var)\n",
    "                      for grad, var in self.grads_and_vars]\n",
    "        self.train_step = self.opt.apply_gradients(capped_gvs)\n",
    "\n",
    "    def lesion_units(self, sess, units, verbose=False):\n",
    "        \"\"\"Lesion units given by units\n",
    "\n",
    "        Args:\n",
    "            sess: tensorflow session\n",
    "            units : can be None, an integer index, or a list of integer indices\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert to numpy array\n",
    "        if units is None:\n",
    "            return\n",
    "        elif not hasattr(units, '__iter__'):\n",
    "            units = np.array([units])\n",
    "        else:\n",
    "            units = np.array(units)\n",
    "\n",
    "        # This lesioning will work for both RNN and GRU\n",
    "        n_input = self.hp['n_input']\n",
    "        for v in self.var_list:\n",
    "            if 'kernel' in v.name or 'weight' in v.name:\n",
    "                # Connection weights\n",
    "                v_val = sess.run(v)\n",
    "                if 'output' in v.name:\n",
    "                    # output weights\n",
    "                    v_val[units, :] = 0\n",
    "                elif 'rnn' in v.name:\n",
    "                    # recurrent weights\n",
    "                    v_val[n_input + units, :] = 0\n",
    "                sess.run(v.assign(v_val))\n",
    "\n",
    "        if verbose:\n",
    "            print('Lesioned units:')\n",
    "            print(units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebae316",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(model_dir, hp=hp)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "#     if load_model:\n",
    "#         model.restore(model_dir)  # complete restore\n",
    "#     else:\n",
    "#         # Assume everything is restored\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Set trainable parameters\n",
    "#     if trainables is None or trainables == 'all':\n",
    "    var_list = model.var_list  # train everything\n",
    "#     elif trainables == 'input':\n",
    "#         # train all nputs\n",
    "#         var_list = [v for v in model.var_list\n",
    "#                     if ('input' in v.name) and ('rnn' not in v.name)]\n",
    "#     elif trainables == 'rule':\n",
    "#         # train rule inputs only\n",
    "#         var_list = [v for v in model.var_list if 'rule_input' in v.name]\n",
    "#     else:\n",
    "#         raise ValueError('Unknown trainables')\n",
    "#     model.set_optimizer(var_list=var_list)\n",
    "\n",
    "#     # penalty on deviation from initial weight\n",
    "#     if hp['l2_weight_init'] > 0:\n",
    "#         anchor_ws = sess.run(model.weight_list)\n",
    "#         for w, w_val in zip(model.weight_list, anchor_ws):\n",
    "#             model.cost_reg += (hp['l2_weight_init'] *\n",
    "#                                tf.nn.l2_loss(w - w_val))\n",
    "\n",
    "#         model.set_optimizer(var_list=var_list)\n",
    "\n",
    "#     # partial weight training\n",
    "#     if ('p_weight_train' in hp and\n",
    "#         (hp['p_weight_train'] is not None) and\n",
    "#         hp['p_weight_train'] < 1.0):\n",
    "#         for w in model.weight_list:\n",
    "#             w_val = sess.run(w)\n",
    "#             w_size = sess.run(tf.size(w))\n",
    "#             w_mask_tmp = np.linspace(0, 1, w_size)\n",
    "#             hp['rng'].shuffle(w_mask_tmp)\n",
    "#             ind_fix = w_mask_tmp > hp['p_weight_train']\n",
    "#             w_mask = np.zeros(w_size, dtype=np.float32)\n",
    "#             w_mask[ind_fix] = 1e-1  # will be squared in l2_loss\n",
    "#             w_mask = tf.constant(w_mask)\n",
    "#             w_mask = tf.reshape(w_mask, w.shape)\n",
    "#             model.cost_reg += tf.nn.l2_loss((w - w_val) * w_mask)\n",
    "#         model.set_optimizer(var_list=var_list)\n",
    "\n",
    "    step = 0\n",
    "    while step * hp['batch_size_train'] < max_steps:\n",
    "        try:\n",
    "            # Validation\n",
    "            if step % display_step == 0:\n",
    "                log['trials'].append(step * hp['batch_size_train'])\n",
    "                log['times'].append(time.time()-t_start)\n",
    "                log = do_eval(sess, model, log, hp['rule_trains'],n_loc=n_loc,step=step)\n",
    "                #if log['perf_avg'][-1] > model.hp['target_perf']:\n",
    "                #check if minimum performance is above target    \n",
    "                # if log['perf_min'][-1] >= model.hp['target_perf']:\n",
    "                if all(elem >= model.hp['target_perf'] for elem in log['perf_min'][-5:]):\n",
    "                    print('Perf reached the target: {:0.2f}'.format(\n",
    "                        hp['target_perf']))\n",
    "                    break\n",
    "\n",
    "                if rich_output:\n",
    "                    display_rich_output(model, sess, step, log, model_dir)\n",
    "\n",
    "            # Training\n",
    "            rule_train_now = hp['rng'].choice(hp['rule_trains'],\n",
    "                                              p=hp['rule_probs'])\n",
    "            # Generate a random batch of trials.\n",
    "            # Each batch has the same trial length\n",
    "            trial = generate_trials(\n",
    "                    rule_train_now, hp, 'random',\n",
    "                    batch_size=hp['batch_size_train'],step=step,log=log,display_step=display_step,n_loc=n_loc)\n",
    "\n",
    "            # Generating feed_dict.\n",
    "            feed_dict = tools.gen_feed_dict(model, trial, hp)\n",
    "            sess.run(model.train_step, feed_dict=feed_dict)\n",
    "\n",
    "            step += 1\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Optimization interrupted by user\")\n",
    "            break\n",
    "\n",
    "    print(\"Optimization finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5305ebb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_hidden in [8, 16, 32, 64, 128, 256]:\n",
    "    ## Create folders to save model and training info\n",
    "    model_folder = 'leaky2_rnn_%s_n_hidden_%s'%(datetime.now(pytz.timezone('Asia/Singapore')).strftime(\"%d_%m_%Y_%H_%M_%S\"),n_hidden)\n",
    "    checkpoint_path = model_folder+\"/cp.ckpt\"\n",
    "    saved_model_folder = model_folder+'/saved_model'\n",
    "    loss_curve_folder = anova_folder = tuning_curve_folder = model_folder+'/'\n",
    "\n",
    "    os.makedirs(model_folder)\n",
    "\n",
    "    cell = LeakyRNNCell2(n_hidden)\n",
    "    rnn_layer = tf.keras.layers.RNN(cell,input_shape=(inputs_train.shape[1:]),\n",
    "                                    return_sequences=True)\n",
    "\n",
    "\n",
    "    hidden = rnn_layer(inputs)\n",
    "    delay1_hidden = hidden[:,1,:]\n",
    "    delay2_hidden = hidden[:,3,:]\n",
    "    plot_tuning_curves(delay1_hidden,tuning_curve_folder,'untrained_delay1_hidden')\n",
    "    plot_tuning_curves(delay2_hidden,tuning_curve_folder,'untrained_delay2_hidden')\n",
    "\n",
    "    get_anova_stats(anova_folder+'untrained_', rnn_layer)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(rnn_layer)\n",
    "    model.add(TimeDistributed(Dense(32)))\n",
    "    model.summary()\n",
    "\n",
    "    ### Check performance and selectivity before training\n",
    "    print(\"n_hidden %s untrained performance: %s\"%(n_hidden, get_model_performance(model)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ### Create callbacks to saves the model's weights and earlystopping\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                    save_weights_only=True,\n",
    "                                                    verbose=1)\n",
    "    es_callback = tf.keras.callbacks.EarlyStopping(restore_best_weights=True, patience=10)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),loss=tf.keras.losses.mse)\n",
    "\n",
    "    ### Training the model\n",
    "    history = model.fit(inputs_train, outputs_train, validation_data=(inputs_test,outputs_test), batch_size=64, epochs=500, verbose=2,\n",
    "              callbacks=[cp_callback, es_callback])\n",
    "    plot_loss_over_epochs(history, foldername=loss_curve_folder)\n",
    "    model.save(saved_model_folder)\n",
    "\n",
    "\n",
    "\n",
    "    # model = tf.keras.models.load_model(saved_model_folder)\n",
    "\n",
    "    ### Check performance and selectivity after training\n",
    "    print(\"n_hidden %s trained performance: %s\"%(n_hidden, get_model_performance(model)))\n",
    "\n",
    "    # simplernn = SimpleRNN(n_hidden, input_shape=(inputs_train.shape[1:]), return_sequences=True, weights=model.layers[0].get_weights())\n",
    "    rnn_layer = tf.keras.layers.RNN(cell,input_shape=(inputs_train.shape[1:]),\n",
    "                                    return_sequences=True, weights=model.layers[0].get_weights())\n",
    "    hidden = rnn_layer(inputs)\n",
    "\n",
    "    delay1_hidden = hidden[:,1,:]\n",
    "    delay2_hidden = hidden[:,3,:]\n",
    "    plot_tuning_curves(delay1_hidden,tuning_curve_folder,'trained_delay1_hidden')\n",
    "    plot_tuning_curves(delay2_hidden,tuning_curve_folder,'trained_delay2_hidden')\n",
    "\n",
    "    get_anova_stats(anova_folder, rnn_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e5027f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeakyRNNCell_softplus(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, units, **kwargs):\n",
    "        alpha = 0.2\n",
    "        sigma_rec = 0\n",
    "        self.units = units\n",
    "        self.state_size = units\n",
    "        self._activation = tf.nn.softplus\n",
    "        self._w_in_start = 1.0\n",
    "        self._w_rec_start = 0.5\n",
    "        self.rng = np.random.RandomState()\n",
    "        self._alpha = alpha\n",
    "        self._sigma = np.sqrt(2 / alpha) * sigma_rec\n",
    "        super(LeakyRNNCell_softplus, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        w_in0 = (self.rng.randn(input_shape[-1], self.units) /\n",
    "                 np.sqrt(input_shape[-1]) * self._w_in_start)\n",
    "        self.kernel = self.add_weight(shape=(input_shape[-1], self.units), dtype=tf.float32,\n",
    "                                      initializer=tf.constant_initializer(w_in0),\n",
    "                                      name='kernel')\n",
    "        w_rec0 = self._w_rec_start*ortho_group.rvs(dim=self.units, random_state=self.rng)\n",
    "        self.recurrent_kernel = self.add_weight(shape=(self.units, self.units), dtype=tf.float32,\n",
    "                                                initializer=tf.constant_initializer(w_rec0),\n",
    "                                                name='recurrent_kernel')\n",
    "        matrix0 = np.concatenate((w_in0, w_rec0), axis=0)\n",
    "    \n",
    "#         self.kernel = self.add_weight(\n",
    "#                 name='kernel',\n",
    "#                 shape=[input_shape[-1] + self.units, self.units], \n",
    "#                 dtype=tf.float32,\n",
    "#                 initializer=tf.constant_initializer(matrix0))\n",
    "        \n",
    "        self._bias = self.add_weight(\n",
    "                name='bias',\n",
    "                shape=[self.units],\n",
    "                dtype=tf.float32,\n",
    "                initializer=tf.zeros_initializer())\n",
    "        \n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, states):\n",
    "        prev_output = states[0]\n",
    "        h = tf.keras.backend.dot(inputs, self.kernel)\n",
    "#         h = tf.keras.backend.dot(array_ops.concat([inputs, prev_output], 1), self.kernel)\n",
    "        h = h + tf.keras.backend.dot(prev_output, self.recurrent_kernel)\n",
    "        h = tf.nn.bias_add(h, self._bias)\n",
    "        noise = tf.random.normal(tf.shape(prev_output), mean=0, stddev=self._sigma)\n",
    "        h = h + noise\n",
    "        output = self._activation(h)\n",
    "        output = (1-self._alpha) * prev_output + self._alpha * output\n",
    "        return output, [output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5124aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_hidden in [8, 16, 32, 64, 128, 256]:\n",
    "    ## Create folders to save model and training info\n",
    "    model_folder = 'leaky_softplus_rnn_%s_n_hidden_%s'%(datetime.now(pytz.timezone('Asia/Singapore')).strftime(\"%d_%m_%Y_%H_%M_%S\"),n_hidden)\n",
    "    checkpoint_path = model_folder+\"/cp.ckpt\"\n",
    "    saved_model_folder = model_folder+'/saved_model'\n",
    "    loss_curve_folder = anova_folder = tuning_curve_folder = model_folder+'/'\n",
    "\n",
    "    os.makedirs(model_folder)\n",
    "\n",
    "    cell = LeakyRNNCell_softplus(n_hidden)\n",
    "    rnn_layer = tf.keras.layers.RNN(cell,input_shape=(inputs_train.shape[1:]),\n",
    "                                    return_sequences=True)\n",
    "\n",
    "\n",
    "    hidden = rnn_layer(inputs)\n",
    "    delay1_hidden = hidden[:,1,:]\n",
    "    delay2_hidden = hidden[:,3,:]\n",
    "    plot_tuning_curves(delay1_hidden,tuning_curve_folder,'untrained_delay1_hidden')\n",
    "    plot_tuning_curves(delay2_hidden,tuning_curve_folder,'untrained_delay2_hidden')\n",
    "\n",
    "    get_anova_stats(anova_folder+'untrained_', rnn_layer)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(rnn_layer)\n",
    "    model.add(TimeDistributed(Dense(32)))\n",
    "    model.summary()\n",
    "\n",
    "    ### Check performance and selectivity before training\n",
    "    print(\"n_hidden %s untrained performance: %s\"%(n_hidden, get_model_performance(model)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ### Create callbacks to saves the model's weights and earlystopping\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                    save_weights_only=True,\n",
    "                                                    verbose=1)\n",
    "    es_callback = tf.keras.callbacks.EarlyStopping(restore_best_weights=True, patience=10)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),loss=tf.keras.losses.mse)\n",
    "\n",
    "    ### Training the model\n",
    "    history = model.fit(inputs_train, outputs_train, validation_data=(inputs_test,outputs_test), batch_size=64, epochs=500, verbose=2,\n",
    "              callbacks=[cp_callback, es_callback])\n",
    "    plot_loss_over_epochs(history, foldername=loss_curve_folder)\n",
    "    model.save(saved_model_folder)\n",
    "\n",
    "\n",
    "\n",
    "    # model = tf.keras.models.load_model(saved_model_folder)\n",
    "\n",
    "    ### Check performance and selectivity after training\n",
    "    print(\"n_hidden %s trained performance: %s\"%(n_hidden, get_model_performance(model)))\n",
    "\n",
    "    # simplernn = SimpleRNN(n_hidden, input_shape=(inputs_train.shape[1:]), return_sequences=True, weights=model.layers[0].get_weights())\n",
    "    rnn_layer = tf.keras.layers.RNN(cell,input_shape=(inputs_train.shape[1:]),\n",
    "                                    return_sequences=True, weights=model.layers[0].get_weights())\n",
    "    hidden = rnn_layer(inputs)\n",
    "\n",
    "    delay1_hidden = hidden[:,1,:]\n",
    "    delay2_hidden = hidden[:,3,:]\n",
    "    plot_tuning_curves(delay1_hidden,tuning_curve_folder,'trained_delay1_hidden')\n",
    "    plot_tuning_curves(delay2_hidden,tuning_curve_folder,'trained_delay2_hidden')\n",
    "\n",
    "    get_anova_stats(anova_folder, rnn_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87950020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41ef565d",
   "metadata": {},
   "source": [
    "### creating mixed selectivity problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00540dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "stim3_locs = (stim1_locs*stim2_locs)%(2*np.pi)\n",
    "\n",
    "Y = []\n",
    "for i in range(stim_loc_size):\n",
    "    y = add_x_loc(stim3_locs[i])\n",
    "    Y.append(y)\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f02104",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a074eaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_hidden in [8, 16, 32, 64, 128, 256]:\n",
    "\n",
    "    print(\"n_hidden: %s\"%n_hidden)\n",
    "    ## Create folders to save model and training info\n",
    "    model_folder = 'linear_%s_n_hidden_%s'%(datetime.now(pytz.timezone('Asia/Singapore')).strftime(\"%d_%m_%Y_%H_%M_%S\"),n_hidden)\n",
    "    checkpoint_path = model_folder+\"/cp.ckpt\"\n",
    "    saved_model_folder = model_folder+'/saved_model'\n",
    "    loss_curve_folder = anova_folder = tuning_curve_folder = model_folder+'/'\n",
    "\n",
    "    os.makedirs(model_folder)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(tf.keras.Input(shape=(64,)))\n",
    "    model.add(tf.keras.layers.Dense(n_hidden, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "    model.summary()\n",
    "\n",
    "    model2 = Sequential()\n",
    "    model2.add(tf.keras.Input(shape=(64,)))\n",
    "    model2.add(tf.keras.layers.Dense(n_hidden, activation='relu', weights=model.layers[0].get_weights()))\n",
    "    hidden = model2(X)\n",
    "    plot_tuning_curves(hidden,tuning_curve_folder,'untrained_hidden')\n",
    "    get_anova_stats_linear(anova_folder+'untrained_', model2)\n",
    "    \n",
    "    \n",
    "    ### Create callbacks to saves the model's weights and earlystopping\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                    save_weights_only=True,\n",
    "                                                    verbose=1)\n",
    "    es_callback = tf.keras.callbacks.EarlyStopping(restore_best_weights=True, patience=10)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),loss=tf.keras.losses.mse)\n",
    "\n",
    "    ### Training the model\n",
    "    history = model.fit(X, Y, validation_data=(X,Y), batch_size=64, epochs=500, verbose=2,\n",
    "              callbacks=[cp_callback, es_callback])\n",
    "    plot_loss_over_epochs(history, foldername=loss_curve_folder)\n",
    "    model.save(saved_model_folder)\n",
    "    \n",
    "    model2 = Sequential()\n",
    "    model2.add(tf.keras.Input(shape=(64,)))\n",
    "    model2.add(tf.keras.layers.Dense(n_hidden, activation='relu', weights=model.layers[0].get_weights()))\n",
    "\n",
    "    hidden = model2(X)\n",
    "    plot_tuning_curves(hidden,tuning_curve_folder,'trained_hidden')\n",
    "    get_anova_stats_linear(anova_folder+'trained_', model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb828550",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_hidden in [8, 16, 32, 64, 128, 256]:\n",
    "    \n",
    "    activation = 'relu'\n",
    "    kernel_initializer=tf.keras.initializers.RandomNormal\n",
    "\n",
    "    print(\"n_hidden: %s\"%n_hidden)\n",
    "    ## Create folders to save model and training info\n",
    "    model_folder = 'linear_%s_%s_n_hidden_%s'%(activation, datetime.now(pytz.timezone('Asia/Singapore')).strftime(\"%d_%m_%Y_%H_%M_%S\"),n_hidden)\n",
    "    checkpoint_path = model_folder+\"/cp.ckpt\"\n",
    "    saved_model_folder = model_folder+'/saved_model'\n",
    "    loss_curve_folder = anova_folder = tuning_curve_folder = model_folder+'/'\n",
    "\n",
    "    os.makedirs(model_folder)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(tf.keras.Input(shape=(64,)))\n",
    "    model.add(tf.keras.layers.Dense(n_hidden, activation=activation, kernel_initializer=kernel_initializer))\n",
    "    model.add(tf.keras.layers.Dense(32, activation=activation, kernel_initializer=kernel_initializer))\n",
    "    model.summary()\n",
    "\n",
    "    model2 = Sequential()\n",
    "    model2.add(tf.keras.Input(shape=(64,)))\n",
    "    model2.add(tf.keras.layers.Dense(n_hidden, activation=activation, kernel_initializer=kernel_initializer, weights=model.layers[0].get_weights()))\n",
    "    hidden = model2(X)\n",
    "    plot_tuning_curves(hidden,tuning_curve_folder,'untrained_hidden')\n",
    "    get_anova_stats_linear(anova_folder+'untrained_', model2)\n",
    "    \n",
    "    \n",
    "    ### Create callbacks to saves the model's weights and earlystopping\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                    save_weights_only=True,\n",
    "                                                    verbose=1)\n",
    "    es_callback = tf.keras.callbacks.EarlyStopping(restore_best_weights=True, patience=10)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),loss=tf.keras.losses.mse)\n",
    "\n",
    "    ### Training the model\n",
    "    history = model.fit(X, Y, validation_data=(X,Y), batch_size=64, epochs=500, verbose=2,\n",
    "              callbacks=[cp_callback, es_callback])\n",
    "    plot_loss_over_epochs(history, foldername=loss_curve_folder)\n",
    "#     model.save(saved_model_folder)\n",
    "    \n",
    "    model2 = Sequential()\n",
    "    model2.add(tf.keras.Input(shape=(64,)))\n",
    "    model2.add(tf.keras.layers.Dense(n_hidden, activation=activation, kernel_initializer=kernel_initializer, weights=model.layers[0].get_weights()))\n",
    "\n",
    "    hidden = model2(X)\n",
    "    plot_tuning_curves(hidden,tuning_curve_folder,'trained_hidden')\n",
    "    get_anova_stats_linear(anova_folder+'trained_', model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192455e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_hidden in [8, 16, 32, 64, 128, 256]:\n",
    "    \n",
    "    activation = 'softplus'\n",
    "    kernel_initializer=tf.keras.initializers.RandomNormal\n",
    "\n",
    "    print(\"n_hidden: %s\"%n_hidden)\n",
    "    ## Create folders to save model and training info\n",
    "    model_folder = 'linear_%s_%s_n_hidden_%s'%(activation, datetime.now(pytz.timezone('Asia/Singapore')).strftime(\"%d_%m_%Y_%H_%M_%S\"),n_hidden)\n",
    "    checkpoint_path = model_folder+\"/cp.ckpt\"\n",
    "    saved_model_folder = model_folder+'/saved_model'\n",
    "    loss_curve_folder = anova_folder = tuning_curve_folder = model_folder+'/'\n",
    "\n",
    "    os.makedirs(model_folder)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(tf.keras.Input(shape=(64,)))\n",
    "    model.add(tf.keras.layers.Dense(n_hidden, activation=activation, kernel_initializer=kernel_initializer))\n",
    "    model.add(tf.keras.layers.Dense(32, activation=activation, kernel_initializer=kernel_initializer))\n",
    "    model.summary()\n",
    "\n",
    "    model2 = Sequential()\n",
    "    model2.add(tf.keras.Input(shape=(64,)))\n",
    "    model2.add(tf.keras.layers.Dense(n_hidden, activation=activation, kernel_initializer=kernel_initializer, weights=model.layers[0].get_weights()))\n",
    "    hidden = model2(X)\n",
    "    plot_tuning_curves(hidden,tuning_curve_folder,'untrained_hidden')\n",
    "    get_anova_stats_linear(anova_folder+'untrained_', model2)\n",
    "    \n",
    "    \n",
    "    ### Create callbacks to saves the model's weights and earlystopping\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                    save_weights_only=True,\n",
    "                                                    verbose=1)\n",
    "    es_callback = tf.keras.callbacks.EarlyStopping(restore_best_weights=True, patience=10)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),loss=tf.keras.losses.mse)\n",
    "\n",
    "    ### Training the model\n",
    "    history = model.fit(X, Y, validation_data=(X,Y), batch_size=64, epochs=500, verbose=2,\n",
    "              callbacks=[cp_callback, es_callback])\n",
    "    plot_loss_over_epochs(history, foldername=loss_curve_folder)\n",
    "#     model.save(saved_model_folder)\n",
    "    \n",
    "    model2 = Sequential()\n",
    "    model2.add(tf.keras.Input(shape=(64,)))\n",
    "    model2.add(tf.keras.layers.Dense(n_hidden, activation=activation, kernel_initializer=kernel_initializer, weights=model.layers[0].get_weights()))\n",
    "\n",
    "    hidden = model2(X)\n",
    "    plot_tuning_curves(hidden,tuning_curve_folder,'trained_hidden')\n",
    "    get_anova_stats_linear(anova_folder+'trained_', model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9574f3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = model2(X_n)\n",
    "\n",
    "for i in range(min(n_hidden,10)):\n",
    "    if n_hidden<10:\n",
    "        neuron = i\n",
    "    else:\n",
    "        neuron = i*int(n_hidden/10)\n",
    "        \n",
    "    df = pd.DataFrame({'first_stim':np.array([[i]*10 for i in ind_stim_loc1]).flatten(),'second_stim':np.array([[i]*10 for i in ind_stim_loc2]).flatten(),'activity':hidden[:,neuron]})\n",
    "    aov = pg.anova(dv='activity', between=['first_stim', 'second_stim'], data=df,\n",
    "              detailed=True)\n",
    "    print('neuron %s:'%neuron, aov['p-unc'][0],aov['p-unc'][1],aov['p-unc'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746898d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27994069",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 256\n",
    "\n",
    "activation = 'softplus'\n",
    "kernel_initializer=tf.keras.initializers.RandomNormal\n",
    "\n",
    "print(\"n_hidden: %s\"%n_hidden)\n",
    "## Create folders to save model and training info\n",
    "model_folder = 'linear_%s_%s_n_hidden_%s'%(activation, datetime.now(pytz.timezone('Asia/Singapore')).strftime(\"%d_%m_%Y_%H_%M_%S\"),n_hidden)\n",
    "checkpoint_path = model_folder+\"/cp.ckpt\"\n",
    "saved_model_folder = model_folder+'/saved_model'\n",
    "loss_curve_folder = anova_folder = tuning_curve_folder = model_folder+'/'\n",
    "\n",
    "os.makedirs(model_folder)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(tf.keras.Input(shape=(64,)))\n",
    "model.add(tf.keras.layers.Dense(n_hidden, activation=activation, kernel_initializer=kernel_initializer))\n",
    "model.add(tf.keras.layers.Dense(32, activation=activation, kernel_initializer=kernel_initializer))\n",
    "model.summary()\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(tf.keras.Input(shape=(64,)))\n",
    "model2.add(tf.keras.layers.Dense(n_hidden, activation=activation, kernel_initializer=kernel_initializer, weights=model.layers[0].get_weights()))\n",
    "hidden = model2(X)\n",
    "plot_tuning_curves(hidden,'')\n",
    "# get_anova_stats_linear(anova_folder+'untrained_', model2)\n",
    "\n",
    "\n",
    "hidden = model2(X_n)\n",
    "\n",
    "for i in range(min(n_hidden,10)):\n",
    "    if n_hidden<10:\n",
    "        neuron = i\n",
    "    else:\n",
    "        neuron = i*int(n_hidden/10)\n",
    "        \n",
    "    df = pd.DataFrame({'first_stim':np.array([[i]*10 for i in ind_stim_loc1]).flatten(),'second_stim':np.array([[i]*10 for i in ind_stim_loc2]).flatten(),'activity':hidden[:,neuron]})\n",
    "    aov = pg.anova(dv='activity', between=['first_stim', 'second_stim'], data=df,\n",
    "              detailed=True)\n",
    "    print('neuron %s:'%neuron, aov['p-unc'][0],aov['p-unc'][1],aov['p-unc'][2])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "### Create callbacks to saves the model's weights and earlystopping\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                save_weights_only=True,\n",
    "                                                verbose=1)\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(restore_best_weights=True, patience=10)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),loss=tf.keras.losses.mse)\n",
    "\n",
    "### Training the model\n",
    "history = model.fit(X, Y, validation_data=(X,Y), batch_size=64, epochs=500, verbose=2,\n",
    "          callbacks=[cp_callback, es_callback])\n",
    "plot_loss_over_epochs(history, foldername=loss_curve_folder)\n",
    "#     model.save(saved_model_folder)\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(tf.keras.Input(shape=(64,)))\n",
    "model2.add(tf.keras.layers.Dense(n_hidden, activation=activation, kernel_initializer=kernel_initializer, weights=model.layers[0].get_weights()))\n",
    "\n",
    "\n",
    "hidden = model2(X_n)\n",
    "\n",
    "for i in range(min(n_hidden,10)):\n",
    "    if n_hidden<10:\n",
    "        neuron = i\n",
    "    else:\n",
    "        neuron = i*int(n_hidden/10)\n",
    "        \n",
    "    df = pd.DataFrame({'first_stim':np.array([[i]*10 for i in ind_stim_loc1]).flatten(),'second_stim':np.array([[i]*10 for i in ind_stim_loc2]).flatten(),'activity':hidden[:,neuron]})\n",
    "    aov = pg.anova(dv='activity', between=['first_stim', 'second_stim'], data=df,\n",
    "              detailed=True)\n",
    "    print('neuron %s:'%neuron, aov['p-unc'][0],aov['p-unc'][1],aov['p-unc'][2])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad671bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anova_stats_linear(foldername, model):\n",
    "#     hidden = model(X_n)\n",
    "\n",
    "#     neuron = 0\n",
    "#     df = pd.DataFrame({'first_stim':np.array([[i]*10 for i in ind_stim_loc1]).flatten(),'second_stim':np.array([[i]*10 for i in ind_stim_loc2]).flatten(),'activity':hidden[:,neuron]})\n",
    "#     aov = pg.anova(dv='activity', between=['first_stim', 'second_stim'], data=df,\n",
    "#               detailed=True)\n",
    "#     print(aov)\n",
    "#     aov.to_csv(foldername+\"anova.csv\")\n",
    "    \n",
    "    hidden = model(X_n)\n",
    "    for i in range(min(n_hidden,10)):\n",
    "        if n_hidden<10:\n",
    "            neuron = i\n",
    "        else:\n",
    "            neuron = i*int(n_hidden/10)\n",
    "\n",
    "        df = pd.DataFrame({'first_stim':np.array([[i]*10 for i in ind_stim_loc1]).flatten(),'second_stim':np.array([[i]*10 for i in ind_stim_loc2]).flatten(),'activity':hidden[:,neuron]})\n",
    "        aov = pg.anova(dv='activity', between=['first_stim', 'second_stim'], data=df,\n",
    "                  detailed=True)\n",
    "        print('neuron %s:'%neuron, aov['p-unc'][0],aov['p-unc'][1],aov['p-unc'][2])\n",
    "        \n",
    "def plot_tuning_curves(model,foldername='',filename=''):\n",
    "    Z = model(X)\n",
    "    Z_n = model(X_n)\n",
    "    fig,ax = plt.subplots(min(Z.shape[1],10),1,figsize=(5,min(Z.shape[1],10)*3))\n",
    "    for i in range(min(Z.shape[1],10)):\n",
    "        if Z.shape[1]<10:\n",
    "            neuron = i\n",
    "        else:\n",
    "            neuron = i*int(Z.shape[1]/10)\n",
    "        for loc1 in [j*10 for j in range(int(128/10))]:\n",
    "            df = pd.DataFrame({'first_stim':ind_stim_loc1[ind_stim_loc1==loc1],'second_stim':ind_stim_loc2[ind_stim_loc1==loc1],'activity':Z[:,neuron][ind_stim_loc1==loc1]})\n",
    "            x = df['second_stim']\n",
    "            y = df['activity']\n",
    "            ax[i].scatter(x,y,s=1, color=palette1[loc1], label=loc1)\n",
    "            \n",
    "        df_n = pd.DataFrame({'first_stim':np.array([[i]*10 for i in ind_stim_loc1]).flatten(),'second_stim':np.array([[i]*10 for i in ind_stim_loc2]).flatten(),'activity':Z_n[:,neuron]})\n",
    "        aov = pg.anova(dv='activity', between=['first_stim', 'second_stim'], data=df_n, detailed=True)\n",
    "        ax[i].text(1, 0, 'anova p: (%s,%s,%s)'%(round(aov['p-unc'][0],2),round(aov['p-unc'][1],2),round(aov['p-unc'][2],2)), fontsize = 8)\n",
    "        \n",
    "        ax[i].set_ylabel('%dth neuron'%neuron, fontsize=13)\n",
    "    ax[0].legend(loc='upper left', bbox_to_anchor= (1.05, 1.05), title='Stim 1')\n",
    "    ax[0].set_xlabel('Stim 2', fontsize=13)\n",
    "    ax[0].xaxis.set_label_position('top') \n",
    "    if filename != '':\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(foldername+filename+'_stim2')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "        \n",
    "    fig,ax = plt.subplots(min(Z.shape[1],10),1,figsize=(5,min(Z.shape[1],10)*3))\n",
    "    for i in range(min(Z.shape[1],10)):\n",
    "        if Z.shape[1]<10:\n",
    "            neuron = i\n",
    "        else:\n",
    "            neuron = i*int(Z.shape[1]/10)\n",
    "        for loc2 in [j*10 for j in range(int(128/10))]:\n",
    "            df = pd.DataFrame({'first_stim':ind_stim_loc1[ind_stim_loc2==loc2],'second_stim':ind_stim_loc2[ind_stim_loc2==loc2],'activity':Z[:,neuron][ind_stim_loc2==loc2]})\n",
    "            x = df['first_stim']\n",
    "            y = df['activity']\n",
    "            ax[i].scatter(x,y,s=1, color=palette2[loc2], label=loc2)\n",
    "            \n",
    "        df_n = pd.DataFrame({'first_stim':np.array([[i]*10 for i in ind_stim_loc1]).flatten(),'second_stim':np.array([[i]*10 for i in ind_stim_loc2]).flatten(),'activity':Z_n[:,neuron]})\n",
    "        aov = pg.anova(dv='activity', between=['first_stim', 'second_stim'], data=df_n, detailed=True)\n",
    "        ax[i].text(1, 0, 'anova p: (%s,%s,%s)'%(round(aov['p-unc'][0],2),round(aov['p-unc'][1],2),round(aov['p-unc'][2],2)), fontsize = 8)\n",
    "            \n",
    "        ax[i].set_ylabel('%dth neuron'%neuron, fontsize=13)\n",
    "    ax[0].legend(loc='upper left', bbox_to_anchor= (1.05, 1.05), title='Stim 2')\n",
    "    ax[0].set_xlabel('Stim 1', fontsize=13)\n",
    "    ax[0].xaxis.set_label_position('top') \n",
    "    if filename != '':\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(foldername+filename+'_stim1')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e786b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 256\n",
    "\n",
    "activation = 'relu'\n",
    "kernel_initializer=tf.keras.initializers.RandomNormal\n",
    "\n",
    "print(\"n_hidden: %s\"%n_hidden)\n",
    "# ## Create folders to save model and training info\n",
    "# model_folder = 'linear_%s_%s_n_hidden_%s'%(activation, datetime.now(pytz.timezone('Asia/Singapore')).strftime(\"%d_%m_%Y_%H_%M_%S\"),n_hidden)\n",
    "# checkpoint_path = model_folder+\"/cp.ckpt\"\n",
    "# saved_model_folder = model_folder+'/saved_model'\n",
    "# loss_curve_folder = anova_folder = tuning_curve_folder = model_folder+'/'\n",
    "\n",
    "# os.makedirs(model_folder)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(tf.keras.Input(shape=(64,)))\n",
    "model.add(tf.keras.layers.Dense(n_hidden, activation=activation, kernel_initializer=kernel_initializer))\n",
    "model.add(tf.keras.layers.Dense(32, activation=activation, kernel_initializer=kernel_initializer))\n",
    "model.summary()\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(tf.keras.Input(shape=(64,)))\n",
    "model2.add(tf.keras.layers.Dense(n_hidden, activation=activation, kernel_initializer=kernel_initializer, weights=model.layers[0].get_weights()))\n",
    "\n",
    "print(\"untrained hidden:\")\n",
    "plot_tuning_curves(model2,tuning_curve_folder,'untrained_hidden')\n",
    "# get_anova_stats_linear(anova_folder+'untrained_hidden_', model2)\n",
    "\n",
    "print(\"untrained output:\")\n",
    "plot_tuning_curves(model,tuning_curve_folder,'untrained_output')\n",
    "# get_anova_stats_linear(anova_folder+'untrained_output_', model)\n",
    "\n",
    "### Create callbacks to saves the model's weights and earlystopping\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                save_weights_only=True,\n",
    "                                                verbose=1)\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(restore_best_weights=True, patience=10)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),loss=tf.keras.losses.mse)\n",
    "\n",
    "### Training the model\n",
    "history = model.fit(X, Y, validation_data=(X,Y), batch_size=64, epochs=500, verbose=2,\n",
    "          callbacks=[cp_callback, es_callback])\n",
    "plot_loss_over_epochs(history, foldername=loss_curve_folder)\n",
    "model.save(saved_model_folder)\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(tf.keras.Input(shape=(64,)))\n",
    "model2.add(tf.keras.layers.Dense(n_hidden, activation=activation, kernel_initializer=kernel_initializer, weights=model.layers[0].get_weights()))\n",
    "\n",
    "print(\"trained hidden:\")\n",
    "plot_tuning_curves(model2,tuning_curve_folder,'trained_hidden')\n",
    "# get_anova_stats_linear(anova_folder+'trained_hidden_', model2)\n",
    "\n",
    "print(\"trained output:\")\n",
    "plot_tuning_curves(model,tuning_curve_folder,'trained_output')\n",
    "# get_anova_stats_linear(anova_folder+'trained_output_', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201ab13d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
